{"basic configuration": "\n## @param api_key - string - required\n## @env DD_API_KEY - string - required\n## The Datadog API key used by your Agent to submit metrics and events to Datadog.\n## Create a new API key here: https://app.datadoghq.com/organization-settings/api-keys .\n## Read more about API keys here: https://docs.datadoghq.com/account_management/api-app-keys/#api-keys .\napi_key:\n\n## @param app_key - string - optional\n## The application key used to access Datadog's programatic API.\n## Create a new application key here: https://app.datadoghq.com/organization-settings/application-keys .\n## Read more about application keys here: https://docs.datadoghq.com/account_management/api-app-keys/#application-keys .\napp_key:\n\n## @param site - string - optional - default: datadoghq.com\n## @env DD_SITE - string - optional - default: datadoghq.com\n## The site of the Datadog intake to send Agent data to.\n## The site parameter must be set to enable your agent with Remote Configuration.\n## Set to 'datadoghq.eu' to send data to the EU site.\n## Set to 'us3.datadoghq.com' to send data to the US3 site.\n## Set to 'us5.datadoghq.com' to send data to the US5 site.\n## Set to 'ap1.datadoghq.com' to send data to the AP1 site.\n## Set to 'ddog-gov.com' to send data to the US1-FED site.\nsite: datadoghq.com\n\n## @param dd_url - string - optional - default: https://app.datadoghq.com\n## @env DD_DD_URL - string - optional - default: https://app.datadoghq.com\n## @env DD_URL - string - optional - default: https://app.datadoghq.com\n## The host of the Datadog intake server to send metrics to, only set this option\n## if you need the Agent to send metrics to a custom URL, it overrides the site\n## setting defined in \"site\". It does not affect APM, Logs, Remote Configuration, or Live Process intake which have their\n## own \"*_dd_url\" settings.\n## If DD_DD_URL and DD_URL are both set, DD_DD_URL is used in priority.\ndd_url: https://app.datadoghq.com\n\n## @param proxy - custom object - optional\n## @env DD_PROXY_HTTP - string - optional\n## @env DD_PROXY_HTTPS - string - optional\n## @env DD_PROXY_NO_PROXY - space separated list of strings - optional\n## If you need a proxy to connect to the Internet, provide it here (default:\n## disabled). Refer to https://docs.datadoghq.com/agent/proxy/ to understand how to use these settings.\n## For Logs proxy information, refer to https://docs.datadoghq.com/agent/proxy/#proxy-for-logs\nproxy:\n  https: http://<USERNAME>:<PASSWORD>@<PROXY_SERVER_FOR_HTTPS>:<PORT>\n  http: http://<USERNAME>:<PASSWORD>@<PROXY_SERVER_FOR_HTTP>:<PORT>\n  no_proxy:\n    - <HOSTNAME-1>\n    - <HOSTNAME-2>\n\n## @param skip_ssl_validation - boolean - optional - default: false\n## @env DD_SKIP_SSL_VALIDATION - boolean - optional - default: false\n## Setting this option to \"true\" tells the Agent to skip validation of SSL/TLS certificates.\nskip_ssl_validation: false\n\n## @param sslkeylogfile - string - optional - default: \"\"\n## @env DD_SSLKEYLOGFILE - string - optional - default: \"\"\n## sslkeylogfile specifies a destination for TLS master secrets\n## in NSS key log format to allow external programs\n## such as Wireshark to decrypt TLS connections.\n## For more details, see https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS/Key_Log_Format.\n## Use of sslkeylogfile compromises security and should only be\n## used for debugging.\nsslkeylogfile: \"\"\n\n## @param min_tls_version - string - optional - default: \"tlsv1.2\"\n## @env DD_MIN_TLS_VERSION - string - optional - default: \"tlsv1.2\"\n## This option defines the minimum TLS version that will be used when\n## submitting data to the Datadog intake specified in \"site\" or \"dd_url\".\n## This parameter defaults to \"tlsv1.2\".\n## Possible values are: tlsv1.0, tlsv1.1, tlsv1.2, tlsv1.3; values are case-\n## insensitive.\nmin_tls_version: \"tlsv1.2\"\n\n## @param hostname - string - optional - default: auto-detected\n## @env DD_HOSTNAME - string - optional - default: auto-detected\n## Force the hostname name.\nhostname: <HOSTNAME_NAME>\n\n## @param hostname_file - string - optional\n## @env DD_HOSTNAME_FILE - string - optional\n## In some environments, auto-detection of the hostname is not adequate and\n## environment variables cannot be used to set the value. In such cases, the\n## file on the host can also be used provide an appropriate value. If\n## 'hostname' value has been set to a non-empty value, this option is ignored.\nhostname_file: /var/lib/cloud/data/instance-id\n\n## @param hostname_fqdn - boolean - optional - default: false\n## @env DD_HOSTNAME_FQDN - boolean - optional - default: false\n## When the Agent relies on the OS to determine the hostname, make it use the\n## FQDN instead of the short hostname. Recommended value: true\n## More information at https://dtdg.co/flag-hostname-fqdn\nhostname_fqdn: false\n\n## @param hostname_trust_uts_namespace - boolean - optional - default: false\n## @env DD_HOSTNAME_TRUST_UTS_NAMESPACE - boolean - optional - default: false\n## By default the Agent does not trust the hostname value retrieved from non-root UTS namespace,\n## as it's usually a generated name, unrelated to the host (e.g. when running in a container).\n## When enabled, the Agent will trust the value retrieved from non-root UTS namespace instead of failing\n## hostname resolution.\n## (Linux only)\nhostname_trust_uts_namespace: false\n\n## @param host_aliases - list of strings - optional\n## @env DD_HOST_ALIASES - space separated list of strings - optional\n## List of host aliases to report in addition to any aliases collected\n## automatically from cloud providers.\n## More information at\n## https://docs.datadoghq.com/agent/faq/how-datadog-agent-determines-the-hostname/?tab=agentv6v7#host-aliases\nhost_aliases:\n  - <ALIAS-1>\n  - <ALIAS-2>\n\n## @param tags  - list of key:value elements - optional\n## @env DD_TAGS - space separated list of strings - optional\n## List of host tags. Attached in-app to every metric, event, log, trace, and service check emitted by this Agent.\n##\n## This configuration value merges with `DD_EXTRA_TAGS`, allowing some\n## tags to be set in a configuration file (`tags`), and additional tags to be added\n## with an environment variable (`DD_EXTRA_TAGS`).\n##\n## Learn more about tagging: https://docs.datadoghq.com/tagging/\ntags:\n  - team:infra\n  - <TAG_KEY>:<TAG_VALUE>\n\n## @param extra_tags  - list of key:value elements - optional\n## @env DD_EXTRA_TAGS - space separated list of strings - optional\n## List of host tags. Attached in-app to every metric, event, log, trace, and service check emitted by this Agent.\n##\n## This configuration value merges with `tags`, allowing some\n## tags to be set in a configuration file (`tags`), and additional tags to be added\n## with an environment variable (`DD_EXTRA_TAGS`).\n##\n## Learn more about tagging: https://docs.datadoghq.com/tagging/\nextra_tags:\n  - region:northerly\n  - <TAG_KEY>:<TAG_VALUE>\n\n## @param env - string - optional\n## @env DD_ENV - string - optional\n## The environment name where the agent is running. Attached in-app to every\n## metric, event, log, trace, and service check emitted by this Agent.\nenv: <environment name>\n\n## @param tag_value_split_separator - map - optional\n## @env DD_TAG_VALUE_SPLIT_SEPARATOR - list of key:value strings - optional\n## Split tag values according to a given separator. Only applies to host tags,\n## and tags coming from container integrations. It does not apply to tags on dogstatsd metrics,\n## and tags collected by other integrations.\n##\n## Example use-case:\n##\n##  With a raw collected tag \"foo:1;2;3\", using the following configuration:\n##\n##  tag_value_split_separator:\n##    foo: ;\n##\n##  results in the raw tag being transformed into \"foo:1\", \"foo:2\", \"foo:3\" tags\ntag_value_split_separator:\n  <TAG_KEY>: <SEPARATOR>\n\n## @param checks_tag_cardinality - string - optional - default: low\n## @env DD_CHECKS_TAG_CARDINALITY - string - optional - default: low\n## Configure the level of granularity of tags to send for checks metrics and events. Choices are:\n##   * low: add tags about low-cardinality objects (clusters, hosts, deployments, container images, ...)\n##   * orchestrator: add tags about pod, (in Kubernetes), or task (in ECS or Mesos) -level of cardinality\n##   * high: add tags about high-cardinality objects (individual containers, user IDs in requests, ...)\n## WARNING: sending container tags for checks metrics may create more metrics\n## (one per container instead of one per host). This may impact your custom metrics billing.\nchecks_tag_cardinality: low\n\n## @param dogstatsd_tag_cardinality - string - optional - default: low\n## @env DD_DOGSTATSD_TAG_CARDINALITY - string - optional - default: low\n## Configure the level of granularity of tags to send for DogStatsD metrics and events. Choices are:\n##   * low: add tags about low-cardinality objects (clusters, hosts, deployments, container images, ...)\n##   * orchestrator: add tags about pod, (in Kubernetes), or task (in ECS or Mesos) -level of cardinality\n##   * high: add tags about high-cardinality objects (individual containers, user IDs in requests, ...)\n##\n## WARNING: sending container tags for dogstatsd metrics may create more metrics\n## (one per container instead of one per host). This may impact your custom metrics billing.\ndogstatsd_tag_cardinality: low\n\n## @param histogram_aggregates - list of strings - optional - default: [\"max\", \"median\", \"avg\", \"count\"]\n## @env DD_HISTOGRAM_AGGREGATES - space separated list of strings - optional - default: max median avg count\n## Configure which aggregated value to compute.\n## Possible values are: min, max, median, avg, sum and count.\nhistogram_aggregates:\n  - max\n  - median\n  - avg\n  - count\n\n## @param histogram_percentiles - list of strings - optional - default: [\"0.95\"]\n## @env DD_HISTOGRAM_PERCENTILES - space separated list of strings - optional - default: 0.95\n## Configure which percentiles are computed by the Agent. It must be a list of float between 0 and 1.\n## Warning: percentiles must be specified as yaml strings\nhistogram_percentiles:\n  - \"0.95\"\n\n## @param histogram_copy_to_distribution - boolean - optional - default: false\n## @env DD_HISTOGRAM_COPY_TO_DISTRIBUTION - boolean - optional - default: false\n## Copy histogram values to distributions for true global distributions (in beta)\n## Note: This increases the number of custom metrics created.\nhistogram_copy_to_distribution: false\n\n## @param histogram_copy_to_distribution_prefix - string - optional\n## @env DD_HISTOGRAM_COPY_TO_DISTRIBUTION_PREFIX - string - optional\n## A prefix to add to distribution metrics created when histogram_copy_to_distributions is true\nhistogram_copy_to_distribution_prefix: \"<PREFIX>\"\n\n## @param aggregator_stop_timeout - integer - optional - default: 2\n## @env DD_AGGREGATOR_STOP_TIMEOUT - integer - optional - default: 2\n## When stopping the agent, the Aggregator will try to flush out data ready for\n## aggregation (metrics, events, ...). Data are flushed to the Forwarder in order\n## to be sent to Datadog, therefore the Agent might take at most\n## 'aggregator_stop_timeout'+'forwarder_stop_timeout' seconds to exit.\n##\n## You can set the maximum amount of time, in seconds, allocated to the\n## Aggregator to do so. You can disable this feature by setting\n## 'aggregator_stop_timeout' to 0.\naggregator_stop_timeout: 2\n\n## @param aggregator_buffer_size - integer - optional - default: 100\n## @env DD_AGGREGATOR_BUFFER_SIZE - integer - optional - default: 100\n## The default buffer size for the aggregator use a sane value for most of the\n## use cases, however, it could be useful to manually set it in order to trade\n## RSS usage with better performances.\naggregator_buffer_size: 100\n\n## @param forwarder_timeout - integer - optional - default: 20\n## @env DD_FORWARDER_TIMEOUT - integer - optional - default: 20\n## Forwarder timeout in seconds\nforwarder_timeout: 20\n\n## @param forwarder_retry_queue_payloads_max_size - integer - optional - default: 15728640 (15MB)\n## @env DD_FORWARDER_RETRY_QUEUE_PAYLOADS_MAX_SIZE - integer - optional - default: 15728640 (15MB)\n## It defines the maximum size in bytes of all the payloads in the forwarder's retry queue.\n## The actual memory used is greater than the payloads size as there are extra fields like HTTP headers,\n## but no more than 2.5 times the payload size.\nforwarder_retry_queue_payloads_max_size: 15728640\n\n## @param forwarder_num_workers - integer - optional - default: 1\n## @env DD_FORWARDER_NUM_WORKERS - integer - optional - default: 1\n## The number of workers used by the forwarder.\nforwarder_num_workers: 1\n\n## @param forwarder_stop_timeout - integer - optional - default: 2\n## @env DD_FORWARDER_STOP_TIMEOUT - integer - optional - default: 2\n## When stopping the agent, the Forwarder will try to flush all new\n## transactions (not the ones in retry state).  New transactions will be created\n## as the Aggregator flush it's internal data too, therefore the Agent might take\n## at most 'aggregator_stop_timeout'+'forwarder_stop_timeout' seconds to exit.\n##\n## You can set the maximum amount of time, in seconds, allocated to the\n## Forwarder to send those transactions.  You can disable this feature by setting\n## 'forwarder_stop_timeout' to 0.\nforwarder_stop_timeout: 2\n\n## @param http_protocol - string - optional - default: auto\n## @env DD_FORWARDER_HTTP_PROTOCOL - string - optional - default: auto\n## The transport type to use for sending logs. Possible values are \"auto\" or \"http1\".\nforwarder_http_protocol: auto\n\n## @param forwarder_max_concurrent_requests - integer - optional - default: 10\n## @ENV DD_FORWARDER_MAX_CONCURRENT_REQUESTS - integer - optional - default: 10\n## The maximum number of concurrent requests that each worker can have queued up\n## at any one time. If the connection is over HTTP/1 each request will be waiting\n## for the previous request to complete before sending the next one. With HTTP/2\n## each request can be sent before waiting for the response.\nforwarder_max_concurrent_requests: 10\n\n## @param forwarder_storage_max_size_in_bytes - integer - optional - default: 0\n## @env DD_FORWARDER_STORAGE_MAX_SIZE_IN_BYTES - integer - optional - default: 0\n## When the retry queue of the forwarder is full, `forwarder_storage_max_size_in_bytes`\n## defines the amount of disk space the Agent can use to store transactions on the disk.\n## When `forwarder_storage_max_size_in_bytes` is `0`, the transactions are never stored on the disk.\nforwarder_storage_max_size_in_bytes: 50000000\n\n## @param forwarder_storage_max_disk_ratio - float - optional - default: 0.8\n## @env DD_FORWARDER_STORAGE_MAX_DISK_RATIO - float - optional - default: 0.8\n## `forwarder_storage_max_disk_ratio` defines the disk capacity limit for storing transactions.\n## `0.8` means the Agent can store transactions on disk until `forwarder_storage_max_size_in_bytes`\n## is reached or when the disk mount for `forwarder_storage_path` exceeds 80% of the disk capacity,\n## whichever is lower.\nforwarder_storage_max_disk_ratio: 0.8\n\n## @param forwarder_outdated_file_in_days - integer - optional - default: 10\n## @env DD_FORWARDER_OUTDATED_FILE_IN_DAYS - integer - optional - default: 10\n## This value specifies how many days the overflow transactions will remain valid before\n## being discarded. During the Agent restart, if a retry file contains transactions that were\n## created more than `forwarder_outdated_file_in_days` days ago, they are removed.\nforwarder_outdated_file_in_days: 10\n\n## @param forwarder_high_prio_buffer_size - int - optional - default: 100\n## Defines the size of the high prio buffer.\n## Increasing the buffer size can help if payload drops occur due to high prio buffer being full.\nforwarder_high_prio_buffer_size: 100\n\n## @param forwarder_low_prio_buffer_size - int - optional - default: 100\n## Defines the size of the low prio buffer.\nforwarder_low_prio_buffer_size: 100\n\n## @param forwarder_requeue_buffer_size - int - optional - default: 100\n## Defines the size of the requeue prio buffer.\nforwarder_requeue_buffer_size: 100\n\n## @param forwarder_backoff_base - int - optional - default: 2\n## @env DD_FORWARDER_BACKOFF_BASE - integer - optional - default: 2\n## Defines the rate of exponential growth, and the first retry interval range.\n## Do not set a lower value than the default. You may increase it if you use a proxy that benefits from a\n## higher rate of exponential growth.\nforwarder_backoff_base: 2\n\n## @param forwarder_backoff_max - int - optional - default: 64\n## @env DD_FORWARDER_BACKOFF_MAX - integer - optional - default: 64\n## Defines the maximum number of seconds to wait for a retry.\n## Do not set a lower value than the default. You may increase it if you use a proxy that benefits from a\n## higher maximum backoff time.\nforwarder_backoff_max: 64\n\n## @param cloud_provider_metadata - list of strings -  optional - default: [\"aws\", \"gcp\", \"azure\", \"alibaba\", \"oracle\", \"ibm\"]\n## @env DD_CLOUD_PROVIDER_METADATA - space separated list of strings - optional - default: aws gcp azure alibaba oracle ibm\n## This option restricts which cloud provider endpoint will be used by the\n## agent to retrieve metadata. By default the agent will try # AWS, GCP, Azure\n## and alibaba providers. Some cloud provider are not enabled by default to not\n## trigger security alert when querying unknown IP (for example, when enabling\n## Tencent on AWS).\n## Setting an empty list will disable querying any cloud metadata endpoints\n## (falling back on system metadata). Disabling metadata for the cloud provider in which an Agent runs may result in\n## duplicated hosts in your Datadog account and missing Autodiscovery features\n##\n## Possible values are:\n## \"aws\"     AWS EC2, ECS/Fargate\n## \"gcp\"     Google Cloud Provider\n## \"azure\"   Azure\n## \"alibaba\" Alibaba\n## \"tencent\" Tencent\n## \"oracle\"  Oracle Cloud\n## \"ibm\"     IBM Cloud\ncloud_provider_metadata:\n  - \"aws\"\n  - \"gcp\"\n  - \"azure\"\n  - \"alibaba\"\n  - \"oracle\"\n  - \"ibm\"\n\n## @param collect_ec2_tags - boolean - optional - default: false\n## @env DD_COLLECT_EC2_TAGS - boolean - optional - default: false\n## Collect AWS EC2 custom tags as host tags.\n## Requires one of:\n##  - `collect_ec2_tags_use_imds: true` and configuration of the\n##    EC2 instance to allow tags in instance metadata; or\n##  - configuration of the EC2 instance to have an IAM role with\n##    the `EC2:DescribeTags` permission.\n## See docs for further details:\n## https://docs.datadoghq.com/integrations/faq/how-do-i-pull-my-ec2-tags-without-using-the-aws-integration/\ncollect_ec2_tags: false\n\n## @param collect_ec2_instance_info - boolean - optional - default: false\n## @env DD_COLLECT_EC2_INSTANCE_INFO - boolean - optional - default: false\n## Extend host tags with AWS EC2 instance information. The added tags are:\n##  - region\n##  - instance-type\n##  - aws_account\n##  - image\n##  - availability-zone\n##\n## This should only be enabled when the Datadog AWS integration cannot be enabled (see\n## https://docs.datadoghq.com/integrations/amazon_web_services/ for more information on the AWS integration).\n## Using the AWS integration is recommended as it offers more features and a better integration with the AWS environment.\ncollect_ec2_instance_info: false\n\n## @param exclude_ec2_tags - list of strings - optional - default: []\n## @env DD_EXCLUDE_EC2_TAGS - space separated list of strings - optional - default: []\n## EC2 tags to exclude from being converted into host tags. This does not impact tags collected by the AWS Integration\n## (see https://docs.datadoghq.com/integrations/amazon_web_services/ for more information on the AWS integration).\n##\n## This requires 'collect_ec2_tags' setting to be set to true.\nexclude_ec2_tags: []\n\n## @param collect_ec2_tags_use_imds - boolean - optional - default: false\n## @env DD_COLLECT_EC2_TAGS_USE_IMDS - boolean - optional - default: false\n## Use instance metadata service (IMDS) instead of EC2 API to collect AWS EC2 custom tags.\n##\n## This requires 'collect_ec2_tags' setting to be set to true.\ncollect_ec2_tags_use_imds: false\n\n## @param ec2_metadata_timeout - integer - optional - default: 300\n## @env DD_EC2_METADATA_TIMEOUT - integer - optional - default: 300\n## Timeout in milliseconds on calls to the AWS EC2 metadata endpoints.\nec2_metadata_timeout: 300\n\n## @param ec2_prefer_imdsv2 - boolean - optional - default: false\n## @env DD_EC2_PREFER_IMDSV2 - boolean - optional - default: false\n## If this flag is true then the agent will request EC2 metadata using IMDS v2,\n## which offers additional security for accessing metadata. However, in some\n## situations (such as a containerized agent on a plain EC2 instance) it may\n## require additional configuration on the AWS side. See the AWS guidelines\n## for further details:\n## https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html#instance-metadata-transition-to-version-2\nec2_prefer_imdsv2: false\n\n## @param ec2_prioritize_instance_id_as_hostname - boolean - optional - default: false\n## @env DD_EC2_PRIORITIZE_INSTANCE_ID_AS_HOSTNAME - boolean - optional - default: false\n## On EC2, prefer the instance ID as the Agent hostname even when the OS hostname does not match\n## the known generic EC2 prefixes. By default, the Agent uses the instance ID if the OS hostname\n## has a generic EC2 prefix to avoid merging multiple instances under one host. This is useful\n## when using custom images that share the same system hostname.\nec2_prioritize_instance_id_as_hostname: false\n\n## @param collect_gce_tags - boolean - optional - default: true\n## @env DD_COLLECT_GCE_TAGS - boolean - optional - default: true\n## Collect Google Cloud Engine metadata as host tags\ncollect_gce_tags: true\n\n## @param exclude_gce_tags - list of strings - optional - default: [\"bosh_settings\" ,\"cli-cert\" ,\"common-psm1\" ,\"configure-sh\" ,\"containerd-configure-sh\" ,\"disable-address-manager\" ,\"disable-legacy-endpoints\" ,\"enable-oslogin\" ,\"gce-container-declaration\" ,\"google-container-manifest\" ,\"ipsec-cert\" ,\"k8s-node-setup-psm1\" ,\"kube-env\" ,\"kubeconfig\" ,\"kubelet-config\" ,\"serial-port-logging-enable\" ,\"shutdown-script\" ,\"ssh-keys\" ,\"sshKeys\" ,\"ssl-cert\" ,\"startup-script\" ,\"user-data\" ,\"windows-keys\" ,\"windows-startup-script-ps1\"]\n## @env DD_EXCLUDE_GCE_TAGS - space separated list of strings - optional - default: bosh_settings cli-cert common-psm1 configure-sh containerd-configure-sh disable-address-manager disable-legacy-endpoints enable-oslogin gce-container-declaration google-container-manifest ipsec-cert k8s-node-setup-psm1 kube-env kubeconfig kubelet-config serial-port-logging-enable shutdown-script ssh-keys sshKeys ssl-cert startup-script user-data windows-keys windows-startup-script-ps1\n## Google Cloud Engine metadata attribute to exclude from being converted into\n## host tags -- only applicable when collect_gce_tags is true.\nexclude_gce_tags:\n  - \"bosh_settings\"\n  - \"cli-cert\"\n  - \"common-psm1\"\n  - \"configure-sh\"\n  - \"containerd-configure-sh\"\n  - \"disable-address-manager\"\n  - \"disable-legacy-endpoints\"\n  - \"enable-oslogin\"\n  - \"gce-container-declaration\"\n  - \"google-container-manifest\"\n  - \"ipsec-cert\"\n  - \"k8s-node-setup-psm1\"\n  - \"kube-env\"\n  - \"kubeconfig\"\n  - \"kubelet-config\"\n  - \"serial-port-logging-enable\"\n  - \"shutdown-script\"\n  - \"ssh-keys\"\n  - \"sshKeys\"\n  - \"ssl-cert\"\n  - \"startup-script\"\n  - \"user-data\"\n  - \"windows-keys\"\n  - \"windows-startup-script-ps1\"\n\n## @param gce_send_project_id_tag - bool - optional - default: false\n## @env DD_GCE_SEND_PROJECT_ID_TAG - bool - optional - default: false\n## Send the project ID host tag with the `project_id:` tag key in addition to\n## the `project:` tag key.\ngce_send_project_id_tag: false\n\n## @param gce_metadata_timeout - integer - optional - default: 1000\n## @env DD_GCE_METADATA_TIMEOUT - integer - optional - default: 1000\n## Timeout in milliseconds on calls to the GCE metadata endpoints.\ngce_metadata_timeout: 1000\n\n## @param collect_gpu_tags - boolean - optional - default: true\n## @env DD_COLLECT_GPU_TAGS - boolean - optional - default: true\n## Collect GPU related host tags\ncollect_gpu_tags: false\n\n## @param azure_hostname_style - string - optional - default: \"os\"\n## @env DD_AZURE_HOSTNAME_STYLE - string - optional - default: \"os\"\n## Changes how agent hostname is set on Azure virtual machines.\n##\n## Possible values:\n##   \"os\" - use the hostname reported by the operating system (default)\n##   \"name\" - use the instance name\n##   \"name_and_resource_group\" - use a combination of the instance name and resource group name\n##   \"full\" - use a combination of the instance name, resource group name and subscription id\n##   \"vmid\" - use the instance id\nazure_hostname_style: \"os\"\n\n## @param scrubber - custom object - optional\n## Configuration for scrubbing sensitive information from the Agent's logs, configuration and flares.\nscrubber:\n  @param scrubber.additional_keys - list of strings - optional\n  @env DD_SCRUBBER_ADDITIONAL_KEYS - space-separated list of strings - optional\n  By default, the Agent removes known sensitive keys from Agent and integrations YAML configs before\n  including them in the flare.\n  Use this parameter to define additional sensitive keys that the Agent should scrub from\n  the YAML files included in the flare.\n  additional_keys:\n    - \"sensitive_key_1\"\n    - \"sensitive_key_2\"\n\n## @param no_proxy_nonexact_match - boolean - optional - default: false\n## @env DD_NO_PROXY_NONEXACT_MATCH - boolean - optional - default: false\n## Enable more flexible no_proxy matching. See https://godoc.org/golang.org/x/net/http/httpproxy#Config\n## for more information on accepted matching criteria.\nno_proxy_nonexact_match: false\n\n## @param use_proxy_for_cloud_metadata - boolean - optional - default: false\n## @env DD_USE_PROXY_FOR_CLOUD_METADATA - boolean - optional - default: false\n## By default cloud provider IP's are added to the transport's `no_proxy` list.\n## Use this parameter to remove them from the `no_proxy` list.\nuse_proxy_for_cloud_metadata: false\n\n## @param inventories_configuration_enabled - boolean - optional - default: true\n## @env DD_INVENTORIES_CONFIGURATION_ENABLED - boolean - optional - default: true\n## By default the Agent sends its own configuration to Datadog to be displayed in the `Agent Configuration` section of the host\n## detail panel. See https://docs.datadoghq.com/infrastructure/list/#agent-configuration for more information.\n##\n## The Agent configuration is scrubbed of any sensitive information.\ninventories_configuration_enabled: true\n\n## @env DD_METADATA_IP_RESOLUTION_FROM_HOSTNAME - boolean - optional - default: false\n## By default, the Agent uses the first interface in the list of network interfaces to determine the IP address of the host.\n## If you set this option to true, the Agent tries to resolve the host name to determine the host's IP address.\n## If this is unsuccessful, the Agent falls back to the default behavior.\n## This option is useful when the first interface is not the one you want to use to determine the host's IP address, or when\n##\u00a0you define the hostname in the /etc/hosts configuration file.\nmetadata_ip_resolution_from_hostname: false\n\n## @param auto_exit - custom object - optional\n## Configuration for the automatic exit mechanism: the Agent stops when some conditions are met.\nauto_exit:\n\n  @param noprocess - custom object - optional\n  Configure the `noprocess` automatic exit method.\n  Detect when no other processes (non-agent) are running to trigger automatic exit. `HOST_PROC` is taken into account when gathering processes.\n  Feature is only supported on POSIX systems.\n  noprocess:\n    @param enabled - boolean - optional - default: false\n    @env DD_AUTO_EXIT_NOPROCESS_ENABLED - boolean - optional - default: false\n    Enable the `noprocess` method\n    enabled: false\n\n    @param excluded_processes - list of strings - optional\n    @env DD_AUTO_EXIT_NOPROCESS_EXCLUDED_PROCESSES - space separated list of strings - optional\n    List of regular expressions to exclude extra processes (on top of built-in list).\n    excluded_processes: []\n\n  @param validation_period - integer - optional - default: 60\n  @env DD_AUTO_EXIT_VALIDATION_PERIOD - integer - optional - default: 60\n  Time (in seconds) delay during which the auto exit validates that the selected method continuously detects an exit condition, before exiting.\n  The value is verified every 30s. By default, three consecutive checks need to return true to trigger an automatic exit.\n  validation_period: 60\n\n## @param fips - custom object - optional\n## Uncomment this parameter and the one below to enable them.\nfips:\n\n  @param enabled - boolean - optional - default: false\n  @env DD_FIPS_ENABLED - boolean - optional - default: false\n  This feature is in BETA.\n  #\n  Enable the use of the FIPS proxy to send data to the DataDog backend. Enabling this will force all outgoing traffic\n  from the Agent to the local proxy.\n  It's important to note that enabling this will not make the Datadog Agent FIPS compliant, but will force all outgoing\n  traffic to a local FIPS compliant proxy. The FIPS proxy need to be installed locally in addition to the agent.\n  #\n  When setting this to true the following settings would be overridden, ignoring the values from the\n  configuration:\n  - dd_url\n  - apm_config.apm_dd_url\n  - apm_config.profiling_dd_url\n  - apm_config.telemetry.dd_url\n  - process_config.process_dd_url\n  - logs_config.use_http\n  - logs_config.logs_no_ssl\n  - logs_config.logs_dd_url\n  - database_monitoring.metrics.dd_url\n  - database_monitoring.activity.dd_url\n  - database_monitoring.samples.dd_url\n  - compliance_config.endpoints.dd_url\n  - runtime_security_config.endpoints.dd_url\n  - network_devices.metadata.dd_url\n  The agent will also ignore 'proxy.*' settings and environment variables related to proxy (HTTP_PROXY, HTTPS_PROXY,\n  DD_PROXY_HTTP and DD_PROXY_HTTPS).\n  enabled: false\n\n  @param local_address - string - optional - default: localhost\n  @env DD_FIPS_LOCAL_ADDRESS - string - optional - default: localhost\n  The local address that the FIPS proxy will bind ports on.\n  local_address: localhost\n\n## @param observability_pipelines_worker - custom object - optional\n## Configuration for forwarding telemetry to an Observability Pipelines Worker instead of Datadog.\n## https://www.datadoghq.com/product/observability-pipelines/\n## Note: This config is interchangeable with `vector`\nobservability_pipelines_worker:\n\n  @param  metrics - custom object - optional\n  Specific configurations for metrics\n  metrics:\n\n    @param enabled - boolean - optional - default: false\n    @env DD_OBSERVABILITY_PIPELINES_WORKER_METRICS_ENABLED - boolean - optional - default: false\n    Enables forwarding of metrics to an Observability Pipelines Worker\n    enabled: false\n\n    @param url - string - optional - default: \"\"\n    @env DD_OBSERVABILITY_PIPELINES_WORKER_METRICS_URL - string - optional - default: \"\"\n    URL endpoint for the Observability Pipelines Worker to send metrics to\n    url: \"http://127.0.0.1:8080\"\n\n  @param  logs - custom object - optional\n  Specific configurations for logs\n  logs:\n\n    @param enabled - boolean - optional - default: false\n    @env DD_OBSERVABILITY_PIPELINES_WORKER_LOGS_ENABLED - boolean - optional - default: false\n    Enables forwarding of logs to an Observability Pipelines Worker\n    enabled: false\n\n    @param url - string - optional - default: \"\"\n    @env DD_OBSERVABILITY_PIPELINES_WORKER_LOGS_URL - string - optional - default: \"\"\n    URL endpoint for the Observability Pipelines Worker to send logs to\n    url: \"http://127.0.0.1:8080\"\n\n  @param  traces - custom object - optional\n  Specific configurations for traces\n  traces:\n\n    @param enabled - boolean - optional - default: false\n    @env DD_OBSERVABILITY_PIPELINES_WORKER_TRACES_ENABLED - boolean - optional - default: false\n    Enables forwarding of traces to an Observability Pipelines Worker\n    enabled: false\n\n    @param url - string - optional - default: \"\"\n    @env DD_OBSERVABILITY_PIPELINES_WORKER_TRACES_URL - string - optional - default: \"\"\n    URL endpoint for the Observability Pipelines Worker to send traces to\n    url: \"http://127.0.0.1:8080\"\n", "advanced configuration": "\n## @param confd_path - string - optional\n## @env DD_CONFD_PATH - string - optional\n## The path containing check configuration files. By default, uses the conf.d folder\n## located in the Agent configuration folder.\nconfd_path: \"\"\n\n## @param additional_checksd - string - optional\n## @env DD_ADDITIONAL_CHECKSD - string - optional\n## Additional path indicating where to search for Python checks. By default, uses the checks.d folder\n## located in the Agent configuration folder.\nadditional_checksd: <CHECKD_FOLDER_PATH>\n\n## @param expvar_port - integer - optional - default: 5000\n## @env DD_EXPVAR_PORT - integer - optional - default: 5000\n## The port for the go_expvar server.\nexpvar_port: 5000\n\n## @param cmd_port - integer - optional - default: 5001\n## @env DD_CMD_PORT - integer - optional - default: 5001\n## The port on which the IPC api listens.\ncmd_port: 5001\n\n## @param GUI_port - integer - optional\n## @env DD_GUI_PORT - integer - optional\n## The port for the browser GUI to be served.\n## Setting 'GUI_port: -1' turns off the GUI completely\n## Default is:\n##  * Windows & macOS : `5002`\n##  * Linux: `-1`\n##\nGUI_port: <GUI_PORT>\n\n## @param GUI_session_expiration - duration - optional\n## @env GUI_SESSION_EXPIRATION - duration - optional\n## The duration after which a GUI session will expire.\n## Setting 'GUI_SESSION_EXPIRATION: 0' disable session expiration.\n## Default is \"0s\" (sessions do not expire).\nGUI_session_expiration: <SESSION_EXPIRATION_DURATION>\n\n## @param health_port - integer - optional - default: 0\n## @env DD_HEALTH_PORT - integer - optional - default: 0\n## The Agent can expose its health check on a dedicated http port.\n## This is useful for orchestrators that support http probes.\n## Default is 0 (disabled), set a valid port number (eg. 5555) to enable.\nhealth_port: 0\n\n## @param check_runners - integer - optional - default: 4\n## @env DD_CHECK_RUNNERS - integer - optional - default: 4\n## The `check_runners` refers to the number of concurrent check runners available for check instance execution.\n## The scheduler attempts to spread the instances over the collection interval and will _at most_ be\n## running the number of check runners instances concurrently.\n## Setting the value to 1 would result in checks running sequentially.\n##\n## This is a sensitive setting, and we do NOT recommend changing the default number\n## of check runners in the general case. The level of concurrency has effects on\n## the Agent's: RSS memory, CPU load, resource contention overhead, etc.\ncheck_runners: 4\n\n## @param enable_metadata_collection - boolean - optional - default: true\n## @env DD_ENABLE_METADATA_COLLECTION - boolean - optional - default: true\n## Metadata collection should always be enabled, except if you are running several\n## agents/dsd instances per host. In that case, only one Agent should have it on.\n## WARNING: disabling it on every Agent leads to display and billing issues.\nenable_metadata_collection: true\n\n## @param enable_gohai - boolean - optional - default: true\n## @env DD_ENABLE_GOHAI - boolean - optional - default: true\n## Enable the gohai collection of systems data.\nenable_gohai: true\n\n## @param enable_signing_metadata_collection - boolean - optional - default: true\n## @env DD_ENABLE_SIGNING_METADATA_COLLECTION - boolean - optional - default: true\n## Enable the Linux package signing medatada collection.\nenable_signing_metadata_collection: true\n\n## @param server_timeout - integer - optional - default: 30\n## @env DD_SERVER_TIMEOUT - integer - optional - default: 30\n## IPC api server timeout in seconds.\nserver_timeout: 30\n\n## @param procfs_path - string - optional\n## @env DD_PROCFS_PATH - string - optional\n## Some environments may have the procfs file system mounted in a miscellaneous\n## location. The procfs_path configuration parameter provides a mechanism to\n## override the standard default location: '/proc' - this setting trickles down to\n## integrations and affect their behavior if they rely on the psutil python package.\nprocfs_path: <PROCFS_PATH>\n## @param disable_py3_validation - boolean - optional - default: false\n## @env DD_DISABLE_PY3_VALIDATION - boolean - optional - default: false\n## Disable Python3 validation of python checks.\ndisable_py3_validation: false\n## @param python3_linter_timeout - integer - optional - default: 120\n## @env DD_PYTHON3_LINTER_TIMEOUT - integer - optional - default: 120\n## Timeout in seconds for validation of compatibility with python 3 when running python 2.\npython3_linter_timeout: 120\n\n## @param memtrack_enabled - boolean - optional - default: true\n## @env DD_MEMTRACK_ENABLED - boolean - optional - default: true\n## Enables tracking of memory allocations made from the python runtime loader.\nmemtrack_enabled: true\n\n## @param tracemalloc_debug - boolean - optional - default: false\n## @env DD_TRACEMALLOC_DEBUG - boolean - optional - default: false\n## Enables debugging with tracemalloc for python checks.\n## Please note that when this option is enabled the number of check runners is overridden to 1.\ntracemalloc_debug: false\n\n## @param tracemalloc_include - string - optional\n## @env DD_TRACEMALLOC_INCLUDE - string - optional\n## Comma-separated list of Python checks to enable tracemalloc for when `tracemalloc_debug` is true.\n## By default, all Python checks are enabled.\ntracemalloc_include: <TRACEMALLOC_EXCLUDE>\n\n## @param tracemalloc_exclude - string - optional\n## @env DD_TRACEMALLOC_EXCLUDE - string - optional\n## Comma-separated list of Python checks to disable tracemalloc for when `tracemalloc_debug` is true.\n## By default, all Python checks are enabled. This setting takes precedence over `tracemalloc_include`.\ntracemalloc_exclude: <TRACEMALLOC_INCLUDE>\n\n## @param windows_use_pythonpath - boolean - optional\n## @env DD_WINDOWS_USE_PYTHONPATH - boolean - optional\n## Whether to honour the value of the PYTHONPATH env var when set on Windows.\n## Disabled by default, so we only load Python libraries bundled with the Agent.\nwindows_use_pythonpath: false\n\n## @param secret_backend_type - string - optional\n## @env DD_SECRET_BACKEND_TYPE - string - optional\n##\n## `secret_backend_type` is the type of backend where secrets are stored.\n## Supported backends are: \"aws.secrets\", \"aws.ssm\", \"azure.keyvault\", \"hashicorp.vault\",\n## \"file.json\", \"file.yaml\"\n## For more information see: https://docs.datadoghq.com/agent/configuration/secrets-management\n##\n## This option is ignored if 'secret_backend_command' is set.\nsecret_backend_type: <BACKEND_TYPE>\n\n## @param secret_backend_config - map - optional\n## @env DD_SECRET_BACKEND_CONFIG - map - optional\n##\n## The section contains configuration required by `secret_backend_type` to resolve secrets.\n## The necessary configuration depends upon which type is used.\n## For more information see: https://docs.datadoghq.com/agent/configuration/secrets-management\nsecret_backend_config:\n  <KEY_1>: <VALUE_1>\n\n## @param secret_refresh_interval - integer - optional - default 0\n## @env DD_SECRET_REFRESH_INTERVAL - integer - optional - default 0\n##\n## `secret_refresh_interval` is the interval (in seconds) at which api/app key secrets are refreshed. A 0 value means the feature is disabled.\n## For more information see: https://docs.datadoghq.com/agent/configuration/secrets-management/#refreshing-apiapp-keys-at-runtime\n##\nsecret_refresh_interval: 0\n\n## @param secret_refresh_scatter - boolean - optional - default true\n## @env DD_SECRET_REFRESH_SCATTER - boolean - optional - default true\n##\n## `secret_refresh_scatter`, if set to true, will randomize the first secret refresh. `secret_refresh_interval` needs to be set\n## for this to take effect. This prevents a fleet of Agents from refreshing their secrets at the same time.\n## For more information see: https://docs.datadoghq.com/agent/configuration/secrets-management/#refreshing-apiapp-keys-at-runtime\n##\nsecret_refresh_scatter: true\n\n## @param secret_backend_command - string - optional\n## @env DD_SECRET_BACKEND_COMMAND - string - optional\n## `secret_backend_command` is the path to your custom script to execute to fetch secrets.\n## The executable must have specific rights that differ on Windows and Linux.\n##\n## This option take precedence over `secret_backend_type`.\n##\n## For more information see: https://docs.datadoghq.com/agent/configuration/secrets-management\nsecret_backend_command: <COMMAND_PATH>\n\n## @param secret_backend_arguments - list of strings - optional\n## @env DD_SECRET_BACKEND_ARGUMENTS - space separated list of strings - optional\n## If secret_backend_command is set, specify here a list of arguments to give to the command at each run.\n##\n## This option take precedence over `secret_backend_type`.\nsecret_backend_arguments:\n  - <ARGUMENT_1>\n  - <ARGUMENT_2>\n\n## @param secret_backend_output_max_size - integer - optional - default: 1048576\n## @env DD_SECRET_BACKEND_OUTPUT_MAX_SIZE - integer - optional - default: 1048576\n## The size in bytes of the buffer used to store the command answer (apply to both stdout and stderr)\nsecret_backend_output_max_size: 1048576\n\n## @param secret_backend_timeout - integer - optional - default: 30\n## @env DD_SECRET_BACKEND_TIMEOUT - integer - optional - default: 30\n## The timeout to execute the command in second\nsecret_backend_timeout: 30\n\n## @param secret_backend_skip_checks - boolean - optional - default: false\n## @env DD_SECRET_BACKEND_SKIP_CHECKS - boolean - optional - default: false\n## Disable fetching secrets for check configurations\nsecret_backend_skip_checks: false\n\n## @param secret_backend_remove_trailing_line_break - boolean - optional - default: false\n## @env DD_SECRET_BACKEND_REMOVE_TRAILING_LINE_BREAK - boolean - optional - default: false\n## Remove trailing line breaks from secrets returned by the secret_backend_command. Some secret management tools automatically\n## add a line break when exporting secrets through files.\nsecret_backend_remove_trailing_line_break: false\n\n## @param secret_scope_integration_to_their_k8s_namespace - boolean - optional - default: false\n## @env DD_SECRET_SCOPE_INTEGRATION_TO_THEIR_NAMESPACE - boolean - optional - default: false\n## When using `k8s_secret@` notation to pull secrets from Kubernetes, you can limit integration to only be able to access\n## their own namespace by setting this setting to true.\n## The limitation will only apply integration's config and secret using `k8s_secret@` secrets.\n##\n## This setting is incompatible with 'secret_allowed_k8s_namespace' and 'secret_image_to_handle'\n##\n## See https://docs.datadoghq.com/agent/configuration/secrets-management for more information on using Kubernetes\n## secrets in your configuration.\nsecret_scope_integration_to_their_k8s_namespace: false\n\n## @param secret_allowed_k8s_namespace - list of strings - optiona - default: []\n## @env DD_SECRET_ALLOWED_K8S_NAMESPACE - list of strings - optional - default: []\n## When using `k8s_secret@` notation to pull secrets from Kubernetes, you can limit all integrations to only be able to access\n## a list of specific namespace.\n## The limitation will only apply integration's config and secret using `k8s_secret@` secrets.\n##\n## This setting is incompatible with 'secret_scope_integration_to_their_k8s_namespace' and 'secret_image_to_handle'\n##\n## See https://docs.datadoghq.com/agent/configuration/secrets-management for more information on using Kubernetes\n## secrets in your configuration.\nsecret_allowed_k8s_namespace: []\n\n## @param secret_image_to_handle - map of list of strings - optiona - default:  {}\n## @env DD_SECRET_IMAGE_TO_HANDLE - map of list of strings - optional - default: {}\n## When using `k8s_secret@` notation to pull secrets from Kubernetes, you can fully control which Kubernetes secret can\n## be access by which image. Any secrets using the `k8s_secret@` prefix not listed for the image being monitored will\n## not be resolved.\n## The limitation will only apply integration's config and secret using `k8s_secret@` secrets.\n##\n## This setting is incompatible with 'secret_scope_integration_to_their_k8s_namespace' and 'secret_image_to_handle'\n##\n## See https://docs.datadoghq.com/agent/configuration/secrets-management for more information on using Kubernetes\n## secrets in your configuration.\nsecret_image_to_handle:\n  webserver:\n    - \"k8s_secret@prod/weserver/db_password\"\n    - \"k8s_secret@prod/weserver/db_password\"\n  <Kubernetes image name>:\n    - \"<full secret handle>\"\n- ...## @param profiling - custom object - optional\n## Enter specific configurations for internal profiling.\n##\n## Please note that:\n##   1. This does *not* enable profiling for user applications.\n##   2. This only enables internal profiling of the agent go runtime.\n##   3. To enable profiling for user apps please refer to\n##      https://docs.datadoghq.com/tracing/profiling/\n##   4. Enabling this feature will incur in billing charges and other\n##      unexpected side-effects (ie. agent profiles showing with your\n##      services).\n##\n## Uncomment this parameter and the one below to enable profiling.\ninternal_profiling:\n  @param enabled - boolean - optional - default: false\n  @env DD_INTERNAL_PROFILING_ENABLED - boolean - optional - default: false\n  Enable internal profiling for the Agent process.\n  enabled: false\n", "datadog agent manager system tray configuration": "\n## @param system_tray - custom object - optional\n## This section configures the Datadog Agent Manager System Tray\nsystem_tray:\n  @param log_file - string - optional - default: %ProgramData%\\Datadog\\logs\\ddtray.log\n  @env DD_TRAY_LOG_FILE - string - optional\n  The full path to the file where Datadog Agent Manager System Tray logs are written.\n  log_file: <TRAY_LOG_FILE_PATH>\n", "log collection configuration": "\n## @param logs_enabled - boolean - optional - default: false\n## @env DD_LOGS_ENABLED - boolean - optional - default: false\n## Enable Datadog Agent log collection by setting logs_enabled to true.\nlogs_enabled: false\n\n## @param logs_config - custom object - optional\n## Enter specific configurations for your Log collection.\n## Uncomment this parameter and the one below to enable them.\n## See https://docs.datadoghq.com/agent/logs/\nlogs_config:\n\n  @param container_collect_all - boolean - optional - default: false\n  @env DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL - boolean - optional - default: false\n  Enable container log collection for all the containers (see ac_exclude to filter out containers)\n  container_collect_all: false\n\n  @param logs_dd_url - string - optional\n  @env DD_LOGS_CONFIG_LOGS_DD_URL - string - optional\n  Define the endpoint and port to hit when using a proxy for logs.\n  As of agent version 7.70.0, proxy paths are supported. To forward logs to a\n  specific proxy path, the URL scheme must be specified: https://proxy.example.com:443/logs\n  logs_dd_url: <ENDPOINT>:<PORT>\n\n  @param logs_no_ssl - boolean - optional - default: false\n  @env DD_LOGS_CONFIG_LOGS_NO_SSL - optional - default: false\n  Disable the SSL encryption. This parameter should only be used when logs are\n  forwarded locally to a proxy. It is highly recommended to then handle the SSL encryption\n  on the proxy side.\n  logs_no_ssl: false\n\n  @param processing_rules - list of custom objects - optional\n  @env DD_LOGS_CONFIG_PROCESSING_RULES - list of custom objects - optional\n  Global processing rules that are applied to all logs. The available rules are\n  \"exclude_at_match\", \"include_at_match\" and \"mask_sequences\". More information in Datadog documentation:\n  https://docs.datadoghq.com/agent/logs/advanced_log_collection/#global-processing-rules\n  processing_rules:\n    - type: <RULE_TYPE>\n      name: <RULE_NAME>\n      pattern: <RULE_PATTERN>\n\n  @param auto_multi_line_detection - boolean - optional - default: false\n  @env DD_LOGS_CONFIG_AUTO_MULTI_LINE_DETECTION - boolean - optional - default: false\n  Enable automatic aggregation of multi-line logs for common log patterns.\n  More information can be found in Datadog documentation:\n  https://docs.datadoghq.com/agent/logs/auto_multiline_detection/?tab=configurationfile\n  auto_multi_line_detection: true\n\n  @param force_use_http - boolean - optional - default: false\n  @env DD_LOGS_CONFIG_FORCE_USE_HTTP - boolean - optional - default: false\n  Set this parameter to `true` to always send logs via HTTP(S) protocol and never fall back to\n  raw TCP forwarding (recommended).\n  #\n  By default, the Agent sends logs in HTTPS batches if HTTPS connectivity can\n  be established at Agent startup, and falls back to TCP otherwise. This parameter\n  can be used to override this fallback behavior. It is recommended, but not the default, to\n  maintain compatibility with previous Agent versions.\n  #\n  Note, the logs are forwarded via HTTPS (encrypted) by default. Please use `logs_no_ssl` if you\n  need unencrypted HTTP instead.\n  force_use_http: true\n\n  @param http_protocol - string - optional - default: auto\n  @env DD_LOGS_CONFIG_HTTP_PROTOCOL - string - optional - default: auto\n  The transport type to use for sending logs. Possible values are \"auto\" or \"http1\".\n  http_protocol: auto\n\n  @param http_timeout - integer - optional - default: 10\n  @env DD_LOGS_CONFIG_HTTP_TIMEOUT - integer - optional - default: 10\n  The HTTP timeout to use for sending logs, in seconds.\n  http_timeout: 10\n\n  @param force_use_tcp - boolean - optional - default: false\n  @env DD_LOGS_CONFIG_FORCE_USE_TCP - boolean - optional - default: false\n  By default, logs are sent via HTTP protocol if possible, set this parameter\n  to `true` to always send logs via TCP. If `force_use_http` is set to `true`, this parameter\n  is ignored.\n  force_use_tcp: true\n\n  @param use_compression - boolean - optional - default: true\n  @env DD_LOGS_CONFIG_USE_COMPRESSION - boolean - optional - default: true\n  This parameter is available when sending logs via HTTP protocol. If enabled, the Agent\n  compresses logs before sending them.\n  use_compression: true\n\n  @param compression_level - integer - optional - default: 6\n  @env DD_LOGS_CONFIG_COMPRESSION_LEVEL - boolean - optional - default: false\n  The compression_level parameter accepts values from 0 (no compression)\n  to 9 (maximum compression but higher resource usage). Only takes effect if\n  `use_compression` is set to `true`.\n  compression_level: 6\n\n  @param batch_wait - integer - optional - default: 5\n  @env DD_LOGS_CONFIG_BATCH_WAIT - integer - optional - default: 5\n  The maximum time (in seconds) the Datadog Agent waits to fill each batch of logs before sending.\n  batch_wait: 5\n\n  @param close_timeout - integer - optional - default: 60\n  @env DD_LOGS_CONFIG_CLOSE_TIMEOUT - integer - optional - default: 60\n  The maximum number of seconds the Agent spends reading from a file after it has been rotated.\n  close_timeout: 60\n\n  @param open_files_limit - integer - optional - default: 500\n  @env DD_LOGS_CONFIG_OPEN_FILES_LIMIT - integer - optional - default: 500\n  The maximum number of files that can be tailed in parallel.\n  Note: the default for Mac OS is 200. The default for\n  all other systems is 500.\n  open_files_limit: 500\n\n  @param file_wildcard_selection_mode - string - optional - default: `by_name`\n  @env DD_LOGS_CONFIG_FILE_WILDCARD_SELECTION_MODE - string - optional - default: `by_name`\n  The strategy used to prioritize wildcard matches if they exceed the open file limit.\n  #\n  Choices are `by_name` and `by_modification_time`.\n  #\n  `by_name` means that each log source is considered and the matching files are ordered\n  in reverse name order. While there are less than `logs_config.open_files_limit` files\n  being tailed, this process repeats, collecting from each configured source.\n  #\n  `by_modification_time` takes all log sources and first adds any log sources that\n  point to a specific file. Next, it finds matches for all wildcard sources.\n  This resulting list is ordered by which files have been most recently modified\n  and the top `logs_config.open_files_limit` most recently modified files are\n  chosen for tailing.\n  #\n  WARNING: `by_modification_time` is less performant than `by_name` and will trigger\n  more disk I/O at the configured wildcard log paths.\n  file_wildcard_selection_mode: by_name\n\n  @param max_message_size_bytes - integer - optional - default: 900000\n  @env DD_LOGS_CONFIG_MAX_MESSAGE_SIZE_BYTES - integer - optional - default : 900000\n  The maximum size of single log message in bytes. If maxMessageSizeBytes exceeds\n  the documented API limit of 1MB - any payloads larger than 1MB will be dropped by the intake.\n  https://docs.datadoghq.com/api/latest/logs/\n  max_message_size_bytes: 900000\n\n  @param integrations_logs_files_max_size - integer - optional - default: 10\n  @env DD_LOGS_CONFIG_INTEGRATIONS_LOGS_FILES_MAX_SIZE - integer - optional - default: 10\n  The max size in MB that an integration logs file is allowed to use\n  integrations_logs_files_max_size: 10\n\n  @param integrations_logs_total_usage - integer - optional - default: 100\n  @env DD_LOGS_CONFIG_INTEGRATIONS_LOGS_TOTAL_USAGE - integer - optional - default: 100\n  The total combined usage all integrations logs files can use\n  integrations_logs_total_usage: 100\n\n  @param kublet_api_client_read_timeout - duration - optional - default: 30s\n  @env DD_LOGS_CONFIG_KUBELET_API_CLIENT_READ_TIMEOUT - duration - optional - default: 30s\n  Configure the kubelet API client's timeout used while streaming logs.\n  kublet_api_client_read_timeout: 30\n\n  @param k8s_container_use_kubelet_api - boolean - optional - default: false\n  @env DD_LOGS_CONFIG_K8S_CONTAINER_USE_KUBELET_API - boolean - optional - default: false\n  Enable container log collection via the kubelet API, typically used for EKS Fargate\n  k8s_container_use_kubelet_api: false\n\n  @param streaming - custom object - optional\n  This section allows you to configure streaming logs via remote config.\n  streaming:\n    @param streamlogs_log_file - string - optional\n    @env DD_LOGS_CONFIG_STREAMING_STREAMLOGS_LOG_FILE - string - optional\n    Path to the file containing the streamlogs log file.\n    Default paths:\n      * Windows: c:\\\\programdata\\\\datadog\\\\logs\\\\streamlogs_info\\\\streamlogs.log\n      * Unix: /opt/log/datadog/streamlogs_info/streamlogs.log\n      * Linux: /var/log/datadog/streamlogs_info/streamlogs.log\n    streamlogs_log_file: <path_to_streamlogs_log_file>\n", "trace collection configuration": "\n## @param apm_config - custom object - optional\n## Enter specific configurations for your trace collection.\n## Uncomment this parameter and the one below to enable them.\n## See https://docs.datadoghq.com/agent/apm/\napm_config:\n\n  @param enabled - boolean - optional - default: true\n  @env DD_APM_ENABLED - boolean - optional - default: true\n  Set to true to enable the APM Agent.\n  enabled: true\n\n  @param env - string - optional - default: none\n  @env DD_APM_ENV - string - optional - default: none\n  The environment tag that Traces should be tagged with.\n  If not set the value will be inherited, in order, from the top level\n  \"env\" config option if set and then from the 'env:' tag if present in the\n  'tags' top level config option.\n  env: none\n\n  @param receiver_port - integer - optional - default: 8126\n  @env DD_APM_RECEIVER_PORT - integer - optional - default: 8126\n  The port that the trace receiver should listen on.\n  Set to 0 to disable the HTTP receiver.\n  receiver_port: 8126\n  Please note that UDS receiver is not available in Windows.\n  @ Enabling this setting may result in unexpected behavior.\n  @param receiver_socket - string - optional - default: \"\"\n  @env DD_APM_RECEIVER_SOCKET - string - optional - default: \"\"\n  Accept traces through Unix Domain Sockets.\n  Set to \"\" to disable the UDS receiver.\n  receiver_socket: \"\"\n  @param receiver_socket - string - optional - default: unix:///var/run/datadog/apm.socket\n  @env DD_APM_RECEIVER_SOCKET - string - optional - default: unix:///var/run/datadog/apm.socket\n  Accept traces through Unix Domain Sockets.\n  Set to \"\" to disable the UDS receiver.\n  receiver_socket: /var/run/datadog/apm.socket\n  @param apm_non_local_traffic - boolean - optional - default: false\n  @env DD_APM_NON_LOCAL_TRAFFIC - boolean - optional - default: false\n  Set to true so the Trace Agent listens for non local traffic,\n  i.e if Traces are being sent to this Agent from another host/container\n  apm_non_local_traffic: false\n\n  @param apm_dd_url - string - optional\n  @env DD_APM_DD_URL - string - optional\n  Define the endpoint and port to hit when using a proxy for APM. The traces are forwarded in TCP\n  therefore the proxy must be able to handle TCP connections.\n  apm_dd_url: <ENDPOINT>:<PORT>\n\n  DEPRECATED - please use `target_traces_per_second` instead.\n  @param max_traces_per_second - integer - optional - default: 10\n  @env DD_APM_MAX_TPS - integer - optional - default: 10\n  The target traces per second to sample. Sampling rates to apply are adjusted given\n  the received traffic and communicated to tracers. This configures head base sampling.\n  As of 7.35.0 sampling cannot be disabled and setting 'max_traces_per_second' to 0 no longer\n  disables sampling, but instead sends no traces to the intake. To avoid rate limiting, set this\n  value sufficiently high for your traffic pattern.\n  max_traces_per_second: 10\n\n  @param target_traces_per_second - integer - optional - default: 10\n  @env DD_APM_TARGET_TPS - integer - optional - default: 10\n  The target traces per second to sample. Sampling rates to apply are adjusted given\n  the received traffic and communicated to tracers. This configures head-based sampling.\n  As of 7.35.0 sampling cannot be disabled and setting 'max_traces_per_second' to 0 no longer\n  disables sampling, but instead sends no traces to the intake. To avoid rate limiting, set this\n  value sufficiently high for your traffic pattern.\n  target_traces_per_second: 10\n\n  @param errors_per_second - integer - optional - default: 10\n  @env DD_APM_ERROR_TPS - integer - optional - default: 10\n  The target error trace chunks to receive per second. The TPS is spread\n  to catch all combinations of service, name, resource, http.status, and error.type.\n  Set to 0 to disable the errors sampler.\n  errors_per_second: 10\n\n  @param max_events_per_second - integer - optional - default: 200\n  @env DD_APM_MAX_EPS - integer - optional - default: 200\n  Maximum number of APM events per second to sample.\n  max_events_per_second: 200\n\n  @param max_memory - integer - optional - default: 500000000\n  @env DD_APM_MAX_MEMORY - integer - optional - default: 500000000\n  This value is what the Agent aims to use in terms of memory. If surpassed, the API\n  rate limits incoming requests to aim and stay below this value.\n  Note: The Agent process is killed if it uses more than 150% of `max_memory`.\n  Set the `max_memory` parameter to `0` to disable the memory limitation.\n  max_memory: 500000000\n\n  @param max_cpu_percent - integer - optional - default: 50\n  @env DD_APM_MAX_CPU_PERCENT - integer - optional - default: 50\n  The CPU percentage that the Agent aims to use. If surpassed, the API rate limits\n  incoming requests to aim and stay below this value. Examples: 50 = half a core, 200 = two cores.\n  Set `max_cpu_percent` to `0` to disable rate limiting based on CPU usage.\n  max_cpu_percent: 50\n\n  @param obfuscation - object - optional\n  Defines obfuscation rules for sensitive data.\n  See https://docs.datadoghq.com/tracing/setup_overview/configure_data_security/#agent-trace-obfuscation\n  obfuscation:\n    credit_cards:\n      @param DD_APM_OBFUSCATION_CREDIT_CARDS_ENABLED - boolean - optional\n      Enables obfuscation rules for credit cards. Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_CREDIT_CARDS_LUHN - boolean - optional\n      Enables a Luhn checksum check in order to eliminate false negatives. Disabled by default.\n      luhn: false\n      @param DD_APM_OBFUSCATION_CREDIT_CARDS_KEEP_VALUES - object - optional\n      List of keys that should not be obfuscated.\n      keep_values:\n        - client_id\n    elasticsearch:\n      @param DD_APM_OBFUSCATION_ELASTICSEARCH_ENABLED - boolean - optional\n      Enables obfuscation rules for spans of type \"elasticsearch\". Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_ELASTICSEARCH_KEEP_VALUES - object - optional\n      List of keys that should not be obfuscated.\n      keep_values:\n        - client_id\n      @param DD_APM_OBFUSCATION_ELASTICSEARCH_OBFUSCATE_SQL_VALUES - boolean - optional\n      The set of keys for which their values will be passed through SQL obfuscation\n      obfuscate_sql_values:\n        - val1\n    opensearch:\n      @param DD_APM_OBFUSCATION_OPENSEARCH_ENABLED - boolean - optional\n      Enables obfuscation rules for spans of type \"opensearch\". Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_OPENSEARCH_KEEP_VALUES - object - optional\n      List of keys that should not be obfuscated.\n      keep_values:\n        - client_id\n      @param DD_APM_OBFUSCATION_OPENSEARCH_OBFUSCATE_SQL_VALUES - boolean - optional\n      The set of keys for which their values will be passed through SQL obfuscation\n      obfuscate_sql_values:\n        - val1\n    http:\n      @param DD_APM_OBFUSCATION_HTTP_REMOVE_QUERY_STRING - boolean - optional\n      Enables obfuscation of query strings in URLs\n      remove_query_string: false\n      @param DD_APM_OBFUSCATION_HTTP_REMOVE_PATHS_WITH_DIGITS - boolean - optional\n      If enabled, path segments in URLs containing digits are replaced by \"?\"\n      remove_paths_with_digits: false\n    memcached:\n      @param DD_APM_OBFUSCATION_MEMCACHED_ENABLED - boolean - optional\n      Enables obfuscation rules for spans of type \"memcached\". Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_MEMCACHED_KEEP_COMMAND - boolean - optional\n      If enabled, the full command for the query will be kept, including any lookup\n      keys that could be present. The value for storage commands will still be\n      redacted if Memcached obfuscation is enabled.\n      keep_command: false\n    mongodb:\n      @param DD_APM_OBFUSCATION_MONGODB_ENABLED - boolean - optional\n      Enables obfuscation rules for spans of type \"mongodb\". Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_MONGODB_KEEP_VALUES - object - optional\n      List of keys that should not be obfuscated.\n      keep_values:\n        - document_id\n      @param DD_APM_OBFUSCATION_MONGODB_OBFUSCATE_SQL_VALUES - object - optional\n      The set of keys for which their values will be passed through SQL obfuscation\n      obfuscate_sql_values:\n        - val1\n    redis:\n      @param DD_APM_OBFUSCATION_REDIS_ENABLED - boolean - optional\n      Enables obfuscation rules for spans of type \"redis\". Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_REDIS_REMOVE_ALL_ARGS - boolean - optional\n      When true, replaces all arguments of a redis command with a single \"?\". Disabled by default.\n      remove_all_args: false\n    valkey:\n      @param DD_APM_OBFUSCATION_VALKEY_ENABLED - boolean - optional\n      Enables obfuscation rules for spans of type \"valkey\". Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_VALKEY_REMOVE_ALL_ARGS - boolean - optional\n      When true, replaces all arguments of a valkey command with a single \"?\". Disabled by default.\n      remove_all_args: false\n    @param DD_APM_OBFUSCATION_REMOVE_STACK_TRACES - boolean - optional\n    Enables removing stack traces to replace them with \"?\". Disabled by default.\n    remove_stack_traces: false\n    sql_exec_plan:\n      @param DD_APM_OBFUSCATION_SQL_EXEC_PLAN_ENABLED - boolean - optional\n      Enables obfuscation rules for JSON query execution plans. Disabled by default.\n      enabled: false\n      @param DD_APM_OBFUSCATION_SQL_EXEC_PLAN_KEEP_VALUES - object - optional\n      List of keys that should not be obfuscated.\n      keep_values:\n        - id1\n      @param DD_APM_OBFUSCATION_SQL_EXEC_PLAN_OBFUSCATE_SQL_VALUES - boolean - optional\n      The set of keys for which their values will be passed through SQL obfuscation\n      obfuscate_sql_values:\n        - val1\n    sql_exec_plan_normalize:\n      @param DD_APM_OBFUSCATION_SQL_EXEC_PLAN_NORMALIZE_ENABLED - boolean - optional\n      Enables obfuscation rules for JSON query execution plans, including cost and row estimates.\n      Produces a normalized execution plan. Disabled by default.\n      enabled: false\n      @param DD_APM_OBFUSCATION_SQL_EXEC_PLAN_NORMALIZE_KEEP_VALUES - object - optional\n      List of keys that should not be obfuscated.\n      keep_values:\n        - id1\n      @param DD_APM_OBFUSCATION_SQL_EXEC_PLAN_NORMALIZE_OBFUSCATE_SQL_VALUES - boolean - optional\n      The set of keys for which their values will be passed through SQL obfuscation\n      obfuscate_sql_values:\n        - val1\n    cache:\n      @param DD_APM_OBFUSCATION_CACHE_ENABLED - boolean - optional\n      Enables caching obfuscated statements. Currently supported for SQL and MongoDB queries.\n      Enabled by default.\n      enabled: true\n      @param DD_APM_OBFUSCATION_CACHE_MAX_SIZE - integer - optional - default: 5000000\n      The maximum size of the cache in bytes. The maximum allowed resource length is 5000.\n      Datadog stores a minimum of 1000 queries (5000000 / 5000) by default.\n      max_size: 5000000\n\n  @sql_obfuscation_mode - string - optional - default: \"\"\n  @env DD_APM_SQL_OBFUSCATION_MODE - string - optional - default: \"\"\n  Obfuscator mode for SQL queries.\n  Leave empty to use the default obfuscator.\n  Set to \"obfuscate_only\" to obfuscate the query with the new `sqllexer` obfuscator.\n  Set to \"normalize_only\" to normalize the query with the new `sqllexer` obfuscator.\n  If you use DBM, set to \"obfuscate_and_normalize\" to obfuscate and normalize the query for better APM/DBM correlation.\n  sql_obfuscation_mode: \"\"\n\n  @param filter_tags - object - optional\n  @env DD_APM_FILTER_TAGS_REQUIRE - object - optional\n  @env DD_APM_FILTER_TAGS_REJECT - object - optional\n  Defines rules by which to filter traces based on tags.\n   * require - list of key or key/value strings - traces must have those tags in order to be sent to Datadog\n   * reject - list of key or key/value strings - traces with these tags are dropped by the Agent\n  Please note that:\n    1. Rules take into account the intersection of tags defined.\n    2. When `filter_tags` and `filter_tags_regex` are used at the same time, all rules are united for filtering.\n       In cases where rules in `filter_tags` and `filter_tags_regex` match the same key, the rule from `filter_tags`\n       takes precendence over the rule from `filter_tags_regex`.\n  #\n       For example, in the case of the following configuration:\n         filter_tags:\n           require: [\"foo:bar\"]\n         filter_tags_regex:\n           require: [\"foo:^bar[0-9]{1}$\"]\n       With these rules, traces with a tag `foo:bar1` will be dropped, and those with a `foo:bar` tag will be kept\n  filter_tags:\n    require: [<LIST_OF_KEY_VALUE_TAGS>]\n    reject: [<LIST_OF_KEY_VALUE_TAGS>]\n\n  @param filter_tags_regex - object - optional\n  Defines rules by which to filter traces based on tags with regex pattern for tag values.\n   * require - list of key or key/value regex pattern strings - traces must have those tags in order to be sent to Datadog\n   * reject - list of key or key/value regex pattern strings - traces with these tags are dropped by the Agent\n  Note: Rules take into account the intersection of tags defined.\n  Using regexp patterns for tag filtering can have performance implications, and is slower than typical tag filtering\n  without regexp. However, this regexp is only run on the root span of a trace, so should not have a critical impact\n  on overall performance.\n  More detailed information can be found in the description of the `filter_tags` parameter above\n  filter_tags_regex:\n    require: [<LIST_OF_KEY_VALUE_REGEX_TAGS>]    e.g. [\"<key>:<regex>\"]\n    reject: [<LIST_OF_KEY_VALUE_REGEX_TAGS>]     e.g. [\"<key>:<regex>\"]\n\n  @param replace_tags - list of objects - optional\n  @env DD_APM_REPLACE_TAGS  - list of objects - optional\n  Defines a set of rules to replace or remove certain resources, tags containing\n  potentially sensitive information.\n  Each rules has to contain:\n   * name - string - The tag name to replace, for resources use \"resource.name\".\n   * pattern - string - The pattern to match the desired content to replace\n   * repl - string - what to inline if the pattern is matched\n  #\n  See https://docs.datadoghq.com/tracing/setup_overview/configure_data_security/#replace-rules-for-tag-filtering\n  #\n  replace_tags:\n    - name: \"<TAG_NAME>\"\n      pattern: \"<REGEX_PATTERN>\"\n      repl: \"<PATTERN_TO_INLINE>\"\n\n  @param ignore_resources - list of strings - optional\n  @env DD_APM_IGNORE_RESOURCES - comma separated list of strings - optional\n  An exclusion list of regular expressions can be provided to disable certain traces based on their resource name\n  all entries must be surrounded by double quotes and separated by commas.\n  ignore_resources: [\"(GET|POST) /healthcheck\"]\n\n  @param log_file - string - optional\n  @env DD_APM_LOG_FILE - string - optional\n  The full path to the file where APM-agent logs are written.\n  log_file: <APM_LOG_FILE_PATH>\n\n  @param connection_limit - integer - default: 2000\n  @env DD_APM_CONNECTION_LIMIT - integer - default: 2000\n  The APM connection limit for the Agent.\n  See https://docs.datadoghq.com/tracing/troubleshooting/agent_rate_limits/#max-connection-limit\n  connection_limit: 2000\n\n  @param compute_stats_by_span_kind - bool - default: true\n  @env DD_APM_COMPUTE_STATS_BY_SPAN_KIND - bool - default: true\n  Enables an additional stats computation check on spans to see they have an eligible `span.kind` (server, consumer, client, producer).\n  If enabled, a span with an eligible `span.kind` will have stats computed. If disabled, only top-level and measured spans will have stats computed.\n  NOTE: For stats computed from OTel traces, only top-level spans are considered when this option is off.\n  If you are sending OTel traces and want stats on non-top-level spans, this flag will need to be enabled.\n  If you are sending OTel traces and do not want stats computed by span kind, you need to disable this flag and remove the \"enable_otlp_compute_top_level_by_span_kind\" APM feature if present.\n  compute_stats_by_span_kind: true\n\n  @param peer_service_aggregation - bool - default: true\n  @env DD_APM_PEER_SERVICE_AGGREGATION - bool - default: true\n  DEPRECATED - please use `peer_tags_aggregation` instead.\n  peer_service_aggregation: true\n\n  @param peer_tags_aggregation - bool - default: true\n  @env DD_APM_PEER_TAGS_AGGREGATION - bool - default: true\n  Enables aggregation of peer related tags (e.g., `peer.service`, `db.instance`, etc.) in the Agent.\n  If disabled, aggregated trace stats will not include these tags as dimensions on trace metrics.\n  For the best experience with peer tags, Datadog also recommends enabling `compute_stats_by_span_kind`.\n  If you are using an OTel tracer, it's best to have both enabled because client/producer spans with relevant peer tags\n  may not be marked by the Agent as top-level spans.\n  If enabling both causes the Agent to consume too many resources, try disabling `compute_stats_by_span_kind` first.\n  A high cardinality of peer tags or APM resources can also contribute to higher CPU and memory consumption.\n  You can check for the cardinality of these fields by making trace search queries in the Datadog UI.\n  The default list of peer tags can be found in pkg/trace/stats/concentrator.go.\n  peer_tags_aggregation: true\n\n  @param peer_tags - list of strings - optional\n  @env DD_APM_PEER_TAGS - list of strings - optional\n  Optional list of supplementary peer tags that go beyond the defaults. The Datadog backend validates all tags\n  and will drop ones that are unapproved.\n  peer_tags: []\n\n  @param features - list of strings - optional\n  @env DD_APM_FEATURES - comma separated list of strings - optional\n  Configure additional beta APM features.\n  The list of items available under apm_config.features is not guaranteed to persist across versions;\n  a feature may eventually be promoted to its own configuration option on the agent, or dropped entirely.\n  features: [\"error_rare_sample_tracer_drop\",\"table_names\",\"component2name\",\"sqllexer\",\"enable_otlp_compute_top_level_by_span_kind\",\"disable_receive_resource_spans_v2\", \"disable_operation_and_resource_name_logic_v2\"]\n\n  @param additional_endpoints - object - optional\n  @env DD_APM_ADDITIONAL_ENDPOINTS - object - optional\n  Enables sending data to multiple endpoints and/or with multiple API keys via dual shipping.\n  See https://docs.datadoghq.com/agent/guide/dual-shipping\n  additional_endpoints:\n    \"https://trace.agent.datadoghq.com\":\n      - apikey2\n      - apikey3\n    \"https://trace.agent.datadoghq.eu\":\n      - apikey4\n\n  @param debug - custom object - optional\n  Specifies settings for the debug server of the trace agent.\n  debug:\n\n    @param port - integer - optional - default: 5012\n    @env DD_APM_DEBUG_PORT - string - optional - default: 5012\n    Port for the debug endpoints for the trace Agent. Set it to 0 to disable the server.\n    port: 5012\n\n  @param instrumentation - custom object - optional\n  Specifies settings for Single Step Instrumentation.\n  instrumentation:\n\n    @param enabled - boolean - default: false\n    @env DD_APM_INSTRUMENTATION_ENABLED - boolean - default: false\n    Enables Single Step Instrumentation in the cluster (in beta)\n    enabled: false\n\n    @param enabled_namespaces - list of strings - optional\n    @env DD_APM_INSTRUMENTATION_ENABLED_NAMESPACES - space separated list of strings - optional\n    Enables Single Step Instrumentation in specific namespaces, while Single Step Instrumentation is off in the whole cluster (in beta)\n    Can only be set if DD_APM_INSTRUMENTATION_ENABLED is false. Cannot be set together with DD_APM_INSTRUMENTATION_DISABLED_NAMESPACES.\n    enabled_namespaces:\n      - ns1\n      - apps\n\n    @param disabled_namespaces - list of strings - optional\n    @env DD_APM_INSTRUMENTATION_DISABLED_NAMESPACES - space separated list of strings - optional\n    Disables Single Step Instrumentation in specific namespaces, while Single Step Instrumentation is enabled in the whole cluster (in beta)\n    Can only be set if DD_APM_INSTRUMENTATION_ENABLED is true. Cannot be set together with DD_APM_INSTRUMENTATION_ENABLED_NAMESPACES.\n    disabled_namespaces:\n      - ns2\n      - system-ns\n\n  @param trace_buffer - integer - optional - default: 0\n  @env DD_APM_TRACE_BUFFER - integer - optional - default: 0\n  #\n  WARNING: Do not use this config. It is here for debugging and\n  as a temporary fix in certain load scenarios. Setting this\n  results in a performance deterioration and an increase in memory\n  usage when the Trace Agent is under load. This config may be\n  removed in a future version.\n  #\n  Specifies the number of trace payloads to buffer after decoding.\n  Traces can be buffered when receiving traces faster than the\n  processor can process them.\n  #\n  trace_buffer: 0\n\n  @param probabilistic_sampler - object - optional\n  Enables and configures the Probabilistic Sampler, compatible with the\n  OTel Probabilistic Sampler Processor ( https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/probabilisticsamplerprocessor#probabilistic-sampling-processor )\n  #\n  probabilistic_sampler:\n\n    @env DD_APM_PROBABILISTIC_SAMPLER_ENABLED - boolean - optional - default: false\n    Enables or disables the probabilistic sampler\n    enabled: false\n    @env DD_APM_PROBABILISTIC_SAMPLER_SAMPLING_PERCENTAGE - float - optional - default: 0\n    Samples this percentage (0-100) of traffic\n    sampling_percentage: 0\n    @env DD_APM_PROBABILISTIC_SAMPLER_HASH_SEED - integer - optional - default: 0\n    hash_seed: A seed used for the hash algorithm. This must match other agents and OTel\n               collectors using the probabilistic sampler to ensure consistent sampling.\n    hash_seed: 0\n\n  @param error_tracking_standalone - object - optional\n  Enables Error Tracking Standalone\n  #\n  error_tracking_standalone:\n\n    @param enabled - boolean - optional - default: false\n    @env DD_APM_ERROR_TRACKING_STANDALONE_ENABLED - boolean - optional - default: false\n    Enables or disables Error Tracking Standalone\n    enabled: false\n\n  @param profiling_receiver_timeout - integer - optional - default: 5\n  @env DD_APM_PROFILING_RECEIVER_TIMEOUT - integer - optional - default: 5\n  The timeout in seconds for receiving profile upload requests from client applications.\n  This timeout applies to the HTTP request timeout for profile uploads to the agent.\n  Increase this value if you experience timeouts with large profile uploads.\n  profiling_receiver_timeout: 5\n  @param profiling - custom object - optional\n  Enter specific configurations for internal profiling.\n  #\n  Please note that:\n    1. This does *not* enable profiling for user applications.\n    2. This only enables internal profiling of the agent go runtime.\n    3. To enable profiling for user apps please refer to\n       https://docs.datadoghq.com/tracing/profiling/\n    4. Enabling this feature will incur in billing charges and other\n       unexpected side-effects (ie. agent profiles showing with your\n       services).\n  #\n  Uncomment this parameter and the one below to enable profiling.\n  internal_profiling:\n    @param enabled - boolean - optional - default: false\n    @env DD_INTERNAL_PROFILING_ENABLED - boolean - optional - default: false\n    Enable internal profiling for the trace-agent process.\n    enabled: false\n", "process collection configuration": "\n## @param process_config - custom object - optional\n## Enter specific configurations for your Process data collection.\n## Uncomment this parameter and the one below to enable them.\n## See https://docs.datadoghq.com/graphing/infrastructure/process/\nprocess_config:\n\n  @param process_collection - custom object - optional\n  Specifies settings for collecting processes.\n  process_collection:\n    @param enabled - boolean - optional - default: false\n    Enables collection of information about running processes.\n    enabled: false\n\n  @param container_collection - custom object - optional\n  Specifies settings for collecting containers.\n  container_collection:\n    @param enabled - boolean - optional - default: true\n    Enables collection of information about running containers.\n    enabled: true\n\n  Deprecated - use `process_collection.enabled` and `container_collection.enabled` instead\n  @param enabled - string - optional - default: \"false\"\n  @env DD_PROCESS_CONFIG_ENABLED - string - optional - default: \"false\"\n   A string indicating the enabled state of the Process Agent:\n     * \"false\"    : The Agent collects only containers information.\n     * \"true\"     : The Agent collects containers and processes information.\n     * \"disabled\" : The Agent process collection is disabled.\n  enabled: \"true\"\n\n  @param expvar_port - string - optional - default: 6062\n  @env DD_PROCESS_CONFIG_EXPVAR_PORT - string - optional - default: 6062\n  Port for the debug endpoints for the process Agent.\n  expvar_port: 6062\n\n  @param cmd_port - string - optional - default: 6162\n  Port for configuring runtime settings for the process Agent.\n  cmd_port: 6162\n\n  @param log_file - string - optional\n  @env DD_PROCESS_CONFIG_LOG_FILE - string - optional\n  The full path to the file where process Agent logs are written.\n  log_file: <PROCESS_LOG_FILE_PATH>\n\n  @param intervals - custom object - optional - default: 10s for normal checks and 2s for others.\n  @env DD_PROCESS_CONFIG_INTERVALS_CONTAINER - integer - optional - default: 10\n  @env DD_PROCESS_CONFIG_INTERVALS_CONTAINER_REALTIME - integer - optional - default: 2\n  @env DD_PROCESS_CONFIG_INTERVALS_PROCESS - integer - optional - default: 10\n  @env DD_PROCESS_CONFIG_INTERVALS_PROCESS_REALTIME - integer - optional - default: 2\n  The interval, in seconds, at which the Agent runs each check. If you want consistent\n  behavior between real-time, set the `container_realtime` and `process_realtime` intervals to 10.\n  intervals:\n    container: 10\n    container_realtime: 2\n    process: 10\n    process_realtime: 2\n\n  @param process_discovery - custom object - optional\n  Specifies custom settings for the `process_discovery` object.\n  process_discovery:\n    @param enabled - boolean - optional - default: true\n    Toggles the `process_discovery` check. If enabled, this check gathers information about running integrations.\n    enabled: true\n\n    @param interval - duration - optional - default: 4h - minimum: 10m\n    An interval in hours that specifies how often the process discovery check should run.\n    interval: 4h\n\n  @param blacklist_patterns - list of strings - optional\n  @env DD_PROCESS_CONFIG_BLACKLIST_PATTERNS - space separated list of strings - optional\n  A list of regex patterns that exclude processes if matched.\n  blacklist_patterns:\n    - <REGEX>\n\n  @param queue_size - integer - optional - default: 256\n  @env DD_PROCESS_CONFIG_QUEUE_SIZE - integer - optional - default: 256\n  The number of check results to buffer in memory when a POST fails.\n  queue_size: 256\n\n  @param process_queue_bytes - integer - optional - default: 60000000\n  @env DD_PROCESS_CONFIG_PROCESS_QUEUE_BYTES - integer - optional - default: 60000000\n  The amount of data (in bytes) to buffer in memory when a POST fails.\n  process_queue_bytes: 60000000\n\n  @param rt_queue_size - integer - optional - default: 5\n  @env DD_PROCESS_CONFIG_RT_QUEUE_SIZE - integer - optional - default: 5\n  The number of realtime check results to buffer in memory when a POST fails.\n  rt_queue_size: 5\n\n  @param max_per_message - integer - optional - default: 100\n  @env DD_PROCESS_CONFIG_MAX_PER_MESSAGE - integer - optional - default: 100\n  The maximum number of processes or containers per message.\n  max_per_message: 100\n\n  @param dd_agent_bin - string - optional\n  @env DD_PROCESS_CONFIG_DD_AGENT_BIN - string - optional\n  Overrides the path to the Agent bin used for getting the hostname. Defaults are:\n    * Windows: <AGENT_DIRECTORY>\\embedded\\\\agent.exe\n    * Unix: /opt/datadog-agent/bin/agent/agent\n  dd_agent_bin: <AGENT_BIN_PATH>\n\n  @param dd_agent_env - string - optional - default: \"\"\n  @env DD_PROCESS_CONFIG_DD_AGENT_ENV - string - optional - default: \"\"\n  Overrides of the environment we pass to fetch the hostname.\n  dd_agent_env: \"\"\n\n  @param scrub_args - boolean - optional - default: true\n  @env DD_PROCESS_CONFIG_SCRUB_ARGS - boolean - optional - default: true\n  Hide sensitive data on the Live Processes page.\n  scrub_args: true\n\n  @param custom_sensitive_words - list of strings - optional\n  @env DD_PROCESS_CONFIG_CUSTOM_SENSITIVE_WORDS - space separated list of strings - optional\n  Define your own list of sensitive data to be merged with the default one.\n  Read more on Datadog documentation:\n  https://docs.datadoghq.com/graphing/infrastructure/process/#process-arguments-scrubbing\n  custom_sensitive_words:\n    - 'personal_key'\n    - '*token'\n    - 'sql*'\n    - '*pass*d*'\n\n  @param disable_realtime_checks - boolean - optional - default: false\n  @env DD_PROCESS_CONFIG_DISABLE_REALTIME - boolean - optional - default: false\n  Disable realtime process and container checks\n  disable_realtime_checks: false\n\n  @param profiling - custom object - optional\n  Enter specific configurations for internal profiling.\n  #\n  Please note that:\n    1. This does *not* enable profiling for user applications.\n    2. This only enables internal profiling of the agent go runtime.\n    3. To enable profiling for user apps please refer to\n       https://docs.datadoghq.com/tracing/profiling/\n    4. Enabling this feature will incur in billing charges and other\n       unexpected side-effects (ie. agent profiles showing with your\n       services).\n  #\n  Uncomment this parameter and the one below to enable profiling.\n  internal_profiling:\n  @param enabled - boolean - optional - default: false\n  @env DD_INTERNAL_PROFILING_ENABLED - boolean - optional - default: false\n  Enable internal profiling for the Process Agent process.\n  enabled: false\n\n\n\n", "network path configuration": "\nnetwork_path:\n  @param connections_monitoring - custom object - optional\n  Specific configurations for monitoring network connections via Network Path.\n  connections_monitoring:\n\n    @param enabled - bool - optional - default: false\n    @env DD_NETWORK_PATH_CONNECTIONS_MONITORING_ENABLED - bool - optional - default: false\n    [Beta] Enables monitoring network connections via Network Path.\n    enabled: false\n\n  @param collector - custom object - optional\n  Configuration related to Network Path Collector.\n  Network Path Collector is used to monitor network traffic connections on the host.\n  collector:\n\n    @param workers - integer - optional - default: 4\n    @env DD_NETWORK_PATH_COLLECTOR_WORKERS - integer - optional - default: 4\n    The `workers` refers to the number of concurrent workers available for network path execution.\n    workers: 4\n\n    @param pathtest_interval - integer - optional - default: 10m\n    @env DD_NETWORK_PATH_COLLECTOR_PATHTEST_INTERVAL - integer - optional - default: 10m\n    The `pathtest_interval` refers to the traceroute run interval for monitored connections.\n    pathtest_interval: 10m\n\n    @param pathtest_ttl - integer - optional - default: 35m\n    @env DD_NETWORK_PATH_COLLECTOR_PATHTEST_TTL - integer - optional - default: 35m\n    The `pathtest_ttl` refers to the duration (time-to-live) a connection will be monitored when it's not seen anymore.\n    The TTL is reset each time the connection is seen again.\n    pathtest_ttl: 35m\n\n\n", "synthetics configuration": "\n## @param synthetics - custom object - optional\n## Enter specific configuration for Synthetics tests.\nsynthetics:\n\n  @param collector - custom object - optional\n  Configuration related to run Synthetics tests.\n  collector:\n\n    @param enabled - bool - optional - default: false\n    @env DD_SYNTHETICS_COLLECTOR_ENABLED - bool - optional - default: false\n    [Preview] Enables Synthetics tests.\n    enabled: false\n\n    @param workers - integer - optional - default: 4\n    @env DD_SYNTHETICS_COLLECTOR_WORKERS - integer - optional - default: 4\n    The `workers` refers to the number of concurrent workers available for synthetics tests execution.\n    workers: 4\n\n    @param flush_interval - integer - optional - default: 10s\n    @env DD_SYNTHETICS_COLLECTOR_FLUSH_INTERVAL - integer - optional - default: 10s\n    The `flush_interval` refers to the synthetics run interval for tests execution.\n    flush_interval: 10s\n\n\n", "security agent compliance configuration": "\n## @param compliance_config - custom object - optional\n## Enter specific configuration for continuous compliance checks.\ncompliance_config:\n\n  @param enabled - boolean - optional - default: false\n  @env DD_COMPLIANCE_CONFIG_ENABLED - boolean - optional - default: false\n  Set to true to enable Cloud Security Posture Management (CSPM).\n  enabled: false\n\n  @param dir - string - optional - default: /etc/datadog-agent/compliance.d\n  @env DD_COMPLIANCE_CONFIG_DIR - string - optional - default: /etc/datadog-agent/compliance.d\n  Directory path for compliance checks configuration containing enabled benchmarks\n  dir: /etc/datadog-agent/compliance.d\n\n  @param check_interval - duration - optional - default: 20m\n  @env DD_COMPLIANCE_CONFIG_CHECK_INTERVAL - duration - optional - default: 20m\n  Check interval (see  https://golang.org/pkg/time/#ParseDuration for available options)\n  check_interval: 20m\n\n  @param check_max_events_per_run - integer - optional - default: 100\n  @env DD_COMPLIANCE_CONFIG_CHECK_MAX_EVENTS_PER_RUN - integer - optional - default: 100\n  #\n  check_max_events_per_run: 100\n\n## @param sbom - custom object - optional\n## Enter specific configuration for the Cloud Security Management Vulnerability Management feature\nsbom:\n  @param enabled - boolean - optional - default: false\n  set to true to enable Cloud Security Management Vulnerability Management\n  enabled: false\n\n  uncomment the sections below to enable where the vulnerability scanning is done\n\n  @param enabled - boolean - optional - default: false\n  set to true to enable Infrastructure Vulnerabiltilies\n  host:\n    enabled: false\n\n\n  container_image:\n    enabled: false\n", "system probe configuration": "\n## @param system_probe_config - custom object - optional\n## Enter specific configurations for your System Probe data collection.\n## Uncomment this parameter and the one below to enable them.\nsystem_probe_config:\n  @param sysprobe_socket - string - optional - default: localhost:3333\n  @env DD_SYSTEM_PROBE_CONFIG_SYSPROBE_SOCKET - string - optional - default: localhost:3333\n  The TCP address where system probes are accessed.\n  sysprobe_socket: localhost:3333\n  @param sysprobe_socket - string - optional - default: /opt/datadog-agent/run/sysprobe.sock\n  @env DD_SYSTEM_PROBE_CONFIG_SYSPROBE_SOCKET - string - optional - default: /opt/datadog-agent/run/sysprobe.sock\n  The full path to the location of the unix socket where system probes are accessed.\n  sysprobe_socket: /opt/datadog-agent/run/sysprobe.sock\n  @param log_file - string - optional - default: /var/log/datadog/system-probe.log\n  @env DD_SYSTEM_PROBE_CONFIG_LOG_FILE - string - optional - default: /var/log/datadog/system-probe.log\n  The full path to the file where system-probe logs are written.\n  log_file: /var/log/datadog/system-probe.log\n\n  @param langauge_detection - custom object - optional\n  Enter specific configurations for language detection\n  Uncomment this parameter and the one below to enable them.\n  language_detection:\n\n    @param enabled - bool - optional - default: false\n    @env DD_SYSTEM_PROBE_CONFIG_LANGUAGE_DETECTION_ENABLED - bool - optional - default: false\n    [Beta] Enables language detection via binary analysis in the system probe.\n    enabled: false\n\n  @param health_port - integer - optional - default: 0\n  @env DD_SYSTEM_PROBE_HEALTH_PORT - integer - optional - default: 0\n  The Agent can expose its health check on a dedicated HTTP port.\n  This is useful for orchestrators that support HTTP probes.\n  Default is 0 (disabled). Set a valid port number (example: 5558) to enable.\n  health_port: 0\n  @param profiling - custom object - optional\n  Enter specific configurations for internal profiling.\n  #\n  Please note that:\n    1. This does *not* enable profiling for user applications.\n    2. This only enables internal profiling of the agent go runtime.\n    3. To enable profiling for user apps please refer to\n       https://docs.datadoghq.com/tracing/profiling/\n    4. Enabling this feature will incur in billing charges and other\n       unexpected side-effects (ie. agent profiles showing with your\n       services).\n  #\n  Uncomment this parameter and the one below to enable profiling.\n  internal_profiling:\n    @param enabled - boolean - optional - default: false\n    @env DD_INTERNAL_PROFILING_ENABLED - boolean - optional - default: false\n    Enable internal profiling for the System Probe process.\n    enabled: false\n\n  @param memory_controller - custom object - optional\n  Cgroup memory controller for internal memory profiling.\n  #\n  memory_controller:\n    @param enabled - boolean - optional - default: false\n    Enable cgroup memory controller.\n    enabled: false\n    @param thresholds - map of strings - optional\n    Thresholds and the respective active actions to trigger when\n    memory usage is above the specified threshold.\n    Threshold can be either an absolute value - such as 500MB or 2GB -\n    or a percentage of the cgroup allocated memory such as 50%.\n    The action can be:\n    - gc: to trigger the Go garbage collector\n    - profile: to generate a system-probe memory profile in /tmp\n    - log: to simply log that the threshold was reached\n    thresholds:\n      500MB: gc\n      50%: profile\n\n    @param pressure_levels - map of strings - optional\n    Pressure levels and the respective active actions to trigger when\n    memory usage reaches the specified level.\n    The pressure level is 'low', 'medium' or 'critical'.\n    The actions are the same for thresholds (see above).\n    pressure_levels:\n      medium: gc\n", "system probe network configuration": "\nnetwork_config:\n  Please note that enabling the Network Module of the System\n  Probe will result in a kernel driver being loaded.\n  @param enabled - boolean - optional - default: false\n  Set to true to enable the Network Module of the System Probe\n  enabled: false\n", "system probe universal service monitoring configuration": "\nservice_monitoring_config:\n  Please note that enabling the Universal Service Monitoring\n  Module of the System Probe will result in a kernel driver\n  being loaded.\n  @param enabled - boolean - optional - default: false\n  Set to true to enable the Universal Service Monitoring Module of the System Probe\n  enabled: false\n", "system probe ping configuration": "\nping:\n  @param enabled - boolean - optional - default: false\n  Set to true to enable the Ping Module of the System Probe\n  enabled: false\n", "system probe traceroute configuration": "\ntraceroute:\n  @param enabled - boolean - optional - default: false\n  Set to true to enable the Traceroute Module of the System Probe\n  enabled: false\n", "security agent runtime configuration": "\nruntime_security_config:\n  @param enabled - boolean - optional - default: false\n  @env DD_RUNTIME_SECURITY_CONFIG_ENABLED - boolean - optional - default: false\n  Set to true to enable Cloud Workload Security (CWS).\n  enabled: false\n\n  @param fim_enabled - boolean - optional - default: false\n  Set to true to enable the File Integrity Monitoring (FIM) feature of Cloud Workload Security (CWS).\n  fim_enabled: false\n  @param sysprobe_socket - string - optional - default: localhost:3334\n  @env DD_SYSTEM_PROBE_CONFIG_SYSPROBE_SOCKET - string - optional - default: localhost:3334\n  The TCP address where the security runtime module is accessed.\n  socket: localhost:3334\n  @param socket - string - optional - default: /opt/datadog-agent/run/runtime-security.sock\n  @env DD_RUNTIME_SECURITY_CONFIG_SOCKET - string - optional - default: /opt/datadog-agent/run/runtime-security.sock\n  The full path to the location of the unix socket where security runtime module is accessed.\n  socket: /opt/datadog-agent/run/runtime-security.sock\n  @param policies - custom object - optional\n  Policy files\n  policies:\n    @param dir - string - default: %ProgramData%\\Datadog\\runtime-security.d\n    @env DD_RUNTIME_SECURITY_CONFIG_POLICIES_DIR - string - default: /etc/datadog-agent/runtime-security.d\n    Path from where the policy files are loaded\n    dir: c:\\ProgramData\\Datadog\\runtime-security.d\n    @param dir - string - default: /etc/datadog-agent/runtime-security.d\n    @env DD_RUNTIME_SECURITY_CONFIG_POLICIES_DIR - string - default: /etc/datadog-agent/runtime-security.d\n    Path from where the policy files will be loaded\n    dir: /etc/datadog-agent/runtime-security.d\n  @param syscall_monitor - custom object - optional\n  Syscall monitoring\n  syscall_monitor:\n\n    @param enabled - boolean - optional - default: false\n    @env DD_RUNTIME_SECURITY_CONFIG_SYSCALL_MONITOR_ENABLED - boolean - optional - default: false\n    Set to true to enable the Syscall monitoring (recommended for troubleshooting only).\n     enabled: false\n\n  @param custom_sensitive_words - list of strings - optional\n  @env DD_RUNTIME_SECURITY_CONFIG_CUSTOM_SENSITIVE_WORDS - space separated list of strings - optional\n  Define your own list of sensitive data to be merged with the default one.\n  Read more on Datadog documentation:\n  https://docs.datadoghq.com/graphing/infrastructure/process/#process-arguments-scrubbing\n  custom_sensitive_words:\n    - 'personal_key'\n    - '*token'\n    - 'sql*'\n    - '*pass*d*'\n\n  @param envs_with_value - list of strings - optional\n  @env DD_RUNTIME_SECURITY_CONFIG_ENVS_WITH_VALUE - space separated list of strings - optional\n  Define your own list of non-sensitive environment variable names whose value will not be\n  concealed by the runtime security module.\n  Default: LD_PRELOAD, LD_LIBRARY_PATH, PATH, HISTSIZE, HISTFILESIZE, GLIBC_TUNABLES\n  envs_with_value:\n    - LD_PRELOAD\n    - LD_LIBRARY_PATH\n    - PATH\n    - HISTSIZE\n    - HISTFILESIZE\n\n  @param activity_dump - custom object - optional\n  Activity dump section configures if/how the Agent sends activity dumps to Datadog\n  activity_dump:\n\n    @param enabled - boolean - optional - default: false\n    @env DD_RUNTIME_SECURITY_CONFIG_ACTIVITY_DUMP_ENABLED - boolean - optional - default: false\n    Set to true to activate the security profiles feature.\n    enabled: false\n\n    @param traced_cgroups_count - integer - optional - default: 5\n    @env DD_RUNTIME_SECURITY_CONFIG_ACTIVITY_DUMP_TRACED_CGROUPS_COUNT - integer - optional - default: 5\n    Defines the number of concurrent cgroups to be traced.\n    traced_cgroups_count: 5\n\n    @param dump_duration - duration - optional - default: 30m\n    @env DD_RUNTIME_SECURITY_CONFIG_ACTIVITY_DUMP_DUMP_DURATION - duration - optional - default: 30m\n    Defines the duration of cgroups learning phase. Minimum value is 10m.\n    dump_duration: 30m\n\n  @param network - custom object - optional\n  Network section is used to configure Cloud Workload Security (CWS) network features.\n  network:\n\n    @param enabled - boolean - optional - default: true\n    @env DD_RUNTIME_SECURITY_CONFIG_NETWORK_ENABLED - boolean - optional - default: true\n    Set to true to activate the CWS network detections.\n    enabled: true\n", "datadog agent windows crash detection module": "## Datadog Agent Windows Crash Detection module\n\nwindows_crash_detection:\n  @param enabled - boolean - optional - default: false\n  Enables the system probe module which supports the Windows crash detection check.\n  enabled: false\n", "runtime security configuration": "\nruntime_security_config:\n  @param enabled - boolean - optional - default: false\n  Set to true to enable Cloud Workload Security (CWS).\n  enabled: false\n  @param socket - string - optional - default: localhost:3334\n  The local address and port where the security runtime module is accessed\n  socket: localhost:3334\n  @param socket - string - optional - default: /opt/datadog-agent/run/runtime-security.sock\n  The full path to the location of the unix socket where security runtime module is accessed.\n  socket: /opt/datadog-agent/run/runtime-security.sock\n", "compliance monitoring configuration": "\ncompliance_config:\n  @param enabled - boolean - optional - default: false\n  Set to true to enable Cloud Security Posture Management (CSPM).\n  enabled: false\n", "dogstatsd configuration": "\n## @param use_dogstatsd - boolean - optional - default: true\n## @env DD_USE_DOGSTATSD - boolean - optional - default: true\n## Set this option to false to disable the Agent DogStatsD server.\nuse_dogstatsd: true\n\n## @param dogstatsd_port - integer - optional - default: 8125\n## @env DD_DOGSTATSD_PORT - integer - optional - default: 8125\n## Override the Agent DogStatsD port.\n## Note: Make sure your client is sending to the same UDP port.\ndogstatsd_port: 8125\n\n## @param bind_host - string - optional - default: localhost\n## @env DD_BIND_HOST - string - optional - default: localhost\n## The host to listen on for Dogstatsd and traces. This is ignored by APM when\n## `apm_config.apm_non_local_traffic` is enabled and ignored by DogStatsD when `dogstatsd_non_local_traffic`\n## is enabled. The trace-agent uses this host to send metrics to.\n## The `localhost` default value is invalid in IPv6 environments where dogstatsd listens on \"::1\".\n## To solve this problem, ensure Dogstatsd is listening on IPv4 by setting this value to \"127.0.0.1\".\nbind_host: localhost\n## Please note that UDS receiver is not available in Windows.\n## @ Enabling this setting may result in unexpected behavior.\n## @param dogstatsd_socket - string - optional - default: \"\"\n## @env DD_DOGSTATSD_SOCKET - string - optional - default: \"\"\n## Listen for Dogstatsd metrics on a Unix Socket (*nix only).\n## Set to \"\" to disable this feature.\ndogstatsd_socket: \"\"\n## @param dogstatsd_socket - string - optional - default: \"/var/run/datadog/dsd.socket\"\n## @env DD_DOGSTATSD_SOCKET - string - optional - default: \"/var/run/datadog/dsd.socket\"\n## Listen for Dogstatsd metrics on a Unix Socket (*nix only).\n## Set to a valid and existing filesystem path to enable.\n## Set to \"\" to disable this feature.\ndogstatsd_socket: \"/var/run/datadog/dsd.socket\"\n## @param dogstatsd_origin_detection - boolean - optional - default: false\n## @env DD_DOGSTATSD_ORIGIN_DETECTION - boolean - optional - default: false\n## When using Unix Socket, DogStatsD can tag metrics with container metadata.\n## If running DogStatsD in a container, host PID mode (e.g. with --pid=host) is required.\ndogstatsd_origin_detection: false\n\n## @param dogstatsd_origin_detection_client - boolean - optional - default: false\n## @env DD_DOGSTATSD_ORIGIN_DETECTION_CLIENT - boolean - optional - default: false\n## Whether the Agent should use a client-provided container ID to enrich the metrics, events and service checks with container tags.\n## Note: This requires using a client compatible with DogStatsD protocol version 1.2.\ndogstatsd_origin_detection_client: false\n\n## @param dogstatsd_buffer_size - integer - optional - default: 8192\n## @env DD_DOGSTATSD_BUFFER_SIZE - integer - optional - default: 8192\n## The buffer size use to receive statsd packets, in bytes.\ndogstatsd_buffer_size: 8192\n\n## @param dogstatsd_non_local_traffic - boolean - optional - default: false\n## @env DD_DOGSTATSD_NON_LOCAL_TRAFFIC - boolean - optional - default: false\n## Set to true to make DogStatsD listen to non local UDP traffic.\ndogstatsd_non_local_traffic: false\n\n## @param dogstatsd_stats_enable - boolean - optional - default: false\n## @env DD_DOGSTATSD_STATS_ENABLE - boolean - optional - default: false\n## Publish DogStatsD's internal stats as Go expvars.\ndogstatsd_stats_enable: false\n\n## @param dogstatsd_logging_enabled - boolean - optional - default: true\n## Set to true to write DogstatsD metrics received by the Agent to dogstats_stats log files.\n## Requires `dogstatsd_stats_enable: true`.\ndogstatsd_logging_enabled: true\n\n## @param dogstatsd_log_file_max_size - custom - optional - default: 10MB\n## Maximum size of dogstatsd log file. Use either a size (for example, 10MB) or\n## provide value in bytes (for example, 10485760.)\ndogstatsd_log_file_max_size: 10MB\n\n## @param dogstatsd_queue_size - integer - optional - default: 1024\n## @env DD_DOGSTATSD_QUEUE_SIZE - integer - optional - default: 1024\n## Configure the internal queue size of the Dogstatsd server.\n## Reducing the size of this queue will reduce the maximum memory usage of the\n## Dogstatsd server but as a trade-off, it could increase the number of packet drops.\ndogstatsd_queue_size: 1024\n\n## @param dogstatsd_stats_buffer - integer - optional - default: 10\n## @env DD_DOGSTATSD_STATS_BUFFER - integer - optional - default: 10\n## Set how many items should be in the DogStatsD's stats circular buffer.\ndogstatsd_stats_buffer: 10\n\n## @param dogstatsd_stats_port - integer - optional - default: 5000\n## @env DD_DOGSTATSD_STATS_PORT - integer - optional - default: 5000\n## The port for the go_expvar server.\ndogstatsd_stats_port: 5000\n\n## @param dogstatsd_so_rcvbuf - integer - optional - default: 0\n## @env DD_DOGSTATSD_SO_RCVBUF - integer - optional - default: 0\n## The number of bytes allocated to DogStatsD's socket receive buffer (POSIX system only).\n## By default, the system sets this value. If you need to increase the size of this buffer\n## but keep the OS default value the same, you can set DogStatsD's receive buffer size here.\n## The maximum accepted value might change depending on the OS.\ndogstatsd_so_rcvbuf: 0\n\n## @param dogstatsd_metrics_stats_enable - boolean - optional - default: false\n## @env DD_DOGSTATSD_METRICS_STATS_ENABLE - boolean - optional - default: false\n## Set this parameter to true to have DogStatsD collects basic statistics (count/last seen)\n## about the metrics it processed. Use the Agent command \"dogstatsd-stats\" to visualize\n## those statistics.\ndogstatsd_metrics_stats_enable: false\n\n## @param dogstatsd_tags - list of key:value elements - optional\n## @env DD_DOGSTATSD_TAGS - list of key:value elements - optional\n## Additional tags to append to all metrics, events and service checks received by\n## this DogStatsD server.\ndogstatsd_tags:\n  - <TAG_KEY>:<TAG_VALUE>\n## @param dogstatsd_mapper_profiles - list of custom object - optional\n## @env DD_DOGSTATSD_MAPPER_PROFILES - list of custom object - optional\n## The profiles will be used to convert parts of metrics names into tags.\n## If a profile prefix is matched, other profiles won't be tried even if that profile matching rules doesn't match.\n## The profiles and matching rules are processed in the order defined in this configuration.\n##\n## For each profile, following fields are available:\n##    name (required): profile name\n##    prefix (required): mapping only applies to metrics with the prefix. If set to `*`, it will match everything.\n##    mappings: mapping rules, see below.\n## For each mapping, following fields are available:\n##    match (required): pattern for matching the incoming metric name e.g. `test.job.duration.*`\n##    match_type (optional): pattern type can be `wildcard` (default) or `regex` e.g. `test\\.job\\.(\\w+)\\.(.*)`\n##    name (required): the metric name the metric should be mapped to e.g. `test.job.duration`\n##    tags (optional): list of key:value pair of tag key and tag value\n##      The value can use $1, $2, etc, that will be replaced by the corresponding element capture by `match` pattern\n##      This alternative syntax can also be used: ${1}, ${2}, etc\ndogstatsd_mapper_profiles:\n  - name: <PROFILE_NAME>                        e.g. \"airflow\", \"consul\", \"some_database\"\n    prefix: <PROFILE_PREFIX>                    e.g. \"airflow.\", \"consul.\", \"some_database.\"\n    mappings:\n      - match: <METRIC_TO_MATCH>                e.g. `test.job.duration.*` to match `test.job.duration.my_job_name`\n        match_type: <MATCH_TYPE>                e.g. `wildcard` or `regex`\n        name: <MAPPED_METRIC_NAME>              e.g. `test.job.duration`\n        tags:\n          <TAG_KEY>: <TAG_VALUE_TO_EXPAND>      e.g. `job_name: \"$1\"`, $1 is replaced by value capture by *\n      - match: 'test.worker.*.*.start_time'     to match `test.worker.<worker_type>.<worker_name>.start_time`\n        name: 'test.worker.start_time'\n        tags:\n          worker_type: '$1'\n          worker_name: '$2'\n      - match: 'test\\.task\\.duration\\.(\\w+)\\.(.*)'     no need to escape in yaml context using single quote\n        match_type: regex\n        name: 'test.task'\n        tags:\n          task_type: '$1'\n          task_name: '$2'\n\n## @param dogstatsd_mapper_cache_size - integer - optional - default: 1000\n## @env DD_DOGSTATSD_MAPPER_CACHE_SIZE - integer - optional - default: 1000\n## Size of the cache (max number of mapping results) used by Dogstatsd mapping feature.\ndogstatsd_mapper_cache_size: 1000\n\n## @param dogstatsd_entity_id_precedence - boolean - optional - default: false\n## @env DD_DOGSTATSD_ENTITY_ID_PRECEDENCE - boolean - optional - default: false\n## Disable enriching Dogstatsd metrics with tags from \"origin detection\" when Entity-ID is set.\ndogstatsd_entity_id_precedence: false\n\n## @param dogstatsd_no_aggregation_pipeline - boolean - optional - default: true\n## @env DD_DOGSTATSD_NO_AGGREGATION_PIPELINE - boolean - optional - default: true\n## Enable the no-aggregation pipeline in DogStatsD: a pipeline receiving metrics\n## with timestamp and forwarding them to the intake without extra processing except\n## for tagging.\ndogstatsd_no_aggregation_pipeline: true\n\n## @param dogstatsd_no_aggregation_pipeline_batch_size - integer - optional - default: 2048\n## @env DD_DOGSTATSD_NO_AGGREGATION_PIPELINE_BATCH_SIZE - integer - optional - default: 2048\n## How many metrics maximum in payloads sent by the no-aggregation pipeline to the intake.\ndogstatsd_no_aggregation_pipeline_batch_size: 2048\n\n## @param statsd_forward_host - string - optional - default: \"\"\n## @env DD_STATSD_FORWARD_HOST - string - optional - default: \"\"\n## Forward every packet received by the DogStatsD server to another statsd server.\n## WARNING: Make sure that forwarded packets are regular statsd packets and not \"DogStatsD\" packets,\n## as your other statsd server might not be able to handle them.\nstatsd_forward_host: \"\"\n\n## @param statsd_forward_port - integer - optional - default: 0\n## @env DD_STATSD_FORWARD_PORT - integer - optional - default: 0\n## Port or the \"statsd_forward_host\" to forward StatsD packet to.\nstatsd_forward_port: 0\n\n## @param statsd_metric_namespace - string - optional - default: \"\"\n## @env DD_STATSD_METRIC_NAMESPACE - string - optional - default: \"\"\n## Set a namespace for all StatsD metrics coming from this host.\n## Each metric received is prefixed with the namespace before it's sent to Datadog.\nstatsd_metric_namespace: \"\"\n## @param metadata_providers - list of custom object - optional\n## @env DD_METADATA_PROVIDERS - list of custom object - optional\n## Metadata providers, add or remove from the list to enable or disable collection.\n## Intervals are expressed in seconds. You can also set a provider's interval to 0\n## to disable it.\nmetadata_providers:\n  - name: k8s\n    interval: 60\n", "jmx configuration": "\n## @param jmx_custom_jars - list of strings - optional\n## @env DD_JMX_CUSTOM_JARS - space separated list of strings - optional\n## If you only run Autodiscovery tests, jmxfetch might fail to pick up custom_jar_paths\n## set in the check templates. If that is the case, force custom jars here.\njmx_custom_jars:\n  - /jmx-jars/jboss-cli-client.jar\n\n## @param jmx_use_cgroup_memory_limit - boolean - optional - default: false\n## @env DD_JMX_USE_CGROUP_MEMORY_LIMIT - boolean - optional - default: false\n## When running in a memory cgroup, openjdk 8u131 and higher can automatically adjust\n## its heap memory usage in accordance to the cgroup/container's memory limit.\n## The Agent set a Xmx of 200MB if none is configured.\n## Note: OpenJDK version < 8u131 or >= 10 as well as other JVMs might fail\n## to start if this option is set.\njmx_use_cgroup_memory_limit: false\n\n## @param jmx_use_container_support - boolean - optional - default: false\n## @env DD_JMX_USE_CONTAINER_SUPPORT - boolean - optional - default: false\n## When running in a container, openjdk 10 and higher can automatically detect\n## container specific configuration instead of querying the operating system\n## to adjust resources allotted to the JVM.\n## Note: openjdk versions prior to 10 and other JVMs might fail to start if\n## this option is set.\njmx_use_container_support: false\n\n## @param jmx_max_ram_percentage - float - optional - default: 25.0\n## @env DD_JMX_MAX_RAM_PERCENTAGE - float - optional - default: 25.0\n## When running in a container with jmx_use_container_support enabled, the JVM can\n## automatically declare the maximum heap size based off of a percentage of\n## total container allocated memory. This option is overwritten if\n## you use -Xmx to manually define the size of the heap. This option applies\n## to containers with a total memory limit greater than ~250mb. If\n## jmx_use_container_support is disabled this option has no effect.\njmx_max_ram_percentage: 25.0\n\n## @param jmx_log_file - string - optional\n## @env DD_JMX_LOG_FILE - string - optional\n## Path of the log file where JMXFetch logs are written.\njmx_log_file: <JMXFETCH_LOG_FILE_PATH>\n\n## @param jmx_max_restarts - integer - optional - default: 3\n## @env DD_JMX_MAX_RESTARTS - integer - optional - default: 3\n## Number of JMX restarts allowed in the restart-interval before giving up.\njmx_max_restarts: 3\n\n## @param jmx_restart_interval - integer - optional - default: 5\n## @env DD_JMX_RESTART_INTERVAL - integer - optional - default: 5\n## Duration of the restart interval in seconds.\njmx_restart_interval: 5\n\n## @param jmx_check_period - integer - optional - default: 15000\n## @env DD_JMX_CHECK_PERIOD - integer - optional - default: 15000\n## Duration of the period for check collections in milliseconds.\njmx_check_period: 15000\n\n## @param jmx_thread_pool_size - integer - optional - default: 3\n## @env DD_JMX_THREAD_POOL_SIZE - integer - optional - default: 3\n## JMXFetch collects multiples instances concurrently. Defines the maximum level of concurrency:\n##   * Higher concurrency increases CPU utilization during metric collection.\n##   * Lower concurrency results in lower CPU usage but may increase the total collection time.\n## A value of 1 processes instances serially.\njmx_thread_pool_size: 3\n\n## @param jmx_collection_timeout - integer - optional - default: 60\n## @env DD_JMX_COLLECTION_TIMEOUT - integer - optional - default: 60\n## Defines the maximum waiting period in seconds before timing up on metric collection.\njmx_collection_timeout: 60\n\n## @param jmx_reconnection_thread_pool_size - integer - optional - default: 3\n## @env DD_JMX_RECONNECTION_THREAD_POOL_SIZE - integer - optional - default: 3\n## JMXFetch reconnects to multiples instances concurrently. Defines the maximum level of concurrency:\n##   * Higher concurrency increases CPU utilization during reconnection.\n##   * Lower concurrency results in lower CPU usage but may increase the total reconnection time\n## A value of 1 processes instance reconnections serially.\njmx_reconnection_thread_pool_size: 3\n\n## @param jmx_reconnection_timeout - integer - optional - default: 60\n## @env DD_JMX_RECONNECTION_TIMEOUT - integer - optional - default: 60\n## Determines the maximum waiting period in seconds before timing up on instance reconnection.\njmx_reconnection_timeout: 60\n\n## @param jmx_statsd_telemetry_enabled - boolean - optional - default: false\n## @env DD_JMX_STATSD_TELEMETRY_ENABLED - boolean - optional - default: false\n## Specifies whether the JMXFetch statsd client telemetry is enabled.\njmx_statsd_telemetry_enabled: false\n\n## @param jmx_telemetry_enabled - boolean - optional - default: false\n## @env DD_JMX_TELEMETRY_ENABLED - boolean - optional - default: false\n## Specifies whether additional JMXFetch telemetry is enabled.\njmx_telemetry_enabled: false\n\n## @param jmx_java_tool_options - string - optional\n## @env DD_JMX_JAVA_TOOL_OPTIONS - string - optional\n## If you only run Autodiscovery tests, jmxfetch might fail to pick up custom_jar_paths\n## set in the check templates. If that is the case, force custom jars here.\njmx_java_tool_options: -javaagent:/path/to/agent.jar -XX:+UseG1GC\n", "logging configuration": "\n## @param log_level - string - optional - default: info\n## @env DD_LOG_LEVEL - string - optional - default: info\n## Minimum log level of the Datadog Agent.\n## Valid log levels are: trace, debug, info, warn, error, critical, and off.\n## Note: When using the 'off' log level, quotes are mandatory.\nlog_level: 'info'\n\n## @param log_file - string - optional\n## @env DD_LOG_FILE - string - optional\n## Path of the log file for the Datadog Agent.\n## See https://docs.datadoghq.com/agent/guide/agent-log-files/\nlog_file: <AGENT_LOG_FILE_PATH>\n\n## @param log_format_json - boolean - optional - default: false\n## @env DD_LOG_FORMAT_JSON - boolean - optional - default: false\n## Set to 'true' to output Agent logs in JSON format.\nlog_format_json: false\n\n## @param log_to_console - boolean - optional - default: true\n## @env DD_LOG_TO_CONSOLE - boolean - optional - default: true\n## Set to 'false' to disable Agent logging to stdout.\nlog_to_console: true\n\n## @param disable_file_logging - boolean - optional - default: false\n## @env DD_DISABLE_FILE_LOGGING - boolean - optional - default: false\n## Set to 'true' to disable logging to the log file.\ndisable_file_logging: false\n\n## @param log_file_max_size - custom - optional - default: 10MB\n## @env DD_LOG_FILE_MAX_SIZE - custom - optional - default: 10MB\n## Maximum size of one log file. Use either a size (e.g. 10MB) or\n## provide value in bytes: 10485760\nlog_file_max_size: 10MB\n\n## @param log_file_max_rolls - integer - optional - default: 1\n## @env DD_LOG_FILE_MAX_ROLLS - integer - optional - default: 1\n## Maximum amount of \"old\" log files to keep.\n## Set to 0 to not limit the number of files to create.\nlog_file_max_rolls: 1\n\n## @param log_to_syslog - boolean - optional - default: false\n## @env DD_LOG_TO_SYSLOG - boolean - optional - default: false\n## Set to 'true' to enable logging to syslog.\n## Note: Even if this option is set to 'false', the service launcher of your environment\n## may redirect the Agent process' stdout/stderr to syslog. In that case, if you wish\n## to disable logging to syslog entirely, set 'log_to_console' to 'false' as well.\nlog_to_syslog: false\n\n## @param syslog_uri - string - optional\n## @env DD_SYSLOG_URI - string - optional\n## Define a custom remote syslog uri if needed. If 'syslog_uri' is left undefined/empty,\n## a local domain socket connection is attempted.\nsyslog_uri: <SYSLOG_URI>\n\n## @param syslog_rfc - boolean - optional - default: false\n## @env DD_SYSLOG_RFC - boolean - optional - default: false\n## Set to 'true' to output in an RFC 5424-compliant format for Agent logs.\nsyslog_rfc: false\n\n## @param syslog_pem - string - optional\n## @env DD_SYSLOG_PEM - string - optional\n## If TLS enabled, you must specify a path to a PEM certificate here.\nsyslog_pem: <PEM_CERTIFICATE_PATH>\n\n## @param syslog_key - string - optional\n## @env DD_SYSLOG_KEY - string - optional\n## If TLS enabled, you must specify a path to a private key here.\nsyslog_key: <PEM_KEY_PATH>\n\n## @param syslog_tls_verify - boolean - optional - default: true\n## @env DD_SYSLOG_TLS_VERIFY - boolean - optional - default: true\n## If TLS enabled, you may enforce TLS verification here.\nsyslog_tls_verify: true\n\n## @param log_format_rfc3339 - boolean - optional - default false\n## @env DD_LOG_FORMAT_RFC3339 - boolean - optional - default false\n## If enabled the Agent will log using the RFC3339 format for the log time.\nlog_format_rfc3339: false\n\n## @param log_all_goroutines_when_unhealthy - boolean - optional - default false\n## @env DD_LOG_ALL_GOROUTINES_WHEN_UNHEALTHY - boolean - optional - default false\n## If enabled, when the health probe of an internal component fails, the stack traces\n## of all the goroutines are logged.\nlog_all_goroutines_when_unhealthy: false\n", "autoconfig configuration": "\n## @param autoconf_template_dir - string - optional - default: /datadog/check_configs\n## @env DD_AUTOCONF_TEMPLATE_DIR - string - optional - default: /datadog/check_configs\n## Directory containing configuration templates for Autoconfig.\nautoconf_template_dir: /datadog/check_configs\n\n## @param autoconf_config_files_poll - boolean - optional - default: false\n## @env DD_AUTOCONF_CONFIG_FILES_POLL - boolean - optional - default: false\n## Should the we check for new/updated integration configuration files on disk.\n## WARNING: Only files containing checks configuration are supported (logs configuration are not supported).\nautoconf_config_files_poll: false\n\n## @param autoconf_config_files_poll_interval - integer - optional - default: 60\n## @env DD_AUTOCONF_CONFIG_FILES_POLL_INTERVAL - integer - optional - default: 60\n## How frequently should the Agent check for new/updated integration configuration files (in seconds).\n## This value must be >= 1 (i.e. 1 second).\n## WARNING: Only files containing checks configuration are supported (logs configuration are not supported).\nautoconf_config_files_poll_interval: 60\n\n## @param config_providers - List of custom object - optional\n## @env DD_CONFIG_PROVIDERS - List of custom object - optional\n## The providers the Agent should call to collect checks configurations. Available providers are:\n##   * kubelet - The kubelet provider handles templates embedded in pod annotations.\n##   * docker -  The Docker provider handles templates embedded in container labels.\n##   * clusterchecks - The clustercheck provider retrieves cluster-level check configurations from the cluster-agent.\n##   * kube_services - The kube_services provider watches Kubernetes services for cluster-checks\n##\n## See https://docs.datadoghq.com/guides/autodiscovery/ to learn more\nconfig_providers:\n  - name: kubelet\n    polling: true\n  - name: docker\n    polling: true\n  - name: clusterchecks\n    grace_time_seconds: 60\n  - name: kube_services\n    polling: true\n  - name: etcd\n    polling: true\n    template_dir: /datadog/check_configs\n    template_url: http://127.0.0.1\n    username:\n    password:\n  - name: consul\n    polling: true\n    template_dir: datadog/check_configs\n    template_url: http://127.0.0.1\n    ca_file:\n    ca_path:\n    cert_file:\n    key_file:\n    username:\n    password:\n    token:\n  - name: zookeeper\n    polling: true\n    template_dir: /datadog/check_configs\n    template_url: 127.0.0.1\n    username:\n    password:\n\n## @param extra_config_providers - list of strings - optional\n## @env DD_EXTRA_CONFIG_PROVIDERS - space separated list of strings - optional\n## Add additional config providers by name using their default settings, and pooling enabled.\n## This list is available as an environment variable binding.\nextra_config_providers:\n  - clusterchecks\n\n## @param autoconfig_exclude_features - list of comma separated strings - optional\n## @env DD_AUTOCONFIG_EXCLUDE_FEATURES - list of space separated strings - optional\n## Exclude features automatically detected and enabled by environment autodiscovery.\n## Supported syntax is a list of `(<attribute>:)<regexp>`. Currently only the `name` attribute is supported.\n## When no attribute is present, it defaults to `name:` attribute.\nautoconfig_exclude_features:\n  - cloudfoundry\n  - containerd\n  - cri\n  - docker\n  - ecsec2\n  - ecsfargate\n  - eksfargate\n  - kubernetes\n  - orchestratorexplorer\n  - podman\n\n## @param autoconfig_include_features - list of comma separated strings - optional\n## @env DD_AUTOCONFIG_INCLUDE_FEATURES - list of space separated strings - optional\n## Force activation of features (as if they were discovered by environment autodiscovery).\nautoconfig_include_features:\n  - cloudfoundry\n  - containerd\n  - cri\n  - docker\n  - ecsec2\n  - ecsfargate\n  - eksfargate\n  - kubernetes\n  - orchestratorexplorer\n  - podman\n", "container autodiscovery configuration": "\n## @param container_cgroup_root - string - optional - default: /host/sys/fs/cgroup/\n## @env DD_CONTAINER_CGROUP_ROOT - string - optional - default: /host/sys/fs/cgroup/\n## Change the root directory to look at to get cgroup statistics.\n## Default if environment variable \"DOCKER_DD_AGENT\" is set to \"/host/sys/fs/cgroup\"\n## and \"/sys/fs/cgroup\" if not.\ncontainer_cgroup_root: /host/sys/fs/cgroup/\n\n## @param container_proc_root - string - optional - default: /host/proc\n## @env DD_CONTAINER_PROC_ROOT - string - optional - default: /host/proc\n## Change the root directory to look at to get proc statistics.\n## Default if environment variable \"DOCKER_DD_AGENT\" is set \"/host/proc\" and \"/proc\" if not.\ncontainer_proc_root: /host/proc\n\n## @param listeners - list of key:value elements - optional\n## @env DD_LISTENERS - list of key:value elements - optional\n## Choose \"auto\" if you want to let the Agent find any relevant listener on your host\n## At the moment, the only auto listener supported is Docker\n## If you have already set Docker anywhere in the listeners, the auto listener is ignored\nlisteners:\n  - name: auto\n  - name: docker\n\n## @param extra_listeners - list of strings - optional\n## @env DD_EXTRA_LISTENERS - space separated list of strings - optional\n## You can also add additional listeners by name using their default settings.\n## This list is available as an environment variable binding.\nextra_listeners:\n  - kubelet\n\n## @param ac_exclude - list of comma separated strings - optional\n## @env DD_AC_EXCLUDE - list of space separated strings - optional\n## Exclude containers from metrics and AD based on their name or image.\n## If a container matches an exclude rule, it won't be included unless it first matches an include rule.\n## An excluded container won't get any individual container metric reported for it.\n## See: https://docs.datadoghq.com/agent/guide/autodiscovery-management/\nac_exclude: []\n\n## @param ac_include - list of comma separated strings - optional\n## @env DD_AC_INCLUDE - list of space separated strings - optional\n## Include containers from metrics and AD based on their name or image:\n## See: https://docs.datadoghq.com/agent/guide/autodiscovery-management/\nac_include: []\n\n## @param exclude_pause_container - boolean - optional - default: true\n## @env DD_EXCLUDE_PAUSE_CONTAINER - boolean - optional - default: true\n## Exclude default pause containers from orchestrators.\n## By default the Agent doesn't monitor kubernetes/openshift pause container.\n## They are still counted in the container count (just like excluded containers).\nexclude_pause_container: true\n\n## @param docker_query_timeout - integer - optional - default: 5\n## @env DD_DOCKER_QUERY_TIMEOUT - integer - optional - default: 5\n## Set the default timeout value when connecting to the Docker daemon.\ndocker_query_timeout: 5\n\n## @param ad_config_poll_interval - integer - optional - default: 10\n## @env DD_AD_CONFIG_POLL_INTERVAL - integer - optional - default: 10\n## The default interval in second to check for new autodiscovery configurations\n## on all registered configuration providers.\nad_config_poll_interval: 10\n\n## @param ad_allowed_env_vars - list of strings - optional\n## @env DD_AD_ALLOWED_ENV_VARS - list of strings - optional\n## The list of environment variables that are allowed to be resolved in check\n## configurations.\n## If the list is not set or is empty, the default behavior applies: all envs\n## are allowed.\n## This list only applies when `ad_disable_env_var_resolution` is set to false.\nad_allowed_env_vars:\n  - <ENV_VAR_1>\n  - <ENV_VAR_2>\n\n## @param ad_disable_env_var_resolution - boolean - optional - default: false\n## @env DD_AD_DISABLE_ENV_VAR_RESOLUTION - boolean - optional - default: false\n## Disable environment variable resolution in check configurations.\nad_disable_env_var_resolution: false\n\n## @param cloud_foundry_garden - custom object - optional\n## Settings for Cloudfoundry application container autodiscovery.\ncloud_foundry_garden:\n\n  @param listen_network - string - optional - default: unix\n  @env DD_CLOUD_FOUNDRY_GARDEN_LISTEN_NETWORK - string - optional - default: unix\n  The network on which the garden API is listening. Possible values are `unix` or `tcp`\n  listen_network: unix\n\n  @param listen_address - string - optional - default: /var/vcap/data/garden/garden.sock\n  @env DD_CLOUD_FOUNDRY_GARDEN_LISTEN_ADDRESS - string - optional - default: /var/vcap/data/garden/garden.sock\n  The address on which the garden API is listening.\n  listen_address: /var/vcap/data/garden/garden.sock\n\n## @param podman_db_path - string - optional - default: \"\"\n## @env DD_PODMAN_DB_PATH - string - optional - default: \"\"\n## Settings for Podman DB that Datadog Agent collects container metrics.\npodman_db_path: \"\"\n", "cluster agent configuration": "\n## @param cluster_agent - custom object - optional\n## Settings for the Cluster Agent.\n## See https://docs.datadoghq.com/agent/cluster_agent/\ncluster_agent:\n\n  @param enabled - boolean - optional - default: false\n  Set to true to enable the Cluster Agent.\n  enabled: false\n\n  @param auth_token - string - optional - default: \"\"\n  Auth token used to make requests to the Kubernetes API server.\n  auth_token: \"\"\n\n  @param url - string - optional - default: \"\"\n  The Cluster Agent endpoint. There's no need to set it if \"kubernetes_service_name\" is set.\n  url: \"\"\n\n  @param kubernetes_service_name - string - optional - default: \"datadog-cluster-agent\"\n  Name of the Kubernetes service for the Cluster Agent.\n  kubernetes_service_name: \"datadog-cluster-agent\"\n\n  @param max_leader_connections - integer - optional - default: 100\n  Maximum number of connections between a follower and a leader.\n  max_leader_connections: 100\n\n  @param client_reconnect_period_seconds - integer - optional - default: 1200\n  Set the refersh period for Agent to Cluster Agent connection (new connection is created, old connection is closed).\n  Set to 0 to disable periodic reconnection.\n  client_reconnect_period_seconds: 1200\n\n  @param tagging_fallback - boolean - optional - default: false\n  Set to true to enabled fallback to local metamapper when the connection with the Cluster Agent fails.\n  tagging_fallback: false\n\n  @param server - custom object - optional\n  Sets the connection timeouts\n  server:\n\n    @param read_timeout_seconds - integer - optional - default: 2\n    Read timeout in seconds.\n    read_timeout_seconds: 2\n\n    @param write_timeout_seconds - integer - optional - default: 2\n    Write timeout in seconds.\n    write_timeout_seconds: 2\n\n    @param idle_timeout_seconds - integer - optional - default: 60\n    Idle timeout in seconds.\n    idle_timeout_seconds: 60\n", "cluster check configuration": "\n## @param cluster_checks - custom object - optional\n## Enter specific configurations for your cluster check.\n## The cluster-agent is able to autodiscover cluster resources and dispatch checks on\n## the node-agents (provided the clustercheck config provider is enabled on them).\n## Uncomment this parameter and the one below to enable them.\n## See https://docs.datadoghq.com/agent/kubernetes/cluster/\ncluster_checks:\n\n  @param enabled - boolean - optional - default: false\n  @env DD_CLUSTER_CHECKS_ENABLED - boolean - optional - default: false\n  Set to true to enable the dispatching logic on the leader cluster-agent.\n  enabled: false\n\n  @param node_expiration_timeout - integer - optional - default: 30\n  @env DD_CLUSTER_CHECKS_NODE_EXPIRATION_TIMEOUT - integer - optional - default: 30\n  Set \"node_expiration_timeout\" time in second after which Node-agents that have not\n  queried the cluster-agent are deleted, and their checks re-dispatched to other nodes.\n  node_expiration_timeout: 30\n\n  @param warmup_duration - integer - optional - default: 30\n  @env DD_CLUSTER_CHECKS_WARMUP_DURATION - integer - optional - default: 30\n  Set the \"warmup_duration\" duration in second for the cluster-agent to wait for all\n  node-agents to report to it before dispatching configurations.\n  warmup_duration: 30\n\n  @param cluster_tag_name - string - optional - default: cluster_name\n  @env DD_CLUSTER_CHECKS_CLUSTER_TAG_NAME - string - optional - default: cluster_name\n  If a cluster_name value is set or autodetected, a \"<CLUSTER_NAME>\" tag is added\n  to all cluster-check configurations sent to the node-agents.\n  Set a custom tag name here, or disable it by setting an empty name.\n  cluster_tag_name: cluster_name\n\n  @param extra_tags - list of key:value elements - optional\n  @env DD_CLUSTER_CHECKS_EXTRA_TAGS - list of key:value elements - optional\n  Set a list of additionnal tags can to be added to every cluster-check configuration.\n  extra_tags:\n    - <TAG_KEY>:<TAG_VALUE>\n\n  @param advanced_dispatching_enabled - boolean - optional - default: false\n  @env DD_CLUSTER_CHECKS_ADVANCED_DISPATCHING_ENABLED - boolean - optional - default: false\n  If advanced_dispatching_enabled is true the leader cluster-agent collects stats\n  from the cluster level check runners to optimize the check dispatching logic.\n  advanced_dispatching_enabled: false\n\n  @param rebalance_with_utilization - boolean - optional - default: false\n  @env DD_CLUSTER_CHECKS_REBALANCE_WITH_UTILIZATION - boolean - optional - default: false\n  If rebalance_with_utilization is true, the cluster-agent will rebalance cluster checks using node utilization.\n  rebalance_with_utilization: false\n\n  @param clc_runners_port - integer - optional - default: 5005\n  @env DD_CLUSTER_CHECKS_CLC_RUNNERS_PORT - integer - optional - default: 5005\n  Set the \"clc_runners_port\" used by the cluster-agent client to reach cluster level\n  check runners and collect their stats.\n  clc_runners_port: 5005\n", "admission controller configuration": "\n## @param admission_controller - custom object - optional\n## Enter specific configurations for your admission controller.\n## The Datadog admission controller is a component of the Datadog Cluster Agent.\n## It has two main functionalities:\n## Inject environment variables (DD_AGENT_HOST and DD_ENTITY_ID) to configure DogStatsD and APM tracer libraries into your application containers.\n## Inject Datadog reserved tags (env, service, version) from application labels into the container environment variables.\n## Uncomment this parameter and the one below to enable it.\n## See https://docs.datadoghq.com/agent/cluster_agent/admission_controller/\nadmission_controller:\n\n  @param enabled - boolean - optional - default: false\n  @env DD_ADMISSION_CONTROLLER_ENABLED - boolean - optional - default: false\n  Set to true to enable the admission controller in the cluster-agent.\n  enabled: false\n\n  @param validation - custom object - optional\n  The admission controller's validation configuration.\n  validation:\n\n    @param enabled - boolean - optional - default: true\n    @env DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED - boolean - optional - default: true\n    Set to true to enable validation webhooks controller in the cluster-agent.\n    enabled: true\n\n  @param mutation - custom object - optional\n  The admission controller's mutation configuration.\n  mutation:\n\n    @param enabled - boolean - optional - default: true\n    @env DD_ADMISSION_CONTROLLER_MUTATION_ENABLED - boolean - optional - default: true\n    Set to true to enable mutation webhooks controller in the cluster-agent.\n    enabled: true\n\n  @param mutate_unlabelled - boolean - optional - default: false\n  @env DD_ADMISSION_CONTROLLER_MUTATE_ENABLED - boolean - optional - default: false\n  Enable injecting config without having the pod label admission.datadoghq.com/enabled=\"true\".\n  mutate_unlabelled: false\n\n  @param port - integer - optional - default: 8000\n  @env DD_ADMISSION_CONTROLLER_PORT - integer - optional - default: 8000\n  The admission controller server port.\n  port: 8000\n\n  @param timeout_seconds - integer - optional - default: 10\n  @env DD_ADMISSION_CONTROLLER_TIMEOUT_SECONDS - integer - optional - default: 10\n  The admission controller server timeout in seconds.\n  timeout_seconds: 10\n\n  @param service_name - string - optional - default: datadog-admission-controller\n  @env DD_ADMISSION_CONTROLLER_SERVICE_NAME - string - optional - default: datadog-admission-controller\n  The name of the Kubernetes service that exposes the admission controller.\n  service_name: datadog-admission-controller\n\n  @param webhook_name - string - optional - default: datadog-webhook\n  @env DD_ADMISSION_CONTROLLER_WEBHOOK_NAME - string - optional - default: datadog-webhook\n  The name of the Kubernetes webhook object.\n  webhook_name: datadog-webhook\n\n  @param pod_owners_cache_validity - integer - optional - default: 10\n  @env DD_ADMISSION_CONTROLLER_POD_OWNERS_CACHE_VALIDITY - integer - optional - default: pod_owners_cache_validity\n  The in-memory cache TTL for pod owners in minutes.\n  pod_owners_cache_validity: 10\n\n  @param namespace_selector_fallback - boolean - optional - default: false\n  @env DD_ADMISSION_CONTROLLER_NAMESPACE_SELECTOR_FALLBACK - boolean - optional - default: false\n  Use namespace selectors instead of object selectors to watch objects.\n  For Kubernetes versions from 1.10 to 1.14 (inclusive)\n  namespace_selector_fallback: false\n\n  @param certificate - custom object - optional\n  The webhook's certificate configuration.\n  certificate:\n\n    @param validity_bound - integer - optional - default: 8760\n    @env DD_ADMISSION_CONTROLLER_CERTIFICATE_VALIDITY_BOUND - integer - optional - default: 8760\n    The certificate's validity bound in hours, default 1 year (365*24).\n    validity_bound: 8760\n\n    @param expiration_threshold - integer - optional - default: 720\n    @env DD_ADMISSION_CONTROLLER_CERTIFICATE_EXPIRATION_THRESHOLD - integer - optional - default: 720\n    The certificate's refresh threshold in hours, default 1 month (30*24).\n    expiration_threshold: 720\n\n    @param secret_name - string - optional - default: webhook-certificate\n    @env DD_ADMISSION_CONTROLLER_CERTIFICATE_SECRET_NAME - string - optional - default: webhook-certificate\n    Name of the Secret object containing the webhook certificate.\n    secret_name: webhook-certificate\n\n  @param inject_config - custom object - optional\n  Configuration injection parameters.\n  inject_config:\n\n    @param enabled - boolean - optional - default: true\n    @env DD_ADMISSION_CONTROLLER_INJECT_CONFIG_ENABLED - boolean - optional - default: true\n    Enable configuration injection (configure DogStatsD and APM tracer libraries).\n    enabled: true\n\n    @param endpoint - string - optional - default: /injectconfig\n    @env DD_ADMISSION_CONTROLLER_INJECT_CONFIG_ENDPOINT - string - optional - default: /injectconfig\n    Admission controller's endpoint responsible for handling configuration injection requests.\n    endpoint: /injectconfig\n\n    @param mode - string - optional - default: hostip\n    @env DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE - string - optional - default: hostip\n    The kind of configuration to be injected, it can be \"hostip\", \"service\", or \"socket\".\n    mode: hostip\n\n    @param local_service_name - string - optional - default: datadog\n    @env DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME - string - optional - default: datadog\n    Configure the local service name that exposes the Datadog Agent. Only applicable in \"service\" mode.\n    local_service_name: datadog\n\n    @param socket_path - string - optional - default: /var/run/datadog\n    @env DD_ADMISSION_CONTROLLER_INJECT_CONFIG_SOCKET_PATH - string - optional - default: /var/run/datadog\n    Configure Datadog Agent's socket path. Only applicable in \"socket\" mode.\n    socket_path: /var/run/datadog\n\n    @param trace_agent_socket - string - optional - default: unix:///var/run/datadog/apm.socket\n    @env DD_ADMISSION_CONTROLLER_INJECT_CONFIG_TRACE_AGENT_SOCKET - string - optional - default: unix:///var/run/datadog/apm.socket\n    Configure Trace Agent's socket path in the app container (DD_TRACE_AGENT_URL).\n    Only applicable in \"socket\" mode.\n    trace_agent_socket: unix:///var/run/datadog/apm.socket\n\n    @param type_socket_volumes - boolean - optional - default: false\n    @env DD_ADMISSION_CONTROLLER_INJECT_CONFIG_TYPE_SOCKET_VOLUMES - boolean - optional - default: false\n    When enabled, injected volumes are of type \"Socket\". This means that\n    injected pods will not start until the Agent creates the dogstatsd and\n    trace-agent sockets. This ensures no lost traces or dogstatsd metrics but\n    can cause the pod to wait if the agent has issues creating the sockets.\n    type_socket_volumes: false\n\n  @param inject_tags - custom object - optional\n  Tags injection parameters.\n  inject_tags:\n\n    @param enabled - boolean - optional - default: true\n    @env DD_ADMISSION_CONTROLLER_INJECT_TAGS_ENABLED - boolean - optional - default: true\n    Enable standard tags injection.\n    enabled: true\n\n    @param endpoint - string - optional - default: /injecttags\n    @env DD_ADMISSION_CONTROLLER_INJECT_TAGS_ENDPOINT - string - optional - default: /injecttags\n    Admission controller's endpoint responsible for handling tags injection requests.\n    endpoint: /injecttags\n\n  @param failure_policy - string - optional - default: Ignore\n  @env DD_ADMISSION_CONTROLLER_FAILURE_POLICY - string - optional - default: Ignore\n  Set the failure policy for dynamic admission control.\n  The default of Ignore means that pods will still be admitted even if the webhook is unavailable to inject them.\n  Setting to Fail will require the admission controller to be present and pods to be injected before they are allowed to run.\n  failure_policy: Ignore\n\n  @param reinvocation_policy - string - optional - default: IfNeeded\n  @env DD_ADMISSION_CONTROLLER_REINVOCATION_POLICY - string - optional - default: IfNeeded\n  Set the reinvocation policy for dynamic admission control.\n  See https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#reinvocation-policy\n  reinvocation_policy: IfNeeded\n\n  @param add_aks_selectors - boolean - optional - default: false\n  @env DD_ADMISSION_CONTROLLER_ADD_AKS_SELECTORS - boolean - optional - default: false\n  Adds in the admission controller webhook the selectors that are required in AKS.\n  See https://docs.microsoft.com/en-us/azure/aks/faq#can-i-use-admission-controller-webhooks-on-aks\n  add_aks_selectors: false\n\n  @param auto_instrumentation - custom object - optional\n  Library injection parameters.\n  auto_instrumentation:\n\n    @param init_resources - custom object - optional\n    CPU and Memory resources of the init containers.\n    init_resources:\n\n      @param cpu - string - optional\n      @env DD_ADMISSION_CONTROLLER_AUTO_INSTRUMENTATION_INIT_RESOURCES_CPU - string - optional\n      Configures the CPU request that will be applied for the init container's CPU request and limit.\n      cpu:\n\n      @param memory - string - optional\n      @env DD_ADMISSION_CONTROLLER_AUTO_INSTRUMENTATION_INIT_RESOURCES_MEMORY - string - optional\n      Configures the memory request that will be applied for the init container's memory request and limit.\n      memory:\n\n    @param init_security_context - json - optional\n    @env DD_ADMISSION_CONTROLLER_AUTO_INSTRUMENTATION_INIT_SECURITY_CONTEXT - json - optional\n    Security context for the init containers in JSON format. Follows the Kubernetes security context spec,\n    https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#securitycontext-v1-core,\n    ignores unknown properties.\n    init_security_context: '{\"privileged\": false}'\n    DD_ADMISSION_CONTROLLER_AUTO_INSTRUMENTATION_INIT_SECURITY_CONTEXT='{\"privileged\": false}'\n", "container detection": "\n## @param container_cgroup_prefix - string - optional - default: /docker/\n## @env DD_CONTAINER_CGROUP_PREFIX - string - optional - default: /docker/\n## On hosts with mixed workloads, non-containernized processes can\n## mistakenly be detected as containerized. Use this parameter to\n## tune the detection logic to your system and avoid false-positives.\ncontainer_cgroup_prefix: \"/docker/\"\n\n", "docker tag extraction": "\n## @param docker_labels_as_tags - map - optional\n## @env DD_DOCKER_LABELS_AS_TAGS - json - optional\n## The Agent can extract container label values and set them as metric tags values associated to a <TAG_KEY>.\n## If you prefix your tag name with `+`, it will only be added to high cardinality metrics (Docker check).\ndocker_labels_as_tags:\n  <LABEL_NAME>: <TAG_KEY>\n  <HIGH_CARDINALITY_LABEL_NAME>: +<TAG_KEY>\n## DD_DOCKER_LABELS_AS_TAGS='{\"LABEL_NAME\":\"tag_key\"}'\n\n## @param docker_env_as_tags - map - optional\n## @env DD_DOCKER_ENV_AS_TAGS - json - optional\n## The Agent can extract environment variables values and set them as metric tags values associated to a <TAG_KEY>.\n## If you prefix your tag name with `+`, it will only be added to high cardinality metrics (Docker check).\ndocker_env_as_tags:\n  <ENVVAR_NAME>: <TAG_KEY>\n## DD_DOCKER_ENV_AS_TAGS='{\"ENVVAR_NAME\": \"tag_key\"}'\n", "kubernetes tag extraction": "\n## @param kubernetes_pod_labels_as_tags - map - optional\n## @env DD_KUBERNETES_POD_LABELS_AS_TAGS - json - optional\n## The Agent can extract pod labels values and set them as metric tags values associated to a <TAG_KEY>.\n## If you prefix your tag name with +, it will only be added to high cardinality metrics.\nkubernetes_pod_labels_as_tags:\n  <POD_LABEL>: <TAG_KEY>\n  <HIGH_CARDINALITY_LABEL_NAME>: +<TAG_KEY>\n## DD_KUBERNETES_POD_LABELS_AS_TAGS='{\"LABEL_NAME\":\"tag_key\"}'\n\n## @param kubernetes_pod_annotations_as_tags - map - optional\n## @env DD_KUBERNETES_POD_ANNOTATIONS_AS_TAGS - json - optional\n## The Agent can extract annotations values and set them as metric tags values associated to a <TAG_KEY>.\n## If you prefix your tag name with +, it will only be added to high cardinality metrics.\nkubernetes_pod_annotations_as_tags:\n  <ANNOTATION>: <TAG_KEY>\n  <HIGH_CARDINALITY_ANNOTATION>: +<TAG_KEY>\n## DD_KUBERNETES_POD_ANNOTATIONS_AS_TAGS='{\"ANNOTATION_NAME\":\"tag_key\"}'\n\n## @param kubernetes_namespace_labels_as_tags - map - optional\n## @env DD_KUBERNETES_NAMESPACE_LABELS_AS_TAGS - json - optional\n## The Agent can extract namespace label values and set them as metric tags values associated to a <TAG_KEY>.\n## If you prefix your tag name with +, it will only be added to high cardinality metrics.\nkubernetes_namespace_labels_as_tags:\n  <NAMESPACE_LABEL>: <TAG_KEY>\n  <HIGH_CARDINALITY_NAMESPACE_LABEL_NAME>: +<TAG_KEY>\n## DD_KUBERNETES_NAMESPACE_LABELS_AS_TAGS='{\"<NAMESPACE_LABEL>\": \"<TAG_KEY>\"}'\n\n## @param container_env_as_tags - map - optional\n## @env DD_CONTAINER_ENV_AS_TAGS - map - optional\n## The Agent can extract environment variable values and set them as metric tags values associated to a <TAG_KEY>.\n## Requires the container runtime socket to be reachable. (Supported container runtimes: Containerd, Docker)\ncontainer_env_as_tags:\n  <ENV>: <TAG_KEY>\n\n## @param container_labels_as_tags - map - optional\n## @env DD_CONTAINER_LABELS_AS_TAGS - map - optional\n## The Agent can extract container label values and set them as metric tags values associated to a <TAG_KEY>.\n## If you prefix your tag name with `+`, it will only be added to high cardinality metrics. (Supported container\n## runtimes: Containerd, Docker).\ncontainer_labels_as_tags:\n  <LABEL_NAME>: <TAG_KEY>\n  <HIGH_CARDINALITY_LABEL_NAME>: +<TAG_KEY>\n", "ecs integration configuration": "\n## @param ecs_agent_container_name - string - optional - default: ecs-agent\n## @env DD_ECS_AGENT_CONTAINER_NAME - string - optional - default: ecs-agent\n## The ECS Agent container should be autodetected when running with the\n## default (ecs-agent) name. If not, change the container name here:\necs_agent_container_name: ecs-agent\n\n## @param ecs_agent_url - string - optional - default: http://localhost:51678\n## @env DD_ECS_AGENT_URL - string - optional - default: http://localhost:51678\n## The ECS Agent container should be autodetected when running with the\n## default (ecs-agent) name. If not, change the container name the\n## Agent should look for with ecs_agent_container_name, or force a fixed url here:\necs_agent_url: http://localhost:51678\n\n## @param ecs_collect_resource_tags_ec2 - boolean - optional - default: false\n## @env DD_ECS_COLLECT_RESOURCE_TAGS_EC2 - boolean - optional - default: false\n## The Agent can collect resource tags from the metadata API exposed by the\n## ECS Agent for tasks scheduled with the EC2 launch type.\necs_collect_resource_tags_ec2: false\n\n## @param ecs_resource_tags_replace_colon - boolean - optional - default: false\n## @env DD_ECS_RESOURCE_TAGS_REPLACE_COLON - boolean - optional - default: false\n## The Agent replaces colon `:` characters in the ECS resource tag keys by underscores `_`.\necs_resource_tags_replace_colon: false\n\n## @param ecs_metadata_timeout - integer - optional - default: 500\n## @env DD_ECS_METADATA_TIMEOUT - integer - optional - default: 500\n## Timeout in milliseconds on calls to the AWS ECS metadata endpoints.\necs_metadata_timeout: 500\n\n## @param ecs_task_collection_enabled - boolean - optional - default: true\n## @env DD_ECS_TASK_COLLECTION_ENABLED - boolean - optional - default: true\n## The Agent can collect detailed task information from the metadata API exposed by the ECS Agent,\n## which is used for the orchestrator ECS check.\necs_task_collection_enabled: true\n", "cri integration configuration": "\n## @param cri_socket_path - string - optional - default: \"\"\n## @env DD_CRI_SOCKET_PATH - string - optional - default: \"\"\n## To activate the CRI check, indicate the path of the CRI socket you're using\n## and mount it in the container if needed.\n## If left empty, the CRI check is disabled.\n## see: https://docs.datadoghq.com/integrations/cri/\ncri_socket_path: \"\"\n\n## @param cri_connection_timeout - integer - optional - default: 1\n## @env DD_CRI_CONNECTION_TIMEOUT - integer - optional - default: 1\n## Configure the initial connection timeout in seconds.\ncri_connection_timeout: 1\n\n## @param cri_query_timeout - integer - optional - default: 5\n## @env DD_CRI_QUERY_TIMEOUT - integer - optional - default: 5\n## Configure the timeout in seconds for querying the CRI.\ncri_query_timeout: 5\n", "containerd integration configuration": "\n## @param cri_socket_path - string - optional - default: /var/run/containerd/containerd.sock\n## @env DD_CRI_SOCKET_PATH - string - optional - default: /var/run/containerd/containerd.sock\n## To activate the Containerd check, indicate the path of the Containerd socket you're using\n## and mount it in the container if needed.\n## see: https://docs.datadoghq.com/integrations/containerd/\ncri_socket_path: /var/run/containerd/containerd.sock\n\n## @param cri_query_timeout - integer - optional - default: 5\n## @env DD_CRI_QUERY_TIMEOUT - integer - optional - default: 5\n## Configure the timeout in seconds for querying the Containerd API.\ncri_query_timeout: 5\n\n## Deprecated - use `containerd_namespaces` instead\n## @param containerd_namespace - list of strings - optional - default: []\n## @env DD_CONTAINERD_NAMESPACE - space separated list of strings - optional - default: []\n## Activating the Containerd check also activates the CRI check, as it contains an additional subset of useful metrics.\n## Defaults to [] which configures the agent to report metrics and events from all the containerd namespaces.\n## To watch specific namespaces, list them here.\n## https://github.com/containerd/cri/blob/release/1.2/pkg/constants/constants.go#L22-L23\ncontainerd_namespace:\n  - k8s.io\n\n## @param containerd_namespaces - list of strings - optional - default: []\n## @env DD_CONTAINERD_NAMESPACES - space separated list of strings - optional - default: []\n## Activating the Containerd check also activates the CRI check, as it contains an additional subset of useful metrics.\n## Defaults to [] which configures the agent to report metrics and events from all the containerd namespaces.\n## containerd_namespaces acts as an alias for containerd_namespace. When both containerd_namespaces and containerd_namespace\n## are configured, the Agent merges the two lists.\ncontainerd_namespaces:\n  - k8s.io\n## @param containerd_exclude_namespaces - list of strings - optional - default: [\"moby\"]\n## @env DD_CONTAINERD_EXCLUDE_NAMESPACES - space separated list of strings - optional - default: [\"moby\"]\n## When containerd_namespaces is set to [], containerd_exclude_namespaces\n## allows the exclusion of containers from specific namespaces. By default it\n## excludes \"moby\", to prevent Docker containers from being detected as\n## containerd containers.\ncontainerd_exclude_namespaces:\n  - moby\n", "kubernetes kubelet connectivity configuration": "\n## @param kubernetes_kubelet_host - string - optional\n## @env DD_KUBERNETES_KUBELET_HOST - string - optional\n## The kubelet host should be autodetected when running inside a pod.\n## If you run into connectivity issues, set the host here according to your cluster setup.\nkubernetes_kubelet_host: <KUBLET_HOST>\n\n## @param kubernetes_http_kubelet_port - integer - optional - default: 10255\n## @env DD_KUBERNETES_HTTP_KUBELET_PORT - integer - optional - default: 10255\n## The kubelet http port should be autodetected when running inside a pod.\n## If you run into connectivity issues, set the http port here according to your cluster setup.\nkubernetes_http_kubelet_port: 10255\n\n## @param kubernetes_https_kubelet_port - integer - optional - default: 10250\n## @env DD_KUBERNETES_HTTPS_KUBELET_PORT - integer - optional - default: 10250\n## The kubelet https port should be autodetected when running inside a pod.\n## If you run into connectivity issues, set the https port here according to your cluster setup.\nkubernetes_https_kubelet_port: 10250\n\n## @param kubelet_tls_verify - boolean - optional - default: true\n## @env DD_KUBELET_TLS_VERIFY - boolean - optional - default: true\n## Set to false if you don't want the Agent to verify the kubelet's certificate when using HTTPS.\nkubelet_tls_verify: true\n\n## @param kubelet_client_ca - string - optional - default: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n## @env DD_KUBELET_CLIENT_CA - string - optional - default: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n## Kublet client CA file path.\nkubelet_client_ca: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n\n## @param kubelet_auth_token_path - string - optional\n## @env DD_KUBELET_AUTH_TOKEN_PATH - string - optional\n## If authentication is needed, the Agent uses the pod's service account's\n## credentials. If you want to use a different account, or are running the Agent\n## on the host, set a custom token file path here.\nkubelet_auth_token_path: <TOKEN_FILE_PATH>\n\n## @param kubelet_client_crt - string - optional\n## @env DD_KUBELET_CLIENT_CRT - string - optional\n## Set a custom Client CRT file path.\nkubelet_client_crt: <CRT_FILE_PATH>\n\n## @param kubelet_client_key - string - optional\n## @env DD_KUBELET_CLIENT_KEY - string - optional\n## Set a custom Client key file path.\nkubelet_client_key: <CLIENT_KEY_FILE_PATH>\n\n## @param kubelet_cache_pods_duration - integer - optional - default: 5\n## @env DD_KUBELET_CACHE_PODS_DURATION - integer - optional - default: 5\n## Polling frequency in seconds of the Agent to the kubelet \"/pods\" endpoint.\nkubelet_cache_pods_duration: 5\n\n## @param kubernetes_pod_expiration_duration - integer - optional - default: 900\n## @env DD_KUBERNETES_POD_EXPIRATION_DURATION - integer - optional - default: 900\n## Set the time in second after which the Agent ignores the pods that have exited.\n## Set the duration to 0 to disable this filtering.\nkubernetes_pod_expiration_duration: 900\n\n## @param kubelet_listener_polling_interval - integer - optional - default: 5\n## @env DD_KUBELET_LISTENER_POLLING_INTERVAL - integer - optional - default: 5\n## Polling frequency in seconds at which autodiscovery will query the pod watcher to detect new pods/containers.\n## Note that kubelet_cache_pods_duration needs to be lower than this setting, or autodiscovery will only poll more frequently the same cached data (kubelet_cache_pods_duration controls the cache refresh frequency).\nkubelet_listener_polling_interval: 5\n", "kubernetes apiserver integration configuration": "\n## @param kubernetes_kubeconfig_path - string - optional - default: \"\"\n## @env DD_KUBERNETES_KUBECONFIG_PATH - string - optional - default: \"\"\n## When running in a pod, the Agent automatically uses the pod's service account\n## to authenticate with the API server.\n## Provide the path to a custom KubeConfig file if you wish to install the Agent out of a pod\n## or customize connection parameters.\n## See https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\nkubernetes_kubeconfig_path: \"\"\n\n## @param kubernetes_apiserver_ca_path - string - optional - default: \"\"\n## @env DD_KUBERNETES_APISERVER_CA_PATH - string - optional - default: \"\"\n## When running in a pod, the Agent automatically uses the pod's service account CA.\n## Use this option to keep using the InCluster config but overriding the default CA Path.\n## This parameter has no effect if `kubernetes_kubeconfig_path` is set.\nkubernetes_apiserver_ca_path: \"\"\n\n## @param kubernetes_apiserver_tls_verify - boolean - optional - default: true\n## @env DD_KUBERNETES_APISERVER_TLS_VERIFY - boolean - optional - default: true\n## When running in a pod, the Agent automatically uses the pod's service account CA.\n## Use this option to keep using the InCluster config but deactivating TLS verification (in case APIServer CA is not ServiceAccount CA)\n## This parameter has no effect if `kubernetes_kubeconfig_path` is set.\nkubernetes_apiserver_tls_verify: true\n\n## @param kubernetes_apiserver_use_protobuf - boolean - optional - default: false\n## @env DD_KUBERNETES_APISERVER_USE_PROTOBUF - boolean - optional - default: false\n## By default, communication with the apiserver is in json format. Setting the following\n## option to true allows communication in the binary protobuf format.\nkubernetes_apiserver_use_protobuf: false\n\n## @param kubernetes_collect_metadata_tags - boolean - optional - default: true\n## @env DD_KUBERNETES_COLLECT_METADATA_TAGS - boolean - optional - default: true\n## Set this to false to disable tag collection for the Agent.\n## Note: In order to collect Kubernetes service names, the Agent needs certain rights.\n## See https://github.com/DataDog/datadog-agent/blob/main/Dockerfiles/agent/README.md#kubernetes\nkubernetes_collect_metadata_tags: true\n\n## @param kubernetes_metadata_tag_update_freq - integer - optional - default: 60\n## @env DD_KUBERNETES_METADATA_TAG_UPDATE_FREQ - integer - optional - default: 60\n## Set how often in secons the Agent refreshes the internal mapping of services to ContainerIDs.\nkubernetes_metadata_tag_update_freq: 60\n\n## @param kubernetes_apiserver_client_timeout - integer - optional - default: 10\n## @env DD_KUBERNETES_APISERVER_CLIENT_TIMEOUT - integer - optional - default: 10\n## Set the timeout for the Agent when connecting to the Kubernetes API server.\nkubernetes_apiserver_client_timeout: 10\n\n## @param collect_kubernetes_events - boolean - optional - default: false\n## @env DD_COLLECT_KUBERNETES_EVENTS - boolean - optional - default: false\n## Set `collect_kubernetes_events` to true to enable collection of kubernetes\n## events to be sent to Datadog.\n## Note: leader election must be enabled below to collect events.\n##       Only the leader Agent collects events.\n## See https://github.com/DataDog/datadog-agent/blob/main/Dockerfiles/agent/README.md#event-collection\ncollect_kubernetes_events: false\n\n## @param kubernetes_event_collection_timeout - integer - optional - default: 100\n## @env DD_KUBERNETES_EVENT_COLLECTION_TIMEOUT - integer - optional - default: 100\n## Set the timeout between two successful event collections in milliseconds.\nkubernetes_event_collection_timeout: 100\n\n## @param leader_election - boolean - optional - default: false\n## @env DD_LEADER_ELECTION - boolean - optional - default: false\n## Set the parameter to true to enable leader election on this node.\n## See https://github.com/DataDog/datadog-agent/blob/main/Dockerfiles/agent/README.md#leader-election\nleader_election: false\n\n## @param leader_lease_duration - integer - optional - default: 60\n## @env DD_LEADER_LEASE_DURATION - integer - optional - default: 60\n## Set the leader election lease in seconds.\nleader_lease_duration: 60\n\n## @param kubernetes_node_labels_as_tags - map - optional\n## @env DD_KUBERNETES_NODE_LABELS_AS_TAGS - json - optional\n## Configure node labels that should be collected and their name as host tags.\n## Note: Some of these labels are redundant with metadata collected by cloud provider crawlers (AWS, GCE, Azure)\nkubernetes_node_labels_as_tags:\n  kubernetes.io/hostname: nodename\n  beta.kubernetes.io/os: os\n## DD_KUBERNETES_NODE_LABELS_AS_TAGS='{\"NODE_LABEL\": \"TAG_KEY\"}'\n\n## @param kubernetes_node_annotations_as_tags - map - optional\n## @env DD_KUBERNETES_NODE_ANNOTATIONS_AS_TAGS - json - optional\n## Configure node annotationss that should be collected and their name as host tags.\nkubernetes_node_annotations_as_tags:\n  cluster.k8s.io/machine: machine\n## DD_KUBERNETES_NODE_ANNOTATIONS_AS_TAGS='{\"NODE_ANNOTATION\": \"TAG_KEY\"}'\n\n## @param kubernetes_node_annotations_as_host_aliases - list - optional\n## @env DD_KUBERNETES_NODE_ANNOTATIONS_AS_HOST_ALIASES - list - optional\n## Configure node annotations that should be collected and used as host aliases.\nkubernetes_node_annotations_as_host_aliases:\n  - cluster.k8s.io/machine\n## DD_KUBERNETES_NODE_ANNOTATIONS_AS_HOST_ALIASES='[\"cluster.k8s.io/machine\"]'\n\n## @param cluster_name - string - optional\n## @env DD_CLUSTER_NAME - string - optional\n## Set a custom kubernetes cluster identifier to avoid host alias collisions.\n## The cluster name can be up to 40 characters with the following restrictions:\n## * Lowercase letters, numbers, and hyphens only.\n## * Must start with a letter.\n## * Must end with a number or a letter.\n##\n## These are the same rules as the ones enforced by GKE:\n## https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters#Cluster.FIELDS.name\ncluster_name: <CLUSTER_IDENTIFIER>\n\n## @param disable_cluster_name_tag_key - boolean - optional - default: false\n## @env DD_DISABLE_CLUSTER_NAME_TAG_KEY - boolean - optional - default: false\n## Disable using the 'cluster_name' tag key to submit orchestrator cluster name tag.\n## The Agent will continue sending the cluster name tag with 'kube|ecs_cluster_name' key\n## regardless of the value of this parameter.\ndisable_cluster_name_tag_key: false\n\n## @param kubernetes_ad_tags_disabled -- list of strings - optional\n## @env DD_KUBERNETES_AD_TAGS_DISABLED -- list of strings - optional\n## Can only be set to a single valid value: [ \"kube_service\" ]\n## in order to not attach the kube_service tag on ready pods\nkubernetes_ad_tags_disabled:\n  - kube_service\n## @param prometheus_scrape - custom object - optional\n## This section configures the Autodiscovery based on the Prometheus annotations\nprometheus_scrape:\n\n  @param enabled - boolean - optional - default: false\n  Enables the prometheus config provider\n  enabled: false\n\n  @param service_endpoints - boolean - optional - default: false\n  Enables Service Endpoints checks in the prometheus config provider\n  service_endpoints: false\n\n  @param checks - custom object - optional\n  Defines any extra prometheus/openmetrics check configurations to be handled by the prometheus config provider\n  checks: {}\n\n  @param version - integer - optional - default: 1\n  Version of the openmetrics check to be scheduled by the Prometheus auto-discovery\n  version: 1\n", "cloud foundry bbs configuration for autodiscovery": "\n## @param cloud_foundry_bbs - custom object - optional\n## This section configures how the Cluster Agent accesses BBS API to gather information\n## necessary for autodiscovery on BBS-based Cloud Foundry deployments.\ncloud_foundry_bbs:\n\n  @param url - string - optional - default: https://bbs.service.cf.internal:8889\n  @env DD_CLOUD_FOUNDRY_BBS_URL - string - optional - default: https://bbs.service.cf.internal:8889\n  URL of the BBS API.\n  url: https://bbs.service.cf.internal:8889\n\n  @param poll_interval - integer - optional - default: 15\n  @env DD_CLOUD_FOUNDRY_BBS_POLL_INTERVAL - integer - optional - default: 15\n  Refresh rate of BBS API, in seconds. Values lower than 10 might influence\n  performance of other operations in the cluster.\n  poll_interval: 15\n\n  @param ca_file - string - optional - default: \"\"\n  @env DD_CLOUD_FOUNDRY_BBS_CA_FILE - string - optional - default: \"\"\n  PEM-encoded CA certificate used when connecting to the BBS API.\n  ca_file: \"\"\n\n  @param cert_file - string - optional - default: \"\"\n  @env DD_CLOUD_FOUNDRY_BBS_CERT_FILE - string - optional - default: \"\"\n  PEM-encoded client certificate used when connecting to the BBS API.\n  cert_file: \"\"\n\n  @param key_file - string - optional - default: \"\"\n  @env DD_CLOUD_FOUNDRY_BBS_KEY_FILE - string - optional - default: \"\"\n  PEM-encoded client key used when connecting to the BBS API.\n  key_file: \"\"\n\n  @param env_include - list of strings - optional - default: []\n  @env DD_CLOUD_FOUNDRY_BBS_ENV_INCLUDE - list of strings - optional\n  List of regular expressions to allow a set of environment variables to be included as container tags\n  env_include: []\n  @param env_exclude - list of strings - optional - default: []\n  @env DD_CLOUD_FOUNDRY_BBS_ENV_EXCLUDE - list of strings - optional\n  List of regular expressions to forbid a set of environment variables to be included as container tags\n  env_exclude: []\n", "cloud foundry cloud controller configuration for autodiscovery": "\n## @param cloud_foundry_cc - custom object - optional\n## This section configures how the Cluster Agent accesses CC API to gather information\n## necessary for autodiscovery on Cloud Foundry deployments.\ncloud_foundry_cc:\n\n  @param url - string - optional - default: https://cloud-controller-ng.service.cf.internal:9024\n  @env DD_CLOUD_FOUNDRY_CC_URL - string - optional - default: https://cloud-controller-ng.service.cf.internal:9024\n  URL of the CC API.\n  url: https://cloud-controller-ng.service.cf.internal:9024\n\n  @param client_id - string - optional\n  @env DD_CLOUD_FOUNDRY_CC_CLIENT_ID\n  Client ID for oauth with UAA to get a token to access the CC API.\n  client_id: <UAA_CLIENT_ID>\n\n  @param client_secret - string - optional\n  @env DD_CLOUD_FOUNDRY_CC_CLIENT_SECRET\n  Client secrect for oauth with UAA to get a token to access the CC API.\n  client_secret: <UAA_CLIENT_SECRET>\n\n  @param skip_ssl_validation - boolean - optional - default: false\n  @env DD_CLOUD_FOUNDRY_CC_SKIP_SSL_VALIDATION\n  Whether or not to skip SSL validation when interacting with CC API.\n  skip_ssl_validation: false\n\n  @param poll_interval - integer - optional - default: 60\n  @env DD_CLOUD_FOUNDRY_CC_POLL_INTERVAL\n  Refresh rate of CC API, in seconds. Values lower than 10 might influence\n  performance of other operations in the cluster.\n  poll_interval: 60\n\n  @param apps_batch_size - integer - optional - default: 5000\n  @env DD_CLOUD_FOUNDRY_CC_APPS_BATCH_SIZE\n  Number of apps per page to collect when calling the list apps endpoint of the CC API. Max 5000.\n  apps_batch_size: 5000\n", "network devices configuration": "\n## @param network_devices - custom object - optional\n## Configuration related to Network Devices Monitoring\nnetwork_devices:\n\n  @param namespace - string - optional - default: default\n  Namespace can be used to disambiguate devices with the same IP.\n  Changing namespace will cause devices being recreated in NDM app.\n  It should contain less than 100 characters and should not contain any of\n  `<`, `>`, `\\n`, `\\t`, `\\r` characters.\n  This field is used by NDM features (SNMP check, SNMP Traps listener, etc).\n  namespace: default\n\n  @param autodiscovery - custom object - optional\n  Creates and schedules a listener to automatically discover your SNMP devices.\n  Discovered devices can then be monitored with the SNMP integration by using\n  the auto_conf.yaml file provided by default.\n  autodiscovery:\n\n    @param workers - integer - optional - default: 2\n    The number of concurrent tasks used to discover SNMP devices. Increasing this value\n    discovers devices faster but at the cost of increased resource consumption.\n    workers: 2\n\n    @param discovery_interval - integer - optional - default: 3600\n    How often to discover new SNMP devices, in seconds. Decreasing this value\n    discovers devices faster (within the limit of the time taken to scan subnets)\n    but at the cost of increased resource consumption.\n    discovery_interval: 3600\n\n    @param discovery_allowed_failures - integer - optional - default: 3\n    The number of failed requests to a given SNMP device before removing it from the list of monitored\n    devices.\n    If a device shuts down, the Agent stops monitoring it after `discovery_interval * discovery_allowed_failures` seconds.\n    discovery_allowed_failures: 3\n\n    @param loader - string - optional - default: python\n    Check loader to use. Available loaders:\n    - core: (recommended) Uses new corecheck SNMP integration\n    - python: Uses legacy python SNMP integration\n    loader: core\n\n    @param min_collection_interval - number - optional - default: 15\n    This changes the collection interval for the check instances created\n    from discovered SNMP devices.\n    For more information, see:\n    https://docs.datadoghq.com/developers/write_agent_check/#collection-interval\n    min_collection_interval: 15\n\n    @param use_device_id_as_hostname - boolean - optional - default: false\n    Use `device:<DEVICE_ID>` (device_id is composed of `<NAMESPACE>:<DEVICE_IP_ADDRESS>`) as `hostname`\n    for metrics and service checks (meaning that metrics and services checks will have\n    `host:device:<DEVICE_ID>` as tag).\n    This option is needed for custom tags.\n    use_device_id_as_hostname: true\n\n    @param collect_topology - boolean - optional - default: true\n    Enable the collection of topology (LLDP/CDP) data\n    collect_topology: true\n\n    @param collect_vpn - boolean - optional - default: false\n    Enable collection of VPN tunnels and route table data\n    collect_vpn: true\n\n    @param ping - custom object - optional\n    Configure ICMP pings for all hosts in SNMP autodiscovery\n    Devices will be pinged with these settings each time the SNMP\n    check is run.\n    #\n    By default, Datadog tries to use an unprivileged UDP socket to send ICMP\n    pings, but some Linux systems require using a raw socket.\n    #\n    If `linux.use_raw_socket` is set, you must enable the `ping` module\n    of system-probe for elevated privileges. See\n    system-probe.yaml.example for details.\n    ping:\n      enabled: true             Disabled by default\n      timeout: 3000             Timeout in milliseconds\n      count: 2                  Number of ping packets to send per check run\n      interval: 10              Time between sending pings (up to `count` packets) in milliseconds\n      linux:                    Linux-specific configuration\n        use_raw_socket: true    Send pings in a privileged fashion using a raw socket.\n                                This may be required if your system doesn't support\n                                sending pings in an unprivileged fashion (using a UDP socket).\n                                If `use_raw_socket` is set to true, you MUST also enable\n                                system-probe which has elevated privileges. To enable it, see system-probe.yaml.example.\n\n    @param use_deduplication - boolean - optional - default: false\n    Deduplicate IP addresses corresponding to the same device.\n    The deduplication logic is based on the sysName, sysDesc, sysObjectID and sysUptime\n\n    @param configs - list - required\n    The actual list of configurations used to discover SNMP devices in various subnets.\n    Example:\n    configs:\n     - network_address: 10.0.0.0/24\n       authentications:\n         - snmp_version: 1\n           community_string: public\n     - network_address: 10.0.1.0/28\n       authentications:\n         - community_string: public\n       ignored_ip_addresses:\n         - 10.0.1.0\n         - 10.0.1.1\n    configs:\n      @param network_address - string - required\n      The subnet in CIDR format to scan for SNMP devices.\n      All unignored IP addresses in the CIDR range are scanned.\n      For optimal discovery time, be sure to use the smallest network mask\n      possible as is appropriate for your network topology.\n      Ex: 10.0.1.0/24\n      - network_address: <NETWORK>\n\n        @param ignored_ip_addresses - list of strings - optional\n        A list of IP addresses to ignore when scanning the network.\n        ignored_ip_addresses:\n          - <IP_ADDRESS_1>\n          - <IP_ADDRESS_2>\n\n        @param port - integer - optional - default: 161\n        The UDP port to use when connecting to SNMP devices.\n        port: 161\n\n        @param authentications - list of custom objects - optional\n        A list of authentication configurations to try when connecting to your SNMP devices.\n        The Agent tries each configuration until it successfully connects.\n        Example:\n        authentications:\n          - community_string: public-1\n          - user: myUser\n            authKey: myAuthKey\n          - community_string: public-2\n        authentications:\n          @param snmp_version - integer - optional - default: <BEST_GUESS>\n          Set the version of the SNMP protocol. Available options are: `1`, `2` or `3`.\n          If unset, the Agent tries to guess the correct version based on other configuration\n          parameters, for example: if `user` is set, the Agent uses SNMP v3.\n          - snmp_version: <VERSION>\n\n            @param timeout - integer - optional - default: 5\n            The number of seconds before timing out.\n            timeout: 5\n\n            @param retries - integer - optional - default: 3\n            The number of retries before failure.\n            retries: 3\n\n            @param community_string - string - optional\n            Required for SNMP v1 & v2.\n            Enclose the community string with single quote like below (to avoid special characters being interpreted).\n            Ex: 'public'\n            community_string: '<COMMUNITY>'\n\n            @param user - string - optional\n            The username to connect to your SNMP devices.\n            SNMPv3 only.\n            user: <USERNAME>\n\n            @param authKey - string - optional\n            The passphrase to use with your Authentication type.\n            SNMPv3 only.\n            authKey: <AUTHENTICATION_KEY>\n\n            @param authProtocol - string - optional\n            The authentication protocol to use when connecting to your SNMP devices.\n            Available options are: MD5, SHA, SHA224, SHA256, SHA384, SHA512\n            Defaults to MD5 when `authentication_key` is specified.\n            SNMPv3 only.\n            authProtocol: <AUTHENTICATION_PROTOCOL>\n\n            @param privKey - string - optional\n            The passphrase to use with your privacy protocol.\n            SNMPv3 only.\n            privKey: <PRIVACY_KEY>\n\n            @param privProtocol - string - optional\n            The privacy protocol to use when connecting to your SNMP devices.\n            Available options are: DES, AES (128 bits), AES192, AES192C, AES256, AES256C\n            Defaults to DES when `privacy_key` is specified.\n            SNMPv3 only.\n            privProtocol: <PRIVACY_PROTOCOL>\n\n            @param context_name - string - optional\n            The name of your context (optional SNMP v3-only parameter).\n            context_name: <CONTEXT_NAME>\n\n        @param tags - list of strings - optional\n        A list of tags to attach to every metric and service check of all devices discovered in the subnet.\n        #\n        Learn more about tagging at https://docs.datadoghq.com/tagging\n        tags:\n          - <KEY_1>:<VALUE_1>\n          - <KEY_2>:<VALUE_2>\n\n        @param ad_identifier - string - optional - default: snmp\n        A unique identifier to attach to devices from that subnetwork.\n        When configuring the SNMP integration in snmp.d/auto_conf.yaml,\n        specify the corresponding ad_identifier at the top of the file.\n        ad_identifier: snmp\n\n        @param loader - string - optional - default: python\n        Check loader to use. Available loaders:\n        - core: will use corecheck SNMP integration\n        - python: will use python SNMP integration\n        loader: core\n\n        @param min_collection_interval - number - optional - default: 15\n        This changes the collection interval for the check instances created from\n        discovered SNMP devices. It applies to each specific config from `snmp_listener.configs`\n        and has precedence over `snmp_listener.min_collection_interval`.\n        For more information, see:\n        https://docs.datadoghq.com/developers/write_agent_check/#collection-interval\n        min_collection_interval: 15\n\n        @param use_device_id_as_hostname - boolean - optional - default: false\n        Use `device:<DEVICE_ID>` (device_id is composed of `<NAMESPACE>:<DEVICE_IP_ADDRESS>`) as `hostname`\n        for metrics and service checks (meaning that metrics and services checks will have\n        `host:device:<DEVICE_ID>` as tag).\n        This option is needed for custom tags.\n        use_device_id_as_hostname: true\n\n        @param oid_batch_size - integer - optional - default: 5\n        The number of OIDs handled by each batch.\n        oid_batch_size: 5\n\n        @param interface_configs - map - optional\n        This option is used to override interface inbound/outbound speed, add interface tags, or disable monitoring for specific interfaces\n        Example:\n        interface_configs:\n          \"10.0.0.1\":              target device IP address\n            - match_field: \"name\"  (required) the field to match, can be `name` (interface name) or `index` (ifIndex)\n              match_value: \"eth0\"  (required) the value to match\n              in_speed: 50         (optional) inbound speed value in bits per sec, no value or 0 means no override\n              out_speed: 25        (optional) outbound speed value in bits per sec, no value or 0 means no override\n              tags:                (optional) interface level tags\n                - \"testTagKey:testTagValue\"\n                - \"tagKey2:tagValue2\"\n              disabled: true         (optional) disables monitoring for matched interfaces, default value is false\n        interface_config:\n          \"10.0.0.1\":\n            - match_field: name\n              match_value: eth0\n              in_speed: 50\n              out_speed: 25\n            - match_field: index\n              match_value: '10'\n              in_speed: 50\n              out_speed: 25\n          \"10.0.0.2\":\n            - match_field: name\n              match_value: eth3\n              in_speed: 50\n              out_speed: 25\n          \"10.0.0.3\":\n            - match_field: name\n              match_value: eth4\n              tags:\n                - \"monitored:true\"\n                - \"customKey:customValue\"\n          \"10.0.0.4\":\n            - match_field: index\n              match_value: '2'\n              disabled: true\n\n        @param ping - custom object - optional\n        Configure ICMP pings for all hosts in SNMP autodiscovery\n        Devices will be pinged with these settings each time the SNMP\n        check is run.\n        #\n        By default, Datadog tries to use an unprivileged UDP socket to send ICMP\n        pings, but some linux systems require using a raw socket.\n        #\n        If `linux.use_raw_socket` is set, you must enable the `ping` module\n        of system-probe for elevated privileges. See\n        system-probe.yaml.example for details.\n        ping:\n          enabled: true             Disabled by default\n          timeout: 3000             Timeout in milliseconds\n          count: 2                  Number of ping packets to send per check run\n          interval: 10              Time between sending pings (up to `count` packets) in milliseconds\n          linux:                    Linux-specific configuration\n            use_raw_socket: true    Send pings in a privileged fashion using a raw socket.\n                                    This may be required if your system doesn't support\n                                    sending pings in an unprivileged fashion (using a UDP socket).\n                                    If `use_raw_socket` is set to true, you MUST also enable\n                                    system-probe which has elevated privileges. To enable it, see system-probe.yaml.example.\n\n  @param snmp_traps - custom object - optional\n  This section configures SNMP traps collection.\n  Traps are forwarded as logs and can be found in the logs explorer with a source:snmp-traps query\n  snmp_traps:\n\n    @param enabled - boolean - optional - default: false\n    Set to true to enable collection of traps.\n    enabled: false\n\n    @param port - integer - optional - default: 9162\n    @env DD_SNMP_TRAPS_CONFIG_PORT - integer - optional - default: 9162\n    The UDP port to use when listening for incoming trap packets.\n    Because the Datadog Agent does not run as root, the port cannot be below 1024.\n    However, if you run `sudo setcap 'cap_net_bind_service=+ep' /opt/datadog-agent/bin/agent/agent`,\n    the Datadog Agent can listen on ports below 1024.\n    port: 9162\n\n    @param community_strings - list of strings - required\n    A list of known SNMP community strings that devices can use to send traps to the Agent.\n    Traps with an unknown community string are ignored.\n    Enclose the community string with single quote like below (to avoid special characters being interpreted).\n    Must be non-empty.\n    community_strings:\n      - '<COMMUNITY_1>'\n      - '<COMMUNITY_2>'\n\n    @param users - list of custom objects - optional\n    List of SNMPv3 users that can be used to listen for traps.\n    Each user can contain:\n     * user         - string - The username used by devices when sending Traps to the Agent.\n     * authKey      - string - (Optional) The passphrase to use with the given user and authProtocol\n     * authProtocol - string - (Optional) The authentication protocol to use when listening for traps from this user.\n                               Available options are: MD5, SHA, SHA224, SHA256, SHA384, SHA512.\n                               Defaults to MD5 when authKey is set.\n     * privKey      - string - (Optional) The passphrase to use with the given user privacy protocol.\n     * privProtocol - string - (Optional) The privacy protocol to use when listening for traps from this user.\n                               Available options are: DES, AES (128 bits), AES192, AES192C, AES256, AES256C.\n                               Defaults to DES when privKey is set.\n    users:\n      - user: <USERNAME>\n        authKey: <AUTHENTICATION_KEY>\n        authProtocol: <AUTHENTICATION_PROTOCOL>\n        privKey: <PRIVACY_KEY>\n        privProtocol: <PRIVACY_PROTOCOL>\n\n    @param bind_host - string - optional\n    The hostname to listen on for incoming trap packets.\n    Binds to 0.0.0.0 by default (accepting all packets).\n    bind_host: 0.0.0.0\n\n    stop_timeout - float - optional - default: 5.0\n    The maximum number of seconds to wait for the trap server to stop when the Agent shuts down.\n    stop_timeout: 5.0\n\n  @param netflow - custom object - optional\n  This section configures NDM NetFlow (and sFlow, IPFIX) collection.\n  netflow:\n\n    @param enabled - boolean - optional - default: false\n    Set to true to enable collection of NetFlow traffic.\n    enabled: false\n\n    @param listeners - custom object - optional\n    This section configures one or more listeners ports that will receive flow traffic.\n    Each listener have the following options:\n     * flow_type    - string - The flow type correspond to the incoming flow protocol.\n                               Choices are: netflow5, netflow9, ipfix, sflow5\n     * port         - string - (Optional) The port used to receive incoming flow traffic.\n                               Default port differ by flow type: netflow5(2055), netflow9(2055), ipfix(4739), sflow5(6343)\n     * bind_host    - string - (Optional) The hostname to listen on for incoming netflow packets.\n                               Binds to 0.0.0.0 by default (accepting all packets).\n     * workers      - string - (Optional) Number of workers to use for this listener.\n                               Defaults to 1.\n     * mapping      - (Optional) List of NetflowV9/IPFIX fields to additionally collect.\n                                 Defaults to None.\n        * field       - integer - The Netflow field type ID to collect.\n        * destination - string  - Name of the collected field, is queryable under @<destination> in Datadog.\n                                 Default fields can be overridden, for example, `destination.port` overrides\n                                 the default destination port collected.\n        * type        - string  - The field type.\n                                 Available options are: string, integer, hex.\n                                 Defaults to hex.\n        * endianness  - string  - (Optional) If type is integer, endianness can be set using this parameter.\n                                 Available options are: big, little.\n                                 Defaults to big.\n    listeners:\n      - flow_type: netflow9\n        port: 2055\n        mapping:\n          - field: 1234\n            destination: transport_rtp_ssrc\n            type: integer\n      - flow_type: netflow5\n        port: 2056\n      - flow_type: ipfix\n        port: 4739\n      - flow_type: sflow5\n        port: 6343\n\n    stop_timeout - integer - optional - default: 5\n    The maximum number of seconds to wait for the NetFlow listeners to stop when the Agent shuts down.\n    stop_timeout: 5\n\n    @param reverse_dns_enrichment_enabled - boolean - optional - default: false\n    Set to true to enable reverse DNS enrichment of private source and destination IP addresses in NetFlow records.\n    reverse_dns_enrichment_enabled: false\n\n## @param reverse_dns_enrichment - custom object - optional\n## This section configures the reverse DNS enrichment component that can be used by other components in the Datadog Agent.\nreverse_dns_enrichment:\n\n  @param workers - integer - optional - default: 10\n  The number of concurrent workers used to perform reverse DNS lookups.\n  workers: 10\n\n  @param chan_size - integer - optional - default: 5000\n  The size of the channel used to send reverse DNS lookup requests to the workers.\n  chan_size: 5000\n\n  @param cache - custom object - optional\n  This section configures the cache used by the reverse DNS enrichment component.\n  cache:\n\n    @param enabled - boolean - optional - default: true\n    Set to true to enable reverse DNS enrichment caching.\n    enabled: true\n\n    @param entry_ttl - duration - optional - default: 24h\n    The amount of time that a cache entry remains valid before it is expired and removed from the cache.\n    entry_ttl: 24h\n\n    @param clean_interval - duration - optional - default: 2h\n    An interval that specifies how often expired entries are removed from the cache to free space.\n    clean_interval: 2h\n\n    @param persist_interval - duration - optional - default: 2h\n    An interval that specifies how often the cache is persisted to disk so the cache can be reloaded when the Agent is upgraded or restarted.\n    persist_interval: 2h\n\n    @param max_retries - integer - optional - default: 10\n    The maximum number of retries to perform when a DNS lookup operation fails, after which the hostname \"\" is returned and cached for the IP address.\n    max_retries: 10\n\n    @param max_size - integer - optional - default: 1000000\n    The maximum size in entries of the cache, above which additional entries will not be cached.\n    max_size: 1000000\n\n  @param rate_limiter - custom object - optional\n  This section configures the rate limiter used by the reverse DNS enrichment component.\n  rate_limiter:\n\n    @param enabled - boolean - optional - default: true\n    Set to true to enable the reverse DNS enrichment rate limiter.\n    enabled: true\n\n    @param limit_per_sec - integer - optional - default: 1000\n    The maximum number of reverse DNS lookups allowed per second by the rate limiter.\n    limit_per_sec: 1000\n\n    @param limit_throttled_per_sec - integer - optional - default: 1\n    The maximum number of reverse DNS lookups allowed per second when the rate limiter is throttled due to errors exceeding the threshold.\n    limit_throttled_per_sec: 1\n\n    @param throttle_error_threshold - integer - optional - default: 10\n    The number of consecutive errors that will trigger the rate limiter to throttle down to limit_throttled_per_sec.\n    throttle_error_threshold: 10\n\n    @param recovery_intervals - integer - optional - default: 5\n    The number of intervals over which to increase the rate limit back to limit_per_sec when lookups are again successful after being throttled due to errors.\n    recovery_intervals: 5\n\n    @param recovery_interval - duration - optional - default: 5s\n    The interval between incrementally increasing the rate limit back to limit_per_sec when lookups are again successful after being throttled due to errors.\n    The rate limit will be increased by (limit_per_sec - limit_throttled_per_sec) / recovery_intervals every recovery_interval, until it reaches\n    limit_per_sec.  For example, with limit_per_sec=1000, limit_throttled_per_sec=1, recovery_intervals=5, recovery_interval=5s, the limit will\n    be increased by 200 every 5 seconds until reaching 1000.\n    recovery_interval: 5s\n\n## @param ha_agent - custom object - optional\n## This section configures High Availability Agent feature.\nha_agent:\n\n  @param enabled - boolean - optional - default: false\n  Set to true to enable High Availability Agent feature.\n  enabled: false\n\n## @param config_id - string - optional\n## The config_id configuration is used by High Availability Agent to assign a specific config ID to an Agent.\n## When used with `ha_agent.enabled: true`, all Agents with the same config_id will be part of the same group of Agents;\n## meaning that, one Agent within this group is designated as primary and others as standby.\nconfig_id:\n", "opentelemetry configuration": "\n## @param otlp_config - custom object - optional\n## This section configures OTLP ingest in the Datadog Agent.\notlp_config:\n\n  @param receiver - custom object - optional\n  The receiver configuration. It follows the OpenTelemetry Collector's OTLP Receiver Configuration.\n  This template lists the most commonly used settings; see the OpenTelemetry Collector documentation\n  for a full list of available settings:\n  https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/otlpreceiver/config.md\n  receiver:\n\n    @param protocols - custom object - optional\n    Configuration for the supported protocols.\n    protocols:\n\n      @param grpc - custom object - optional\n      Configuration for OTLP/gRPC listener.\n      Setting this as an empty section enables the OTLP/gRPC listener with default options.\n      grpc:\n\n        @param endpoint - string - optional - default: 0.0.0.0:4317\n        @env DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_GRPC_ENDPOINT - string - optional - default: 0.0.0.0:4317\n        The OTLP/gRPC listener endpoint.\n        endpoint: 0.0.0.0:4317\n\n        @param transport - string - optional - default: tcp\n        @env DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_GRPC_TRANSPORT - string - optional - default: tcp\n        The OTLP/gRPC listener transport protocol.\n        Known protocols are \"tcp\", \"udp\", \"ip\", \"unix\", \"unixgram\", and \"unixpacket\".\n        transport: tcp\n\n        @param max_recv_msg_size_mib - number - optional - default: 4\n        @env DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_GRPC_MAX_RECV_MSG_SIZE_MIB - number - optional - default: 4\n        The maximum size (in MiB) of messages accepted by the OTLP/gRPC endpoint.\n        max_recv_msg_size_mib: 4\n\n      @param http - custom object - optional\n      Configuration for OTLP/HTTP listener.\n      Setting this as an empty section enables the OTLP/HTTP listener with default options.\n      http:\n\n        @param endpoint - string - optional - default: 0.0.0.0:4318\n        @env DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_HTTP_ENDPOINT - string - optional - default: 0.0.0.0:4318\n        The OTLP/HTTP listener endpoint.\n        endpoint: 0.0.0.0:4318\n\n  @param metrics - custom object - optional\n  Metrics-specific configuration for OTLP ingest in the Datadog Agent.\n  metrics:\n\n    @param enabled - boolean - optional - default: true\n    @env DD_OTLP_CONFIG_METRICS_ENABLED - boolean - optional - default: true\n    Set to false to disable metrics support in the OTLP ingest endpoint.\n    To enable the OTLP ingest, the otlp_config.receiver section must be set.\n    enabled: true\n\n    @param resource_attributes_as_tags - boolean - optional - default: false\n    @env DD_OTLP_CONFIG_METRICS_RESOURCE_ATTRIBUTES_AS_TAGS - boolean - optional - default: false\n    Set to true to add resource attributes of a metric to its metric tags. Please note that any of\n    the subset of resource attributes in this list https://docs.datadoghq.com/opentelemetry/guide/semantic_mapping/\n    are converted to Datadog conventions and set to to metric tags whether this option is enabled or not.\n    resource_attributes_as_tags: false\n\n    @param instrumentation_scope_metadata_as_tags - boolean - optional - default: true\n    @env DD_OTLP_CONFIG_METRICS_INSTRUMENTATION_SCOPE_METADATA_AS_TAGS - boolean - optional - default: true\n    Set to true to add metadata about the instrumentation scope that created a metric.\n    instrumentation_scope_metadata_as_tags: true\n\n    @param tag_cardinality - string - optional - default: low\n    @env DD_OTLP_CONFIG_METRICS_TAG_CARDINALITY - string - optional - default: low\n    Configure the level of granularity of tags to send for OTLP metrics. Choices are:\n      * low: add tags about low-cardinality objects (clusters, hosts, deployments, container images, ...)\n      * orchestrator: add tags about pod, (in Kubernetes), or task (in ECS or Mesos) -level of cardinality\n      * high: add tags about high-cardinality objects (individual containers, user IDs in requests, ...)\n    WARNING: sending container tags for checks metrics may create more metrics\n    (one per container instead of one per host). This may impact your custom metrics billing.\n    tag_cardinality: low\n\n    @param delta_ttl - int - optional - default: 3600\n    @env DD_OTLP_CONFIG_METRICS_DELTA_TTL - int - optional - default: 3600\n    The amount of time (in seconds) that values are kept in memory for\n    calculating deltas for cumulative monotonic metrics.\n    delta_ttl: 3600\n\n    @param histograms - custom object - optional\n    Configuration for OTLP Histograms.\n    See https://docs.datadoghq.com/metrics/otlp/?tab=histogram for details.\n    histograms:\n\n      @param mode - string - optional - default: distributions\n      @env DD_OTLP_CONFIG_METRICS_HISTOGRAMS_MODE - string - optional - default: distributions\n      How to report histograms. Valid values are:\n      #\n      - `distributions` to report metrics as Datadog distributions (recommended).\n      - `nobuckets` to not report bucket metrics,\n      - `counters` to report one metric per histogram bucket.\n      mode: distributions\n\n      Deprecated - use `send_aggregation_metrics` instead. This flag will override `send_aggregation_metrics` if both are set.\n      @param send_count_sum_metrics - boolean - optional - default: false\n      @env DD_OTLP_CONFIG_METRICS_HISTOGRAMS_SEND_COUNT_SUM_METRICS - boolean - optional - default: false\n      Whether to report sum, count, min, and max as separate histogram metrics.\n      send_count_sum_metrics: false\n\n      @param send_aggregation_metrics - boolean - optional - default: false\n      @env DD_OTLP_CONFIG_METRICS_HISTOGRAMS_SEND_AGGREGATION_METRICS - boolean - optional - default: false\n      Whether to report sum, count, min, and max as separate histogram metrics.\n      send_aggregation_metrics: false\n\n    @param sums - custom object - optional\n    Configuration for OTLP Sums.\n    See https://docs.datadoghq.com/metrics/otlp/?tab=sum for details.\n    sums:\n\n      @param cumulative_monotonic_mode - string - optional - default: to_delta\n      @env DD_OTLP_CONFIG_METRICS_SUMS_CUMULATIVE_MONOTONIC_MODE - string - optional - default: to_delta\n      How to report cumulative monotonic sums. Valid values are:\n      #\n      - `to_delta` to calculate delta for sum in the client side and report as Datadog counts.\n      - `raw_value` to report the raw value as a Datadog gauge.\n      cumulative_monotonic_mode: to_delta\n\n      @param initial_cumulative_monotonic_value - string - optional - default: auto\n      How to report the initial value for cumulative monotonic sums. Valid values are:\n      #\n      - `auto` reports the initial value if its start timestamp is set and it happens after the process was started.\n      - `drop` always drops the initial value.\n      - `keep` always reports the initial value.\n      initial_cumulative_monotonic_value: auto\n\n    @param summaries - custom object - optional\n    Configuration for OTLP Summaries.\n    See https://docs.datadoghq.com/metrics/otlp/?tab=summary for more details.\n    summaries:\n\n      @param mode - string - optional - default: gauges\n      @env DD_OTLP_CONFIG_METRICS_SUMMARIES_MODE - string - optional - default: gauges\n      How to report summaries. Valid values are:\n      #\n      - `noquantiles` to not report quantile metrics.\n      - `gauges` to report one gauge metric per quantile.\n      mode: gauges\n\n  @param traces - custom object - optional\n  Traces-specific configuration for OTLP ingest in the Datadog Agent.\n  traces:\n\n    @param enabled - boolean - optional - default: true\n    @env DD_OTLP_CONFIG_TRACES_ENABLED - boolean - optional - default: true\n    Set to false to disable traces support in the OTLP ingest endpoint.\n    To enable the OTLP ingest, the otlp_config.receiver section must be set.\n    enabled: true\n\n    @param span_name_as_resource_name - boolean - optional - default: false\n    @env DD_OTLP_CONFIG_TRACES_SPAN_NAME_AS_RESOURCE_NAME - boolean - optional - default: false\n    If set to true the OpenTelemetry span name will used in the Datadog resource name.\n    If set to false the resource name will be filled with the instrumentation library name + span kind.\n    span_name_as_resource_name: false\n\n    @param span_name_remappings - map - optional\n    @env DD_OTLP_CONFIG_TRACES_SPAN_NAME_REMAPPINGS - json - optional\n    Defines a map of span names and preferred names to map to. This can be used to automatically map Datadog Span\n    Operation Names to an updated value.\n    span_name_remappings:\n      \"io.opentelemetry.javaagent.spring.client\": \"spring.client\"\n      \"instrumentation:express.server\": \"express\"\n      \"go.opentelemetry.io_contrib_instrumentation_net_http_otelhttp.client\": \"http.client\"\n    span_name_remappings:\n      <OLD_NAME>: <NEW_NAME>\n\n    @param probabilistic_sampler - custom object - optional\n    Probabilistic sampler controlling the rate of ingestion. Using this sampler works consistently\n    in a distributed system where the sampling rate is shared. Exceptions are made for errors and\n    rare traces (if enabled via apm_config.enable_rare_sampler).\n    probabilistic_sampler:\n      @param sampling_percentage - number - optional - default: 100\n      @env DD_OTLP_CONFIG_TRACES_PROBABILISTIC_SAMPLER_SAMPLING_PERCENTAGE - number - optional - default: 100\n      If `apm_config.probabilistic_sampler.enabled` is enabled, this config is ignored, `apm_config.probabilistic_sampler.enabled.sampling_percentage`\n      is used instead.\n      Percentage of traces to ingest (0 100]. Invalid values (<= 0 || > 100) are disconsidered and the default is used.\n      If incoming spans have a sampling.priority set by the user, it will be followed and the sampling percentage will\n      be overridden.\n      sampling_percentage: 100\n\n    @param ignore_missing_datadog_fields - boolean - optional - default: false\n    @env DD_OTLP_CONFIG_IGNORE_MISSING_DATADOG_FIELDS - boolean - optional - default: false\n    IgnoreMissingDatadogFields specifies whether to recompute DD span fields if the corresponding \"datadog.\"\n    namespaced span attributes are missing. If true (default), incoming \"datadog.\" namespaced\n    OTLP span attributes are used to construct the DD span. If these attributes are missing, they are recomputed from the other\n    OTLP semantic convention attributes. If it is false, a field is populated only if its associated \"datadog.\"\n    OTLP span attribute exists; otherwise, the field is left empty.\n    ignore_missing_datadog_fields: false\n\n  @param logs - custom object - optional\n  Logs-specific configuration for OTLP ingest in the Datadog Agent.\n  logs:\n\n    @param enabled - boolean - optional - default: false\n    @env DD_OTLP_CONFIG_LOGS_ENABLED - boolean - optional - default: false\n    Set to true to enable logs support in the OTLP ingest endpoint.\n    To enable the OTLP ingest, the otlp_config.receiver section must be set.\n    enabled: true\n\n  @param debug - custom object - optional\n  Debug-specific configuration for OTLP ingest in the Datadog Agent.\n  This template lists the most commonly used settings; see the OpenTelemetry Collector documentation\n  for a full list of available settings:\n  https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/debugexporter#getting-started\n  debug:\n    @param verbosity - string - optional - default: normal\n    @env DD_OTLP_CONFIG_DEBUG_VERBOSITY - string - optional - default: normal\n    Verbosity of debug logs when Datadog Agent receives otlp traces/metrics.\n    Valid values are basic, normal, detailed, none.\n    verbosity: normal\n", "datadog apm auto-injection configuration": "\n## @param injection_controller_config - custom object\n## This section configures the Datadog APM Auto Injection controller.\n## Uncomment this parameter and the one below to enable them.\ninjection_controller_config:\n\n  @param enabled - boolean - optional - default: false\n  Set to true to enable the APM Auto-injection.\n  Please note that enabling this service will result in a kernel driver being loaded.\n  enabled: false\n\n  @param log_file - string - optional - default: c:\\programdata\\datadog\\logs\\apm-inject.log\n  The full path to the file where injection controller logs are written.\n  log_file: c:\\programdata\\datadog\\logs\\apm-inject.log\n\n  @param log_level - string - optional - default: info\n  Minimum log level of the injection controller.\n  Valid log levels are: debug, info, warn, and error.\n  log_level: 'info'\n\n  @param log_to_console - boolean - optional - default: true\n  Set to 'false' to disable injection controller logging to stdout.\n  log_to_console: true\n\n  @param socket_port - integer - optional - default: 3030\n  The port used for the injection controller communications API (served on localhost).\n  socket_port: 3030\n\n  internal_profiling:\n    @param enabled - boolean - optional - default: false\n    Enable internal profiling for the injection controller process.\n    enabled: false\n\n## @param service_configs - list of custom objects\n## This section configures the services which will be automatically injected with APM\n## configurations, as well as the APM configurations which will be injected.\nservice_configs:\n\n  @param service configuration - custom object\n  In order to configure APM auto-injection for a service or set of services, an injection condition\n  and APM configuration must be provided.\n  #\n  Example:\n  - conditions:\n      command_line_regex: executable_name.exe\n    configuration:\n      service_language: dotnet\n      dd_env: staging\n      dd_service: exampleService\n      dd_version: 1.2.3\n  #\n  To learn about all the available service matching conditions & configuration options, visit\n  https://docs.datadoghq.com/tracing/trace_collection/library_injection_local\n", "datadog apm configuration": "\n## @section APM Configuration Rules\n##\n## Enable and configure APM, profiling, and security monitoring features.\n##\n## Settings can be configured via environment variables or the application_monitoring.yaml file.\n## Configuration precedence (highest to lowest priority):\n##   1. Fleet-managed config file\n##      (etc/datadog-agent/managed/datadog-agent/stable/application_monitoring.yaml)\n##   2. Environment variables\n##   3. Local config file\n##      (etc/datadog-agent/application_monitoring.yaml)\napm_configuration_default:\n\n  @param DD_APM_TRACING_ENABLED - boolean - optional - default: true\n  Enable Datadog tracing.\n  Docs: https://docs.datadoghq.com/tracing/trace_collection/\n  DD_APM_TRACING_ENABLED: true\n\n  @param DD_RUNTIME_METRICS_ENABLED - boolean - optional - default: false\n  Enable runtime metrics.\n  Docs: https://docs.datadoghq.com/tracing/metrics/runtime_metrics/?tab=java#environment-variables\n  DD_RUNTIME_METRICS_ENABLED: false\n\n  @param DD_LOGS_INJECTION - boolean - optional - default: false\n  Enable automatic trace and span ID injection into logs.\n  Docs: https://docs.datadoghq.com/tracing/other_telemetry/connect_logs_and_traces/\n  DD_LOGS_INJECTION: false\n\n  @param DD_PROFILING_ENABLED - boolean - optional - default: false\n  Enable continuous profiling.\n  Docs: https://docs.datadoghq.com/profiler/\n  DD_PROFILING_ENABLED: false\n\n  @param DD_DATA_STREAMS_ENABLED - boolean - optional - default: false\n  Enable data streams monitoring.\n  Docs: https://docs.datadoghq.com/data_streams/\n  DD_DATA_STREAMS_ENABLED: false\n\n  @param DD_APPSEC_ENABLED - boolean - optional - default: false\n  Enable the Application Security product.\n  Docs: https://docs.datadoghq.com/security/application_security/\n  DD_APPSEC_ENABLED: false\n\n  @param DD_IAST_ENABLED - boolean - optional - default: false\n  Enable Interactive Application Security Testing (IAST).\n  Docs: https://docs.datadoghq.com/security/code_security/iast/setup/#amazon-ecs\n  DD_IAST_ENABLED: false\n\n  @param DD_DYNAMIC_INSTRUMENTATION_ENABLED - boolean - optional - default: false\n  Enable Dynamic Instrumentation.\n  Docs: https://docs.datadoghq.com/dynamic_instrumentation/\n  DD_DYNAMIC_INSTRUMENTATION_ENABLED: false\n\n  @param DD_DATA_JOBS_ENABLED - boolean - optional - default: false\n  Enable data jobs visibility.\n  DD_DATA_JOBS_ENABLED: false\n\n  @param DD_APPSEC_SCA_ENABLED - boolean - optional - default: false\n  Enable Software Composition Analysis.\n  Docs: https://docs.datadoghq.com/security/code_security/software_composition_analysis/\n  DD_APPSEC_SCA_ENABLED: false\n\n  @param DD_TRACE_DEBUG - boolean - optional - default: false\n  Enable debug logging for the tracer.\n  Docs: https://docs.datadoghq.com/tracing/troubleshooting/tracer_debug_logs\n  DD_TRACE_DEBUG: false\n"}